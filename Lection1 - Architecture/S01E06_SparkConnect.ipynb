{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(dbutils.widgets.get(\"test_widget\"))\n",
    "\n",
    "sdf = spark.range(100000000000, 20)\n",
    "print(sdf.count())\n",
    "\n",
    "spark.sql(\"Select * from catalog.test.database\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .remote(\"sc://localhost\") \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "== Parsed Logical Plan ==\n",
      "Range (1000, 4, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Range (1000, 4, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Range (1000, 4, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Range (1000, 4, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.range(1000, 4)\n",
    "print(sdf.count())\n",
    "sdf.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parquet = \"D:/Spark/Data/format_parquet_large.parquet\"\n",
    "sdf_schema = \"id bigint, date date, timestamp timestamp, idstring string, idfirst string, idlast string\"\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 100000)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id < 100000)\n",
      "+- Relation [id#52L,date#53,timestamp#54,idstring#55,idfirst#56,idlast#57] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, date: date, timestamp: timestamp, idstring: string, idfirst: string, idlast: string\n",
      "Filter (id#52L < cast(100000 as bigint))\n",
      "+- Relation [id#52L,date#53,timestamp#54,idstring#55,idfirst#56,idlast#57] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(id#52L) AND (id#52L < 100000))\n",
      "+- Relation [id#52L,date#53,timestamp#54,idstring#55,idfirst#56,idlast#57] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(id#52L) AND (id#52L < 100000))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#52L,date#53,timestamp#54,idstring#55,idfirst#56,idlast#57] Batched: true, DataFilters: [isnotnull(id#52L), (id#52L < 100000)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/format_parquet_large.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,100000)], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_parquet = \"D:/Spark/Data/format_parquet_large.parquet\"\n",
    "sdf_schema = \"id bigint, date date, timestamp timestamp, idstring string, idfirst string, idlast string\"\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 100000)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "sdf_parquet.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder.parquet/\n",
    "    file1.parquet\n",
    "    file2.parquet\n",
    "    file3.parquet\n",
    "    file4.parquet\n",
    "    file5.parquet\n",
    "    ...\n",
    "    fileN.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-3.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
