{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark partitions are influenced when loading the data with parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need this?\n",
    "- Understand how Spark creates partitioning and what influences it makes performance and debugging better\n",
    "- You learned how the number of partitions, empty partitions and distribution of data within partitions influences the performance (added the previous comments below)\n",
    "- Coalesce and especially repartition are expensive operation. If we can influence the spark partitions during loading already is a big win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n",
    "results_dict = {}\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10**decimals\n",
    "    return math.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "#source: https://realpython.com/python-rounding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_num_partitions(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    #get spark values\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    #Calculate maxSpliPartitionBytes\n",
    "    # a) If we have bigger files bytesPerCorePadded will be bigger then openCostInBytes but also maxPartitionBytes. \n",
    "    # In this case we would limit the size as mayPartitionBytes will be the result of maxSplitPartitionBytes. e.g. 1 GB dataset, 4 cores, maxPartitions 128 MB\n",
    "    # b) If bytesPerCorePadded is the result of maxSplitPartitionBytes we have a fair split of the data over all cores, e.g. 1 GB dataset, 4 cores, maxPartitions 300 MB\n",
    "    # c) If bytesPerCore is to small we want to limit amount of Partitions to be opened. This is the cost here.\n",
    "    paddedFileSize = file_size + num_files * openCostInBytes\n",
    "    bytesPerCorePadded = paddedFileSize / minPartitionNum\n",
    "    maxSplitPartitionBytes = min(maxPartitionBytes, max(openCostInBytes, bytesPerCorePadded))\n",
    "    #Estimation of partitions from Internet\n",
    "    estimated_num_partitions_int = paddedFileSize/maxSplitPartitionBytes\n",
    "    #Own Estimator\n",
    "    avg_file_size_padded = paddedFileSize/num_files\n",
    "    bytesPerCore = file_size / minPartitionNum\n",
    "    #Calculate number of files fitting into one partitions. Then calculate the number of partitions\n",
    "    files_per_partition = max(1, math.floor(maxSplitPartitionBytes/avg_file_size_padded))\n",
    "    estimated_num_partitions = num_files/files_per_partition\n",
    "    print(\" \")\n",
    "    print(\"******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\")\n",
    "    print(f\"Avg file Size Padded: {round(avg_file_size_padded/1024/1024, 1)} MB or {avg_file_size_padded} bytes\")\n",
    "    print(f\"Padded File Size: {round(paddedFileSize/1024/1024, 1)} MB or {paddedFileSize} bytes\")\n",
    "    print(f\"SizePerCore: {round(bytesPerCore/1024/1024, 1)} MB or {bytesPerCore} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(bytesPerCorePadded/1024/1024, 1)} MB or {bytesPerCorePadded} bytes\")\n",
    "    print(f\"MaxSplitPartitionBytes: {round(maxSplitPartitionBytes/1024/1024, 1)} MB or {maxSplitPartitionBytes} bytes\")\n",
    "    print(f\"MaxFilesPerPartition {files_per_partition}\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(estimated_num_partitions)}, unrounded: {estimated_num_partitions}\")\n",
    "    print(f\"EstimatedPartitionsInternet: {math.ceil(estimated_num_partitions_int)}, unrounded: {estimated_num_partitions_int}\")\n",
    "\n",
    "    results_dict[\"paddedFileSize\"] = round(paddedFileSize/1024/1024, 1)\n",
    "    results_dict[\"MBPerCore\"] = round(bytesPerCore/1024/1024, 1)\n",
    "    results_dict[\"MBPerCorePadded\"] = round(bytesPerCorePadded/1024/1024, 1)\n",
    "    results_dict[\"maxSplitPartitionBytes\"] = round(maxSplitPartitionBytes/1024/1024, 1)\n",
    "    results_dict[\"avg_file_size_padded\"] = round(avg_file_size_padded/1024/1024, 1)\n",
    "    results_dict[\"Maxfiles_per_partition\"] = files_per_partition\n",
    "    results_dict[\"MyEstimationPartitions\"] = math.ceil(estimated_num_partitions)\n",
    "    results_dict[\"InternetEstimationPartitions\"] = math.ceil(estimated_num_partitions_int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_bytes_rows_partition(sdf):\n",
    "    sdf = (\n",
    "        sdf.select(f.mean(\"numFiles\"), f.mean(\"compressedBytes\"), f.mean(\"rows\"))\n",
    "        .withColumn(\"avg(compressedMB)\", f.round(f.col(\"avg(compressedBytes)\")/1024/1204, 1))\n",
    "        .select(\"avg(numFiles)\", \"avg(compressedBytes)\", \"avg(compressedMB)\", \"avg(rows)\")\n",
    "    )\n",
    "    sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What influences the no. of partitions when loading parquet files\n",
    "- Num Cores in the cluster,\n",
    "  - Correctly speaking it's the \"spark.sql.files.minPartitionNum\" config\n",
    "  - It defaults to the default parallism which is our num of cores = 4\n",
    "- File Size or better estimated file size\n",
    "- Num parquet files\n",
    "- Num of blocks/rowgroups within a parquet file\n",
    "- Max Partition Size:\n",
    "  - Influences the size of a partition  \n",
    "  - based on the config\"spark.sql.files.maxPartitionBytes\"\n",
    "  - defaults to 128 MB \n",
    "- Max Cost Per Bytes\n",
    "  - Represents the cost of creating a new partition\n",
    "  - based on the config \"spark.sql.files.minPartitionNum\"\n",
    "  - defaults to 4 MB\n",
    "  - Technically it adds the cost, e.g. 4 MB, to each file which is called padding\n",
    "  - Through this less but bigger partitions are created around the size of the open cost value\n",
    "  - Usually no influence, except of smaller files, default of 4MB works\n",
    "  - Official description: The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimate, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first). This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "- https://stackoverflow.com/questions/69034543/number-of-tasks-while-reading-hdfs-in-spark\n",
    "- https://stackoverflow.com/questions/75924368/skewed-partitions-when-setting-spark-sql-files-maxpartitionbytes\n",
    "- https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "- https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summary: Why do we need this?\n",
    "- Understand how Spark creates partitioning and what influences it makes performance and debugging better\n",
    "- You learned how the number of partitions, empty partitions and distribution of data within partitions influences the performance (added the previous comments below)\n",
    "- Coalesce and especially repartition are expensive operation. If we can influence the spark partitions during loading already is a big win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recap for reading: How partitions influence performance\n",
    "\n",
    "## The most important thing you want a good parallisation. \n",
    "- This means your number of partitions should always depend on the number of cores you have available. In spark language: spark.sparkContext.defaultParallelism. Recommendations are a factor of 2-4. But really depends on memory and data size. Small data sizes run perfectly with a factor 1x.\n",
    "- To have a good parallisation you should also have a well (best uniform, worst case normal) distributed dataset. Data skew can even in narrow transformations already make your whole execution dependend on one partition or task as we saw before\n",
    "## Partition size\n",
    "- If your partition size is really big > 1GB you might have OOM (out of memory), Garbage collection (GC) and other errors\n",
    "- Recommendations in the internet say anything between 100-1000 MB. Spark sets his max partition bytes parameter for example to 128 MB. It really depends on your machine and available memory of course. Definitly don't scratch the limits of available memory.\n",
    "## Distribution overhead\n",
    "- As we saw in previous experiments a to high number of partitions leads to a lot of scheduling and distribution overhead.\n",
    "- A good sign is if your actual aexecution time makes not at least 90 % of the total task time. Also if your tasks are below 100 ms it's usually to short\n",
    "\n",
    "See also here: https://stackoverflow.com/questions/64600212/how-to-determine-the-partition-size-in-an-apache-spark-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple experiments\n",
    "- Experiment 1: 4 files, a 64.8 MB, sum 259.3 MB\n",
    "- Experiment 2: 8 files, a 64.9 MB MB, sum 518.9 MB\n",
    "- Experiment 3: 8 files, a 47,5 MB, sum 380 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: D:/Spark/Data/4_files_32000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 8\n",
      "Saved Path: D:/Spark/Data/8_files_64000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Basic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic algorithm\n",
    "def basic_algorithm(file_size):\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])    \n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    size_per_core = file_size/minPartitionNum\n",
    "    partition_size = min(maxPartitionBytes, size_per_core)\n",
    "    no_partitions = file_size/partition_size #round up for no_partitions\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Size Per Core: {round(size_per_core/1024/1024, 1)} MB or {size_per_core} bytes\")\n",
    "    print(f\"Partionsize: {round(partition_size/1024/1024, 1)} MB or {partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions)}, unrounded: {no_partitions}\")\n",
    "\n",
    "#Reference: https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00000-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00001-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00003-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00002-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Num files: 4\n",
      "Avg file Size: 64.8 MB or 67965922.75 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4\n",
      "Num rows written: 32000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 30 MB or 31457280 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Size Per Core: 64.8 MB or 67965922.75 bytes\n",
      "Partionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitions: 9, unrounded: 8.6423139890035\n",
      " \n",
      "******** Basic TO Calculate Max PartitionSize ********\n",
      "Padded File Size: 275.3 MB or 288640907 bytes\n",
      "SizePerCorePadded: 68.8 MB or 72160226.75 bytes\n",
      "MaxPartionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitions: 10, unrounded: 9.175647322336832\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 10\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|       67726805|        64.6|8000000|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|       68008198|        64.9|8000000|\n",
      "|        4|       1|              0|         0.0|      0|\n",
      "|        5|       1|       68064947|        64.9|8000000|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|       68063741|        64.9|8000000|\n",
      "|        8|       3|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/4_files_32000000_rows.parquet\"\n",
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=30, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "basic_maxSplitBytes(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "#noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00001-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00003-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00002-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00007-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063635       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00005-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063528       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00004-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063118       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00000-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00006-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063235       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 30 MB or 31457280 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Size Per Core: 129.7 MB or 136029301.75 bytes\n",
      "Partionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitions: 18, unrounded: 17.297020181020102\n",
      " \n",
      "******** Basic TO Calculate Max PartitionSize ********\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxPartionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitions: 19, unrounded: 18.36368684768677\n",
      " \n",
      "******** Advanced TO Calculate Max PartitionSize ********\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxPartionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitions: 19, unrounded: 18.36368684768677\n",
      "Number calculated Partitions: 9\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 19\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|       67726805|        64.6|8000000|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|       68008198|        64.9|8000000|\n",
      "|        4|       1|              0|         0.0|      0|\n",
      "|        5|       1|       68064947|        64.9|8000000|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|       68063741|        64.9|8000000|\n",
      "|        8|       1|              0|         0.0|      0|\n",
      "|        9|       1|       68063118|        64.9|8000000|\n",
      "|       10|       1|              0|         0.0|      0|\n",
      "|       11|       1|       68063528|        64.9|8000000|\n",
      "|       12|       1|              0|         0.0|      0|\n",
      "|       13|       1|       68063235|        64.9|8000000|\n",
      "|       14|       1|              0|         0.0|      0|\n",
      "|       15|       1|       68063635|        64.9|8000000|\n",
      "|       16|       3|              0|         0.0|      0|\n",
      "|       17|       3|              0|         0.0|      0|\n",
      "|       18|       2|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/8_files_64000000_rows.parquet\"\n",
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "sdf_meta_data = get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=30, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "basic_maxSplitBytes(size, num_files)\n",
    "max_split_bytes = maxSplitBytes(size, num_files)\n",
    "file_size_list = get_files_as_list(sdf_meta_data)\n",
    "res = getFilePartitions(file_size_list, max_split_bytes)\n",
    "get_actual_num_partitions(path)\n",
    "#noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    partition_size = min(maxPartitionBytes, size_per_core_padded)\n",
    "    no_partitions_padded = file_size_padded/partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** Basic TO Calculate Max PartitionSize ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(partition_size/1024/1024, 1)} MB or {partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    max_partition_size = min(maxPartitionBytes, max(openCostInBytes, size_per_core_padded))\n",
    "    no_partitions_padded = file_size_padded/max_partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** Advanced TO Calculate Max PartitionSize ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(max_partition_size/1024/1024, 1)} MB or {max_partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")\n",
    "    return max_partition_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68008198,\n",
       " 68063741,\n",
       " 68064947,\n",
       " 68063635,\n",
       " 68063528,\n",
       " 68063118,\n",
       " 67726805,\n",
       " 68063235]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31457280"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_split_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(files_list, max_partition_size):\n",
    "    result_list = []\n",
    "    for file_size in files_list:\n",
    "        result_list.append(file_size)\n",
    "        if file_size > max_partition_size:\n",
    "            remaining_file_size\n",
    "            num_dummy_files = math.ceil(file_size/max_partition_size) - 1\n",
    "            for i in range(0, num_dummy_files):\n",
    "                result_list.append(0)\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_file_size_list = split_files(file_size_list, max_split_bytes)\n",
    "len(dummy_file_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_as_list(parquet_meta_data_sdf):\n",
    "    return list(parquet_meta_data_sdf.select(\"compressedBytes\").toPandas()[\"compressedBytes\"])\n",
    "\n",
    "def getFilePartitions(file_size_list, max_split_bytes):\n",
    "    # Reference to code in GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    partitions = []\n",
    "    current_files = []\n",
    "    current_size = 0\n",
    "\n",
    "    def close_partition():\n",
    "        if current_files:\n",
    "            partition_details = {\n",
    "                \"files\": current_files.copy(),\n",
    "                \"num_files\": len(current_files),\n",
    "                \"partition_size\": sum(current_files)\n",
    "            }\n",
    "        else:\n",
    "            partition_details = {}\n",
    "        partitions.append(partition_details)\n",
    "        current_files.clear()\n",
    "\n",
    "    for file_size in file_size_list:\n",
    "        if current_size + file_size > max_split_bytes:\n",
    "            close_partition()\n",
    "            current_size = 0\n",
    "        current_size += file_size + openCostInBytes\n",
    "        current_files.append(file_size)\n",
    "    close_partition()\n",
    "    print(f\"Number calculated Partitions: {len(partitions)}\")\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 200000 MB or 209715200000 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 200.0 MB or 209715200 bytes\n",
      "Size Per Core: 50.0 MB or 52428800.0 bytes\n",
      "Partionsize: 50.0 MB or 52428800.0 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n"
     ]
    }
   ],
   "source": [
    "file_size = 200\n",
    "set_configs(maxPartitionsMB=200000, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.799999999999997"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68.8 - 50 = 18.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.378378378378378"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50/14.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.8"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68.9*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00001-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00003-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00002-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00007-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063635       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00005-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063528       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00004-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063118       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00000-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00006-6cf07477-2127-47fc-a59f-461f5c3494a3-c000.snappy.parquet|1     |68063235       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 130 MB or 136314880 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Size Per Core: 129.7 MB or 136029301.75 bytes\n",
      "Partionsize: 129.7 MB or 136029301.75 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 68.9 MB or 72208954.875 bytes\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCore: 129.7 MB or 136029301.75 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxSplitPartitionBytes: 130.0 MB or 136314880 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 8, unrounded: 8.0\n",
      "EstimatedPartitionsInternet: 5, unrounded: 4.237773887927716\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 8\n",
      "Duration: 4.34 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       68064947|        64.9|8000000|\n",
      "|        1|       1|       68063741|        64.9|8000000|\n",
      "|        2|       1|       68063635|        64.9|8000000|\n",
      "|        3|       1|       68063528|        64.9|8000000|\n",
      "|        4|       1|       68063235|        64.9|8000000|\n",
      "|        5|       1|       68063118|        64.9|8000000|\n",
      "|        6|       1|       68008198|        64.9|8000000|\n",
      "|        7|       1|       67726805|        64.6|8000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00000-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00001-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00003-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00002-a5294f1d-0061-4db6-8982-3646992e8441-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[67726805, 68008198, 68063741, 68064947]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files_as_list(parquet_meta_data_sdf):\n",
    "    return list(parquet_meta_data_sdf.select(\"compressedBytes\").toPandas()[\"compressedBytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136128688"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68064947 + 68063741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136314880"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "130*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 8\n",
      "Saved Path: D:/Spark/Data/8_files_47000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 47000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 380.0 MB or 398415693 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 47.5 MB or 49801961.625 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 47000000\n",
      "Num rows per file: 5875000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 104 MB or 109051904 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 380.0 MB or 398415693 bytes\n",
      "Size Per Core: 95.0 MB or 99603923.25 bytes\n",
      "Partionsize: 95.0 MB or 99603923.25 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 4\n",
      "Duration: 5.36 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|       2|       99956576|        95.3|11750000|\n",
      "|        1|       2|       99707363|        95.1|11750000|\n",
      "|        2|       2|       99455921|        94.8|11750000|\n",
      "|        3|       2|       99295833|        94.7|11750000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 47000000\n",
    "path = \"D:/Spark/Data/8_files_47000000_rows.parquet\"\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=104, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "#estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n",
      "4194368.8\n",
      "[64.8]\n",
      "partition_closed\n",
      "4194368.8\n",
      "[64.8]\n",
      "partition_closed\n",
      "4194368.8\n",
      "[64.8]\n",
      "partition_closed\n",
      "partition_closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'files': [64.8], 'num_files': 1, 'partition_size': 64.8},\n",
       " {'files': [64.8], 'num_files': 1, 'partition_size': 64.8},\n",
       " {'files': [64.8], 'num_files': 1, 'partition_size': 64.8},\n",
       " {'files': [64.8], 'num_files': 1, 'partition_size': 64.8}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "getFilePartitions([64.8, 64.8, 64.8, 64.8], 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2722.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 472.0 failed 1 times, most recent failure: Lost task 0.0 in stage 472.0 (TID 16770) (DESKTOP-PNH8CDK executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:453)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor129.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:453)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 33 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2722.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 472.0 failed 1 times, most recent failure: Lost task 0.0 in stage 472.0 (TID 16770) (DESKTOP-PNH8CDK executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:453)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor129.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Das System kann die angegebene Datei nicht finden\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:453)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 33 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
