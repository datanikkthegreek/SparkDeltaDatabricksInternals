{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Discover the Secrets of influecing spark partitions during reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need this?\n",
    "- Understand how Spark creates partitioning and what influences it makes performance and debugging better\n",
    "- You learned how the number of partitions, empty partitions and distribution of data within partitions influences the performance (added the previous comments below)\n",
    "- Coalesce and especially repartition are expensive operation. If we can influence the spark partitions during loading already is a big win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n",
    "results_dict = {}\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What influences the no. of partitions when loading parquet files\n",
    "- Num Cores in the cluster,\n",
    "  - Correctly speaking it's the \"spark.sql.files.minPartitionNum\" config\n",
    "  - It defaults to the default parallism which is our num of cores = 4\n",
    "- File Size\n",
    "- Num parquet files\n",
    "- Num of blocks/rowgroups within a parquet file\n",
    "- Max Partition Size:\n",
    "  - Influences the size of a partition  \n",
    "  - based on the config\"spark.sql.files.maxPartitionBytes\"\n",
    "  - defaults to 128 MB \n",
    "- Max Cost Per Bytes\n",
    "  - Represents the cost of creating a new partition\n",
    "  - based on the config \"spark.sql.files.openCostInBytes\"\n",
    "  - defaults to 4 MB\n",
    "  - Technically it adds the cost, e.g. 4 MB, to each file which is called padding\n",
    "  - Through this less but bigger partitions are created around the size of the open cost value\n",
    "  - Usually no influence, except of smaller files, default of 4MB works\n",
    "  - Official description: The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimate, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first). This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "- https://stackoverflow.com/questions/69034543/number-of-tasks-while-reading-hdfs-in-spark\n",
    "- https://stackoverflow.com/questions/75924368/skewed-partitions-when-setting-spark-sql-files-maxpartitionbytes\n",
    "- https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "- https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finalising algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 20\n",
      "Saved Path: D:/Spark/Data/20_files_2000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00007-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68063635       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00004-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68063118       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00005-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68063528       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00000-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00001-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00003-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00006-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68063235       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00002-1ea32cf9-7663-4dd4-8f5f-3415a5abeb4c-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n"
     ]
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/8_files_64000000_rows.parquet\"\n",
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "sdf_meta_data = get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=128, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    \n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    max_partition_size = int(min(maxPartitionBytes, max(openCostInBytes, size_per_core_padded)))\n",
    "    no_partitions_padded = file_size_padded/max_partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(max_partition_size/1024/1024, 1)} MB or {max_partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitionsAvg: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")\n",
    "    return max_partition_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 526.9 MB or 552505815 bytes\n",
      "SizePerCorePadded: 131.7 MB or 138126453.75 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.116489104926586\n"
     ]
    }
   ],
   "source": [
    "max_split_bytes = maxSplitBytes(size, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_as_list(parquet_meta_data_sdf):\n",
    "    return list(parquet_meta_data_sdf.select(\"compressedBytes\").toPandas()[\"compressedBytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68063635,\n",
       " 68063118,\n",
       " 68063528,\n",
       " 67726805,\n",
       " 68008198,\n",
       " 68063741,\n",
       " 68063235,\n",
       " 68064947]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#observation... open bytes covers also slight deviations of files\n",
    "file_size_list = get_files_as_list(sdf_meta_data)\n",
    "file_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(files_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtil.scala#L45\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    for file_size in files_list:\n",
    "        remaining = file_size - max_split_bytes\n",
    "        f = {\n",
    "            \"start\": 0,\n",
    "            \"length\": min(max_split_bytes, file_size),\n",
    "            \"file_size\": file_size\n",
    "        }\n",
    "        result_list.append(f)\n",
    "        while remaining > 0:\n",
    "            if remaining > max_split_bytes:\n",
    "                f = {\n",
    "                    \"start\": file_size - remaining,\n",
    "                    \"length\": max_split_bytes,\n",
    "                    \"file_size\": 0\n",
    "                }\n",
    "                result_list.append(f)\n",
    "            else:\n",
    "                f = {\n",
    "                    \"start\": file_size - remaining,\n",
    "                    \"length\": remaining,\n",
    "                    \"file_size\": 0\n",
    "                }\n",
    "                result_list.append(f)  \n",
    "            remaining = remaining - max_split_bytes\n",
    "    return sorted(result_list, key=lambda d: d['length'], reverse=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0, 'length': 68064947, 'file_size': 68064947},\n",
       " {'start': 0, 'length': 68063741, 'file_size': 68063741},\n",
       " {'start': 0, 'length': 68063635, 'file_size': 68063635},\n",
       " {'start': 0, 'length': 68063528, 'file_size': 68063528},\n",
       " {'start': 0, 'length': 68063235, 'file_size': 68063235},\n",
       " {'start': 0, 'length': 68063118, 'file_size': 68063118},\n",
       " {'start': 0, 'length': 68008198, 'file_size': 68008198},\n",
       " {'start': 0, 'length': 67726805, 'file_size': 67726805}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "splitted_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilePartitions(splitted_files_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n",
    "    \"\"\"\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    partitions = []\n",
    "    current_files = []\n",
    "    current_size = 0\n",
    "\n",
    "    def close_partition():\n",
    "        if current_files:\n",
    "            partition_details = {\n",
    "                \"files\": current_files.copy(),\n",
    "                \"num_files\": len(current_files),\n",
    "            }\n",
    "        else:\n",
    "            partition_details = {}\n",
    "        partitions.append(partition_details)\n",
    "        current_files.clear()\n",
    "\n",
    "    for file in splitted_files_list:\n",
    "        if current_size + file[\"length\"] > max_split_bytes:\n",
    "            close_partition()\n",
    "            current_size = 0\n",
    "        current_size += file[\"length\"] + openCostInBytes\n",
    "        current_files.append(file)\n",
    "    close_partition()\n",
    "    print(f\"Number calculated Partitions: {len(partitions)}\")\n",
    "    return partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number calculated Partitions: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'files': [{'start': 0, 'length': 68064947, 'file_size': 68064947}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68063741, 'file_size': 68063741}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68063635, 'file_size': 68063635}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68063528, 'file_size': 68063528}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68063235, 'file_size': 68063235}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68063118, 'file_size': 68063118}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 68008198, 'file_size': 68008198}],\n",
       "  'num_files': 1},\n",
       " {'files': [{'start': 0, 'length': 67726805, 'file_size': 67726805}],\n",
       "  'num_files': 1}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "file_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 8\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       68064947|        64.9|8000000|\n",
      "|        1|       1|       68063741|        64.9|8000000|\n",
      "|        2|       1|       68063635|        64.9|8000000|\n",
      "|        3|       1|       68063528|        64.9|8000000|\n",
      "|        4|       1|       68063235|        64.9|8000000|\n",
      "|        5|       1|       68063118|        64.9|8000000|\n",
      "|        6|       1|       68008198|        64.9|8000000|\n",
      "|        7|       1|       67726805|        64.6|8000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actual_num_partitions(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_partitions_analysis(file_partitions):\n",
    "    pdf = pd.DataFrame(file_partitions)\n",
    "    pdf[\"partition\"] = pdf.index\n",
    "    sdf_partitions = spark.createDataFrame(pdf)\n",
    "    sdf_partitions = sdf_partitions.withColumn(\"files\", f.explode(sdf_partitions.files))\n",
    "    sdf_partitions = (sdf_partitions\n",
    "                    .withColumn(\"start\", f.col(\"files\").start)\n",
    "                    .withColumn(\"length\", f.col(\"files\").length)\n",
    "                    .withColumn(\"file_size\", f.col(\"files\").file_size)\n",
    "                    .drop(\"files\", \"num_files\")\n",
    "                    .withColumn(\"virt_num_files\", f.lit(1))\n",
    "                    .withColumn(\"real_num_files\", f.when(f.col(\"file_size\") > 0, f.lit(1)).otherwise(f.lit(0)))\n",
    "                    .select(\"partition\", \"start\", \"length\", \"file_size\", \"real_num_files\", \"virt_num_files\")\n",
    "    )\n",
    "    sdf_partitions.show()\n",
    "    sdf_agg = sdf_partitions.groupBy(\"partition\").agg(f.sum(\"file_size\")).withColumnRenamed(\"sum(file_size)\", \"file_size\")\n",
    "    sdf_agg = sdf_agg.orderBy(\"partition\")\n",
    "    sdf_agg = sdf_agg.withColumn(\"file_size_mb\", f.col(\"file_size\")/1024/1024)\n",
    "    sdf_agg.show()\n",
    "    max_size = sdf_agg.select(f.max(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    min_size = sdf_agg.select(f.min(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    median_size = sdf_agg.select(f.median(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    sdf_empty = sdf_agg.filter(f.col(\"file_size\") == 0)\n",
    "    empty_partitions = sdf_empty.count()\n",
    "    return {\n",
    "        \"max_size\": max_size,\n",
    "        \"min_size\": min_size,\n",
    "        \"median_size\": median_size,\n",
    "        \"num_partitions\": len(file_partitions),\n",
    "        \"empty_partition\": empty_partitions\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+---------+--------------+--------------+\n",
      "|partition|start|  length|file_size|real_num_files|virt_num_files|\n",
      "+---------+-----+--------+---------+--------------+--------------+\n",
      "|        0|    0|68064947| 68064947|             1|             1|\n",
      "|        1|    0|68063741| 68063741|             1|             1|\n",
      "|        2|    0|68063635| 68063635|             1|             1|\n",
      "|        3|    0|68063528| 68063528|             1|             1|\n",
      "|        4|    0|68063235| 68063235|             1|             1|\n",
      "|        5|    0|68063118| 68063118|             1|             1|\n",
      "|        6|    0|68008198| 68008198|             1|             1|\n",
      "|        7|    0|67726805| 67726805|             1|             1|\n",
      "+---------+-----+--------+---------+--------------+--------------+\n",
      "\n",
      "+---------+---------+-----------------+\n",
      "|partition|file_size|     file_size_mb|\n",
      "+---------+---------+-----------------+\n",
      "|        0| 68064947|64.91179180145264|\n",
      "|        1| 68063741|64.91064167022705|\n",
      "|        2| 68063635|64.91054058074951|\n",
      "|        3| 68063528|64.91043853759766|\n",
      "|        4| 68063235|64.91015911102295|\n",
      "|        5| 68063118|64.91004753112793|\n",
      "|        6| 68008198| 64.8576717376709|\n",
      "|        7| 67726805| 64.5893144607544|\n",
      "+---------+---------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_size': 68064947,\n",
       " 'min_size': 67726805,\n",
       " 'median_size': 68063381.5,\n",
       " 'num_partitions': 8,\n",
       " 'empty_partition': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_partitions_analysis(file_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simulation Max Partition Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 5 MB or 5242880 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 5.0 MB or 5242880 bytes\n",
      "EstimatedPartitionsAvg: 221, unrounded: 220.8\n",
      "Number calculated Partitions: 220\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 10 MB or 10485760 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 10.0 MB or 10485760 bytes\n",
      "EstimatedPartitionsAvg: 111, unrounded: 110.4\n",
      "Number calculated Partitions: 110\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 15 MB or 15728640 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 15.0 MB or 15728640 bytes\n",
      "EstimatedPartitionsAvg: 74, unrounded: 73.6\n",
      "Number calculated Partitions: 80\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 20 MB or 20971520 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 20.0 MB or 20971520 bytes\n",
      "EstimatedPartitionsAvg: 56, unrounded: 55.2\n",
      "Number calculated Partitions: 60\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 25 MB or 26214400 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 25.0 MB or 26214400 bytes\n",
      "EstimatedPartitionsAvg: 45, unrounded: 44.16\n",
      "Number calculated Partitions: 44\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 30 MB or 31457280 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitionsAvg: 37, unrounded: 36.8\n",
      "Number calculated Partitions: 40\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 40 MB or 41943040 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 40.0 MB or 41943040 bytes\n",
      "EstimatedPartitionsAvg: 28, unrounded: 27.6\n",
      "Number calculated Partitions: 30\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 50 MB or 52428800 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 50.0 MB or 52428800 bytes\n",
      "EstimatedPartitionsAvg: 23, unrounded: 22.08\n",
      "Number calculated Partitions: 22\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 60 MB or 62914560 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 60.0 MB or 62914560 bytes\n",
      "EstimatedPartitionsAvg: 19, unrounded: 18.4\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 70 MB or 73400320 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 70.0 MB or 73400320 bytes\n",
      "EstimatedPartitionsAvg: 16, unrounded: 15.771428571428572\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 80 MB or 83886080 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 80.0 MB or 83886080 bytes\n",
      "EstimatedPartitionsAvg: 14, unrounded: 13.8\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 90 MB or 94371840 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 90.0 MB or 94371840 bytes\n",
      "EstimatedPartitionsAvg: 13, unrounded: 12.266666666666667\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 100 MB or 104857600 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 100.0 MB or 104857600 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 11.04\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 110 MB or 115343360 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 110.0 MB or 115343360 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.036363636363637\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 150 MB or 157286400 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 150.0 MB or 157286400 bytes\n",
      "EstimatedPartitionsAvg: 8, unrounded: 7.36\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 200 MB or 209715200 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 200.0 MB or 209715200 bytes\n",
      "EstimatedPartitionsAvg: 6, unrounded: 5.52\n",
      "Number calculated Partitions: 7\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 256 MB or 268435456 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 256.0 MB or 268435456 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.3125\n",
      "Number calculated Partitions: 5\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 512 MB or 536870912 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 276.0 MB or 289406976 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 1024 MB or 1073741824 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 276.0 MB or 289406976 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "\n",
    "for maxMB in [5,10,15,20,25,30,40,50,60,70,80,90,100,110,128, 150,200, 256,512,1024]:\n",
    "    openCostMB = 4\n",
    "    minPartitions = 4\n",
    "    num_files = 20\n",
    "    file_size_mb = 1024\n",
    "    file_size = round(file_size_mb*1024*1024,0)\n",
    "    avg_size = int(round(file_size/num_files, 0))\n",
    "    file_size_list = [avg_size] * num_files\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"file_size_mb\"] = file_size_mb\n",
    "    results_dict[\"num_files\"] = num_files\n",
    "    results_dict[\"avg_size\"] = file_size_mb/num_files\n",
    "    results_dict[\"numCores\"] = minPartitions\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|file_size_mb|num_files|avg_size|numCores|maxPartitionMB|openCOsts|maxSplitBytes|num_partitions|empty_partition|min_size|median_size|max_size|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|        1024|       20|    51.2|       4|             5|        4|          5.0|           220|            200|     0.0|        0.0|    51.2|\n",
      "|        1024|       20|    51.2|       4|            10|        4|         10.0|           110|             90|     0.0|        0.0|    51.2|\n",
      "|        1024|       20|    51.2|       4|            15|        4|         15.0|            80|             60|     0.0|        0.0|    51.2|\n",
      "|        1024|       20|    51.2|       4|            20|        4|         20.0|            60|             40|     0.0|        0.0|    51.2|\n",
      "|        1024|       20|    51.2|       4|            25|        4|         25.0|            44|             24|     0.0|        0.0|    51.2|\n",
      "|        1024|       20|    51.2|       4|            30|        4|         30.0|            40|             20|     0.0|       25.6|    51.2|\n",
      "|        1024|       20|    51.2|       4|            40|        4|         40.0|            30|             10|     0.0|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|            50|        4|         50.0|            22|              2|     0.0|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|            60|        4|         60.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|            70|        4|         70.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|            80|        4|         80.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|            90|        4|         90.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|           100|        4|        100.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|       4|           110|        4|        110.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           150|        4|        150.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           200|        4|        200.0|             7|              0|   102.4|      153.6|   153.6|\n",
      "|        1024|       20|    51.2|       4|           256|        4|        256.0|             5|              0|   204.8|      204.8|   204.8|\n",
      "|        1024|       20|    51.2|       4|           512|        4|        276.0|             4|              0|   256.0|      256.0|   256.0|\n",
      "|        1024|       20|    51.2|       4|          1024|        4|        276.0|             4|              0|   256.0|      256.0|   256.0|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "sdf_results = spark.createDataFrame(pdf)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"file_size_mb\", \"num_files\", \"avg_size\", \"numCores\", \"maxPartitionMB\", \"openCOsts\", \"maxSplitBytes\", \"num_partitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\")\n",
    "sdf_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simulation Num Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1028.0 MB or 1077936128 bytes\n",
      "SizePerCorePadded: 257.0 MB or 269484032.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.03125\n",
      "Number calculated Partitions: 8\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1040.0 MB or 1090519040 bytes\n",
      "SizePerCorePadded: 260.0 MB or 272629760.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.125\n",
      "Number calculated Partitions: 8\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1044.0 MB or 1094713344 bytes\n",
      "SizePerCorePadded: 261.0 MB or 273678336.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.15625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1056.0 MB or 1107296256 bytes\n",
      "SizePerCorePadded: 264.0 MB or 276824064.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.25\n",
      "Number calculated Partitions: 8\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1064.0 MB or 1115684864 bytes\n",
      "SizePerCorePadded: 266.0 MB or 278921216.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.3125\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1072.0 MB or 1124073472 bytes\n",
      "SizePerCorePadded: 268.0 MB or 281018368.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.375\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1088.0 MB or 1140850688 bytes\n",
      "SizePerCorePadded: 272.0 MB or 285212672.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.5\n",
      "Number calculated Partitions: 16\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1084.0 MB or 1136656384 bytes\n",
      "SizePerCorePadded: 271.0 MB or 284164096.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.46875\n",
      "Number calculated Partitions: 15\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1120.0 MB or 1174405120 bytes\n",
      "SizePerCorePadded: 280.0 MB or 293601280.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.75\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1124.0 MB or 1178599424 bytes\n",
      "SizePerCorePadded: 281.0 MB or 294649856.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.78125\n",
      "Number calculated Partitions: 13\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1144.0 MB or 1199570944 bytes\n",
      "SizePerCorePadded: 286.0 MB or 299892736.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.9375\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1184.0 MB or 1241513984 bytes\n",
      "SizePerCorePadded: 296.0 MB or 310378496.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.25\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1224.0 MB or 1283457024 bytes\n",
      "SizePerCorePadded: 306.0 MB or 320864256.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.5625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1264.0 MB or 1325400064 bytes\n",
      "SizePerCorePadded: 316.0 MB or 331350016.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.875\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1304.0 MB or 1367343104 bytes\n",
      "SizePerCorePadded: 326.0 MB or 341835776.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.1875\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1344.0 MB or 1409286144 bytes\n",
      "SizePerCorePadded: 336.0 MB or 352321536.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.5\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1384.0 MB or 1451229184 bytes\n",
      "SizePerCorePadded: 346.0 MB or 362807296.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.8125\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1424.0 MB or 1493172224 bytes\n",
      "SizePerCorePadded: 356.0 MB or 373293056.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 11.125\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1464.0 MB or 1535115264 bytes\n",
      "SizePerCorePadded: 366.0 MB or 383778816.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 11.4375\n",
      "Number calculated Partitions: 13\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1536.0 MB or 1610612736 bytes\n",
      "SizePerCorePadded: 384.0 MB or 402653184.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 12.0\n",
      "Number calculated Partitions: 12\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1624.0 MB or 1702887424 bytes\n",
      "SizePerCorePadded: 406.0 MB or 425721856.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 13, unrounded: 12.6875\n",
      "Number calculated Partitions: 13\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1824.0 MB or 1912602624 bytes\n",
      "SizePerCorePadded: 456.0 MB or 478150656.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 15, unrounded: 14.25\n",
      "Number calculated Partitions: 15\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 2048.0 MB or 2147483648 bytes\n",
      "SizePerCorePadded: 512.0 MB or 536870912.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 16, unrounded: 16.0\n",
      "Number calculated Partitions: 16\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 3072.0 MB or 3221225472 bytes\n",
      "SizePerCorePadded: 768.0 MB or 805306368.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 24, unrounded: 24.0\n",
      "Number calculated Partitions: 24\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 5120.0 MB or 5368709120 bytes\n",
      "SizePerCorePadded: 1280.0 MB or 1342177280.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 40, unrounded: 40.0\n",
      "Number calculated Partitions: 40\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "\n",
    "for num_files in [1,4,5,8,10,12,16,15,20,24, 25,30,40,50,60,70,80,90,100,110,128, 150,200, 256,512,1024]:\n",
    "    openCostMB = 4\n",
    "    minPartitions = 4\n",
    "    maxMB = 128\n",
    "    file_size_mb = 1024\n",
    "    file_size = round(file_size_mb*1024*1024,0)\n",
    "    avg_size = int(round(file_size/num_files, 0))\n",
    "    file_size_list = [avg_size] * num_files\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"file_size_mb\"] = file_size_mb\n",
    "    results_dict[\"num_files\"] = num_files\n",
    "    results_dict[\"avg_size\"] = file_size_mb/num_files\n",
    "    results_dict[\"numCores\"] = minPartitions\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|file_size_mb|num_files|          avg_size|numCores|maxPartitionMB|openCOsts|maxSplitBytes|num_partitions|empty_partition|min_size|median_size|max_size|\n",
      "+------------+---------+------------------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|        1024|        1|            1024.0|       4|           128|        4|        128.0|             8|              7|     0.0|        0.0|  1024.0|\n",
      "|        1024|        4|             256.0|       4|           128|        4|        128.0|             8|              4|     0.0|      128.0|   256.0|\n",
      "|        1024|        5|             204.8|       4|           128|        4|        128.0|            10|              5|     0.0|      102.4|   204.8|\n",
      "|        1024|        8|             128.0|       4|           128|        4|        128.0|             8|              0|   128.0|      128.0|   128.0|\n",
      "|        1024|       10|             102.4|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       12| 85.33333333333333|       4|           128|        4|        128.0|            12|              0|    85.3|       85.3|    85.3|\n",
      "|        1024|       16|              64.0|       4|           128|        4|        128.0|            16|              0|    64.0|       64.0|    64.0|\n",
      "|        1024|       15| 68.26666666666667|       4|           128|        4|        128.0|            15|              0|    68.3|       68.3|    68.3|\n",
      "|        1024|       20|              51.2|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       24|42.666666666666664|       4|           128|        4|        128.0|            12|              0|    85.3|       85.3|    85.3|\n",
      "|        1024|       25|             40.96|       4|           128|        4|        128.0|            13|              0|    41.0|       81.9|    81.9|\n",
      "|        1024|       30| 34.13333333333333|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       40|              25.6|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       50|             20.48|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       60|17.066666666666666|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       70|14.628571428571428|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       80|              12.8|       4|           128|        4|        128.0|            12|              0|    38.4|       89.6|    89.6|\n",
      "|        1024|       90|11.377777777777778|       4|           128|        4|        128.0|            12|              0|    22.8|       91.0|    91.0|\n",
      "|        1024|      100|             10.24|       4|           128|        4|        128.0|            12|              0|    10.2|       92.2|    92.2|\n",
      "|        1024|      110| 9.309090909090909|       4|           128|        4|        128.0|            13|              0|    18.6|       83.8|    83.8|\n",
      "+------------+---------+------------------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_results = spark.createDataFrame(results)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"file_size_mb\", \"num_files\", \"avg_size\", \"numCores\", \"maxPartitionMB\", \"openCOsts\", \"maxSplitBytes\", \"num_partitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\")\n",
    "sdf_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simulation File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 80.0 MB or 83886185.0 bytes\n",
      "SizePerCorePadded: 20.0 MB or 20971546.25 bytes\n",
      "MaxPartionsize: 20.0 MB or 20971546 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.000000047683657\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 80.0 MB or 83887129.0 bytes\n",
      "SizePerCorePadded: 20.0 MB or 20971782.25 bytes\n",
      "MaxPartionsize: 20.0 MB or 20971782 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.00000004768312\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 80.0 MB or 83896566.0 bytes\n",
      "SizePerCorePadded: 20.0 MB or 20974141.5 bytes\n",
      "MaxPartionsize: 20.0 MB or 20974141 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.000000095355515\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 80.1 MB or 83990938.0 bytes\n",
      "SizePerCorePadded: 20.0 MB or 20997734.5 bytes\n",
      "MaxPartionsize: 20.0 MB or 20997734 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.000000095248373\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 81.0 MB or 84934656 bytes\n",
      "SizePerCorePadded: 20.2 MB or 21233664.0 bytes\n",
      "MaxPartionsize: 20.2 MB or 21233664 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 84.0 MB or 88080384 bytes\n",
      "SizePerCorePadded: 21.0 MB or 22020096.0 bytes\n",
      "MaxPartionsize: 21.0 MB or 22020096 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 88.0 MB or 92274688 bytes\n",
      "SizePerCorePadded: 22.0 MB or 23068672.0 bytes\n",
      "MaxPartionsize: 22.0 MB or 23068672 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 90.0 MB or 94371840 bytes\n",
      "SizePerCorePadded: 22.5 MB or 23592960.0 bytes\n",
      "MaxPartionsize: 22.5 MB or 23592960 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 100.0 MB or 104857600 bytes\n",
      "SizePerCorePadded: 25.0 MB or 26214400.0 bytes\n",
      "MaxPartionsize: 25.0 MB or 26214400 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 120.0 MB or 125829120 bytes\n",
      "SizePerCorePadded: 30.0 MB or 31457280.0 bytes\n",
      "MaxPartionsize: 30.0 MB or 31457280 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 130.0 MB or 136314880 bytes\n",
      "SizePerCorePadded: 32.5 MB or 34078720.0 bytes\n",
      "MaxPartionsize: 32.5 MB or 34078720 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 180.0 MB or 188743680 bytes\n",
      "SizePerCorePadded: 45.0 MB or 47185920.0 bytes\n",
      "MaxPartionsize: 45.0 MB or 47185920 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 208.0 MB or 218103808 bytes\n",
      "SizePerCorePadded: 52.0 MB or 54525952.0 bytes\n",
      "MaxPartionsize: 52.0 MB or 54525952 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 336.0 MB or 352321536 bytes\n",
      "SizePerCorePadded: 84.0 MB or 88080384.0 bytes\n",
      "MaxPartionsize: 84.0 MB or 88080384 bytes\n",
      "EstimatedPartitionsAvg: 4, unrounded: 4.0\n",
      "Number calculated Partitions: 4\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 592.0 MB or 620756992 bytes\n",
      "SizePerCorePadded: 148.0 MB or 155189248.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 5, unrounded: 4.625\n",
      "Number calculated Partitions: 5\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 2128.0 MB or 2231369728 bytes\n",
      "SizePerCorePadded: 532.0 MB or 557842432.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 17, unrounded: 16.625\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 4176.0 MB or 4378853376 bytes\n",
      "SizePerCorePadded: 1044.0 MB or 1094713344.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 33, unrounded: 32.625\n",
      "Number calculated Partitions: 40\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "\n",
    "for file_size_mb in [0.0001,0.001,0.01, 0.1, 1, 4, 8, 10, 20, 40, 50, 100, 128, 256, 512, 1024, 2048, 4096]: \n",
    "    openCostMB = 4\n",
    "    minPartitions = 4\n",
    "    maxMB = 128\n",
    "    num_files = 20\n",
    "    file_size = round(file_size_mb*1024*1024,0)\n",
    "    avg_size = int(round(file_size/num_files, 0))\n",
    "    file_size_list = [avg_size] * num_files\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"file_size_mb\"] = file_size_mb\n",
    "    results_dict[\"num_files\"] = num_files\n",
    "    results_dict[\"avg_size\"] = file_size_mb/num_files\n",
    "    results_dict[\"numCores\"] = minPartitions\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|file_size_mb|num_files|avg_size|numCores|maxPartitionMB|openCOsts|maxSplitBytes|num_partitions|empty_partition|min_size|median_size|max_size|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|      1.0E-4|       20|  5.0E-6|       4|           128|        4|         20.0|             4|              0|     0.0|        0.0|     0.0|\n",
      "|       0.001|       20|  5.0E-5|       4|           128|        4|         20.0|             4|              0|     0.0|        0.0|     0.0|\n",
      "|        0.01|       20|  5.0E-4|       4|           128|        4|         20.0|             4|              0|     0.0|        0.0|     0.0|\n",
      "|         0.1|       20|   0.005|       4|           128|        4|         20.0|             4|              0|     0.0|        0.0|     0.0|\n",
      "|         1.0|       20|    0.05|       4|           128|        4|         20.2|             4|              0|     0.3|        0.3|     0.3|\n",
      "|         4.0|       20|     0.2|       4|           128|        4|         21.0|             4|              0|     1.0|        1.0|     1.0|\n",
      "|         8.0|       20|     0.4|       4|           128|        4|         22.0|             4|              0|     2.0|        2.0|     2.0|\n",
      "|        10.0|       20|     0.5|       4|           128|        4|         22.5|             4|              0|     2.5|        2.5|     2.5|\n",
      "|        20.0|       20|     1.0|       4|           128|        4|         25.0|             4|              0|     5.0|        5.0|     5.0|\n",
      "|        40.0|       20|     2.0|       4|           128|        4|         30.0|             4|              0|    10.0|       10.0|    10.0|\n",
      "|        50.0|       20|     2.5|       4|           128|        4|         32.5|             4|              0|    12.5|       12.5|    12.5|\n",
      "|       100.0|       20|     5.0|       4|           128|        4|         45.0|             4|              0|    25.0|       25.0|    25.0|\n",
      "|       128.0|       20|     6.4|       4|           128|        4|         52.0|             4|              0|    32.0|       32.0|    32.0|\n",
      "|       256.0|       20|    12.8|       4|           128|        4|         84.0|             4|              0|    64.0|       64.0|    64.0|\n",
      "|       512.0|       20|    25.6|       4|           128|        4|        128.0|             5|              0|   102.4|      102.4|   102.4|\n",
      "|      1024.0|       20|    51.2|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|      2048.0|       20|   102.4|       4|           128|        4|        128.0|            20|              0|   102.4|      102.4|   102.4|\n",
      "|      4096.0|       20|   204.8|       4|           128|        4|        128.0|            40|             20|     0.0|      102.4|   204.8|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "sdf_results = spark.createDataFrame(pdf)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"file_size_mb\", \"num_files\", \"avg_size\", \"numCores\", \"maxPartitionMB\", \"openCOsts\", \"maxSplitBytes\", \"num_partitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\")\n",
    "sdf_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Simulation Open Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 0 MB or 0 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1024.0 MB or 1073741824 bytes\n",
      "SizePerCorePadded: 256.0 MB or 268435456.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 8, unrounded: 8.0\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 0.125 MB or 131072 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1026.5 MB or 1076363264 bytes\n",
      "SizePerCorePadded: 256.6 MB or 269090816.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.01953125\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 0.25 MB or 262144 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1029.0 MB or 1078984704 bytes\n",
      "SizePerCorePadded: 257.2 MB or 269746176.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.0390625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 0.5 MB or 524288 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1034.0 MB or 1084227584 bytes\n",
      "SizePerCorePadded: 258.5 MB or 271056896.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.078125\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 0.75 MB or 786432 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1039.0 MB or 1089470464 bytes\n",
      "SizePerCorePadded: 259.8 MB or 272367616.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.1171875\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 1 MB or 1048576 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1044.0 MB or 1094713344 bytes\n",
      "SizePerCorePadded: 261.0 MB or 273678336.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.15625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 2 MB or 2097152 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1064.0 MB or 1115684864 bytes\n",
      "SizePerCorePadded: 266.0 MB or 278921216.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.3125\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 3 MB or 3145728 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1084.0 MB or 1136656384 bytes\n",
      "SizePerCorePadded: 271.0 MB or 284164096.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.46875\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 6 MB or 6291456 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1144.0 MB or 1199570944 bytes\n",
      "SizePerCorePadded: 286.0 MB or 299892736.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.9375\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 8 MB or 8388608 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1184.0 MB or 1241513984 bytes\n",
      "SizePerCorePadded: 296.0 MB or 310378496.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.25\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 10 MB or 10485760 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1224.0 MB or 1283457024 bytes\n",
      "SizePerCorePadded: 306.0 MB or 320864256.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.5625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 12 MB or 12582912 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1264.0 MB or 1325400064 bytes\n",
      "SizePerCorePadded: 316.0 MB or 331350016.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 10, unrounded: 9.875\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 16 MB or 16777216 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1344.0 MB or 1409286144 bytes\n",
      "SizePerCorePadded: 336.0 MB or 352321536.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.5\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 20 MB or 20971520 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1424.0 MB or 1493172224 bytes\n",
      "SizePerCorePadded: 356.0 MB or 373293056.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 11.125\n",
      "Number calculated Partitions: 10\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "\n",
    "for openCostMB in [0,0.125, 0.25, 0.5, 0.75, 1,2,3,4,6,8,10,12,16,20]: \n",
    "    minPartitions = 4\n",
    "    maxMB = 128\n",
    "    num_files = 20\n",
    "    file_size_mb = 1024\n",
    "    file_size = round(file_size_mb*1024*1024,0)\n",
    "    avg_size = int(round(file_size/num_files, 0))\n",
    "    file_size_list = [avg_size] * num_files\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"file_size_mb\"] = file_size_mb\n",
    "    results_dict[\"num_files\"] = num_files\n",
    "    results_dict[\"avg_size\"] = file_size_mb/num_files\n",
    "    results_dict[\"numCores\"] = minPartitions\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|file_size_mb|num_files|avg_size|numCores|maxPartitionMB|openCOsts|maxSplitBytes|num_partitions|empty_partition|min_size|median_size|max_size|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|        1024|       20|    51.2|       4|           128|      0.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|    0.125|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     0.25|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      0.5|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     0.75|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      1.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      2.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      3.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      4.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      6.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|      8.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     10.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     12.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     16.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|     20.0|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "sdf_results = spark.createDataFrame(pdf)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"file_size_mb\", \"num_files\", \"avg_size\", \"numCores\", \"maxPartitionMB\", \"openCOsts\", \"maxSplitBytes\", \"num_partitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\")\n",
    "sdf_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Simulation Min partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 1\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 1104.0 MB or 1157627904.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 2\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 552.0 MB or 578813952.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 276.0 MB or 289406976.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 8\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 138.0 MB or 144703488.0 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 9, unrounded: 8.625\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 10\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 110.4 MB or 115762790.4 bytes\n",
      "MaxPartionsize: 110.4 MB or 115762790 bytes\n",
      "EstimatedPartitionsAvg: 11, unrounded: 10.000000034553418\n",
      "Number calculated Partitions: 10\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 12\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 92.0 MB or 96468992.0 bytes\n",
      "MaxPartionsize: 92.0 MB or 96468992 bytes\n",
      "EstimatedPartitionsAvg: 12, unrounded: 12.0\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 16\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 69.0 MB or 72351744.0 bytes\n",
      "MaxPartionsize: 69.0 MB or 72351744 bytes\n",
      "EstimatedPartitionsAvg: 16, unrounded: 16.0\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 20\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 55.2 MB or 57881395.2 bytes\n",
      "MaxPartionsize: 55.2 MB or 57881395 bytes\n",
      "EstimatedPartitionsAvg: 21, unrounded: 20.000000069106836\n",
      "Number calculated Partitions: 20\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 24\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 46.0 MB or 48234496.0 bytes\n",
      "MaxPartionsize: 46.0 MB or 48234496 bytes\n",
      "EstimatedPartitionsAvg: 24, unrounded: 24.0\n",
      "Number calculated Partitions: 24\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 28\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 39.4 MB or 41343853.71428572 bytes\n",
      "MaxPartionsize: 39.4 MB or 41343853 bytes\n",
      "EstimatedPartitionsAvg: 29, unrounded: 28.00000048374785\n",
      "Number calculated Partitions: 30\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 32\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 1104.0 MB or 1157627904 bytes\n",
      "SizePerCorePadded: 34.5 MB or 36175872.0 bytes\n",
      "MaxPartionsize: 34.5 MB or 36175872 bytes\n",
      "EstimatedPartitionsAvg: 32, unrounded: 32.0\n",
      "Number calculated Partitions: 40\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "\n",
    "for minPartitions in [1,2,4,8,10,12,16,20,24,28,32]: \n",
    "    openCostMB = 4\n",
    "    maxMB = 128\n",
    "    num_files = 20\n",
    "    file_size_mb = 1024\n",
    "    file_size = round(file_size_mb*1024*1024,0)\n",
    "    avg_size = int(round(file_size/num_files, 0))\n",
    "    file_size_list = [avg_size] * num_files\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"file_size_mb\"] = file_size_mb\n",
    "    results_dict[\"num_files\"] = num_files\n",
    "    results_dict[\"avg_size\"] = file_size_mb/num_files\n",
    "    results_dict[\"numCores\"] = minPartitions\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|file_size_mb|num_files|avg_size|numCores|maxPartitionMB|openCOsts|maxSplitBytes|num_partitions|empty_partition|min_size|median_size|max_size|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "|        1024|       20|    51.2|       1|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       2|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       4|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|       8|           128|        4|        128.0|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|      10|           128|        4|        110.4|            10|              0|   102.4|      102.4|   102.4|\n",
      "|        1024|       20|    51.2|      12|           128|        4|         92.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|      16|           128|        4|         69.0|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|      20|           128|        4|         55.2|            20|              0|    51.2|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|      24|           128|        4|         46.0|            24|              4|     0.0|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|      28|           128|        4|         39.4|            30|             10|     0.0|       51.2|    51.2|\n",
      "|        1024|       20|    51.2|      32|           128|        4|         34.5|            40|             20|     0.0|       25.6|    51.2|\n",
      "+------------+---------+--------+--------+--------------+---------+-------------+--------------+---------------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_results = spark.createDataFrame(results)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"file_size_mb\", \"num_files\", \"avg_size\", \"numCores\", \"maxPartitionMB\", \"openCOsts\", \"maxSplitBytes\", \"num_partitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\")\n",
    "sdf_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
