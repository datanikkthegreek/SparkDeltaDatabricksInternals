{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Spark Repartition - How it works and how to screw it up\n",
    "\n",
    "1. Recap: How Repartitioning works\n",
    "2. Round Robin Partitioning\n",
    "3. Hash Partitioning\n",
    "4. Range Partitioning\n",
    "5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        .withColumn(\"idfirsttwo\", f.col(\"idstring\").substr(0,2))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|\n",
      "+---+----------+--------------------+--------+-------+------+----------+\n",
      "|  0|2024-01-31|2024-01-31 23:09:...|       0|      0|     0|         0|\n",
      "|  1|2024-01-31|2024-01-31 23:09:...|       1|      1|     1|         1|\n",
      "|  2|2024-01-31|2024-01-31 23:09:...|       2|      2|     2|         2|\n",
      "|  3|2024-01-31|2024-01-31 23:09:...|       3|      3|     3|         3|\n",
      "|  4|2024-01-31|2024-01-31 23:09:...|       4|      4|     4|         4|\n",
      "|  5|2024-01-31|2024-01-31 23:09:...|       5|      5|     5|         5|\n",
      "|  6|2024-01-31|2024-01-31 23:09:...|       6|      6|     6|         6|\n",
      "|  7|2024-01-31|2024-01-31 23:09:...|       7|      7|     7|         7|\n",
      "|  8|2024-01-31|2024-01-31 23:09:...|       8|      8|     8|         8|\n",
      "|  9|2024-01-31|2024-01-31 23:09:...|       9|      9|     9|         9|\n",
      "| 10|2024-01-31|2024-01-31 23:09:...|      10|      1|     0|        10|\n",
      "| 11|2024-01-31|2024-01-31 23:09:...|      11|      1|     1|        11|\n",
      "| 12|2024-01-31|2024-01-31 23:09:...|      12|      1|     2|        12|\n",
      "| 13|2024-01-31|2024-01-31 23:09:...|      13|      1|     3|        13|\n",
      "| 14|2024-01-31|2024-01-31 23:09:...|      14|      1|     4|        14|\n",
      "| 15|2024-01-31|2024-01-31 23:09:...|      15|      1|     5|        15|\n",
      "| 16|2024-01-31|2024-01-31 23:09:...|      16|      1|     6|        16|\n",
      "| 17|2024-01-31|2024-01-31 23:09:...|      17|      1|     7|        17|\n",
      "| 18|2024-01-31|2024-01-31 23:09:...|      18|      1|     8|        18|\n",
      "| 19|2024-01-31|2024-01-31 23:09:...|      19|      1|     9|        19|\n",
      "+---+----------+--------------------+--------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(20, 4)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\")\n",
    "    sdf_part_count.show()\n",
    "    return sdf_part_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", col: str) -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count = sdf_part_count.orderBy(\"partition_id\", col)\n",
    "    sdf_part_count.show()\n",
    "    return sdf_part_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - How repartitioning works\n",
    "- Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html#pyspark.sql.DataFrame.repartition\n",
    "- Repartition allows to increase and decrease the number of partitions\n",
    "- Repartition requires shuffling data which can be more unefficient than Coalesce\n",
    "- On the other hand it creates uniform distributions unlike coalesce which only unions partions together\n",
    "- Instead of partition based on the number of partitions you can partition based on a column. This uses Hash Partitioning instead of Round Robin\n",
    "- If no number of partitions is defined the default value depends on spark.sql.shuffle.partitions which defaults to 200 (important later when evaluating wide transformations in later episodes)\n",
    "\n",
    "https://stackoverflow.com/questions/65809909/spark-what-is-the-difference-between-repartition-and-repartitionbyrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Round Robin Partitioning\n",
    "- We define the number of target partitions we want to receive as an output\n",
    "- Per source partition we do the following:\n",
    "    - We choose a random target partition (range 0 to number target partitions)\n",
    "    - Row by row we assign the next target partition, e.g. partition 4, 5, 6, 7, 8, 9, 0 etc.\n",
    "- How to screw it up: \n",
    "    - For a small number of values per partition some partitions might be get assigned less or no values\n",
    "    - This of course only happens rarely with no target partitions bigger than values per source partition as e.g. if you have 5 values per partition with 4 in total and increase to 10 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---------------+\n",
      "|partition_id|count|     count_perc|\n",
      "+------------+-----+---------------+\n",
      "|           1| 5001|25.002499750025|\n",
      "|           3| 5001|25.002499750025|\n",
      "|           2| 5000|24.997500249975|\n",
      "|           0| 5000|24.997500249975|\n",
      "+------------+-----+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = sdf_generator(20002, 4)\n",
    "sdf = sdf.withColumn(\"partition_id_before\", f.spark_partition_id())\n",
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------------------+\n",
      "|partition_id|count|        count_perc|\n",
      "+------------+-----+------------------+\n",
      "|           1| 2000|     9.99900009999|\n",
      "|           6| 2000|     9.99900009999|\n",
      "|           3| 2000|     9.99900009999|\n",
      "|           5| 2000|     9.99900009999|\n",
      "|           9| 2000|     9.99900009999|\n",
      "|           4| 2001|10.003999600039997|\n",
      "|           8| 2001|10.003999600039997|\n",
      "|           7| 2000|     9.99900009999|\n",
      "|           2| 2000|     9.99900009999|\n",
      "|           0| 2000|     9.99900009999|\n",
      "+------------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_part = sdf.repartition(10)\n",
    "rows_per_partition(sdf_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "|  0|2024-01-31|2024-01-31 23:17:...|       0|      0|     0|         0|                  0|\n",
      "|  1|2024-01-31|2024-01-31 23:17:...|       1|      1|     1|         1|                  0|\n",
      "|  2|2024-01-31|2024-01-31 23:17:...|       2|      2|     2|         2|                  0|\n",
      "|  3|2024-01-31|2024-01-31 23:17:...|       3|      3|     3|         3|                  0|\n",
      "|  4|2024-01-31|2024-01-31 23:17:...|       4|      4|     4|         4|                  0|\n",
      "|  5|2024-01-31|2024-01-31 23:17:...|       5|      5|     5|         5|                  1|\n",
      "|  6|2024-01-31|2024-01-31 23:17:...|       6|      6|     6|         6|                  1|\n",
      "|  7|2024-01-31|2024-01-31 23:17:...|       7|      7|     7|         7|                  1|\n",
      "|  8|2024-01-31|2024-01-31 23:17:...|       8|      8|     8|         8|                  1|\n",
      "|  9|2024-01-31|2024-01-31 23:17:...|       9|      9|     9|         9|                  1|\n",
      "| 10|2024-01-31|2024-01-31 23:17:...|      10|      1|     0|        10|                  2|\n",
      "| 11|2024-01-31|2024-01-31 23:17:...|      11|      1|     1|        11|                  2|\n",
      "| 12|2024-01-31|2024-01-31 23:17:...|      12|      1|     2|        12|                  2|\n",
      "| 13|2024-01-31|2024-01-31 23:17:...|      13|      1|     3|        13|                  2|\n",
      "| 14|2024-01-31|2024-01-31 23:17:...|      14|      1|     4|        14|                  2|\n",
      "| 15|2024-01-31|2024-01-31 23:17:...|      15|      1|     5|        15|                  3|\n",
      "| 16|2024-01-31|2024-01-31 23:17:...|      16|      1|     6|        16|                  3|\n",
      "| 17|2024-01-31|2024-01-31 23:17:...|      17|      1|     7|        17|                  3|\n",
      "| 18|2024-01-31|2024-01-31 23:17:...|      18|      1|     8|        18|                  3|\n",
      "| 19|2024-01-31|2024-01-31 23:17:...|      19|      1|     9|        19|                  3|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|partition_id|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "|  4|2024-01-31|2024-01-31 23:17:...|       4|      4|     4|         4|                  0|           0|\n",
      "|  0|2024-01-31|2024-01-31 23:17:...|       0|      0|     0|         0|                  0|           1|\n",
      "|  3|2024-01-31|2024-01-31 23:17:...|       3|      3|     3|         3|                  0|           2|\n",
      "|  1|2024-01-31|2024-01-31 23:17:...|       1|      1|     1|         1|                  0|           3|\n",
      "|  2|2024-01-31|2024-01-31 23:17:...|       2|      2|     2|         2|                  0|           9|\n",
      "|  8|2024-01-31|2024-01-31 23:17:...|       8|      8|     8|         8|                  1|           4|\n",
      "|  5|2024-01-31|2024-01-31 23:17:...|       5|      5|     5|         5|                  1|           5|\n",
      "|  6|2024-01-31|2024-01-31 23:17:...|       6|      6|     6|         6|                  1|           6|\n",
      "|  7|2024-01-31|2024-01-31 23:17:...|       7|      7|     7|         7|                  1|           7|\n",
      "|  9|2024-01-31|2024-01-31 23:17:...|       9|      9|     9|         9|                  1|           8|\n",
      "| 14|2024-01-31|2024-01-31 23:17:...|      14|      1|     4|        14|                  2|           0|\n",
      "| 12|2024-01-31|2024-01-31 23:17:...|      12|      1|     2|        12|                  2|           1|\n",
      "| 10|2024-01-31|2024-01-31 23:17:...|      10|      1|     0|        10|                  2|           2|\n",
      "| 11|2024-01-31|2024-01-31 23:17:...|      11|      1|     1|        11|                  2|           3|\n",
      "| 13|2024-01-31|2024-01-31 23:17:...|      13|      1|     3|        13|                  2|           9|\n",
      "| 19|2024-01-31|2024-01-31 23:17:...|      19|      1|     9|        19|                  3|           0|\n",
      "| 18|2024-01-31|2024-01-31 23:17:...|      18|      1|     8|        18|                  3|           1|\n",
      "| 15|2024-01-31|2024-01-31 23:17:...|      15|      1|     5|        15|                  3|           2|\n",
      "| 16|2024-01-31|2024-01-31 23:17:...|      16|      1|     6|        16|                  3|           8|\n",
      "| 17|2024-01-31|2024-01-31 23:17:...|      17|      1|     7|        17|                  3|           9|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part = sdf_part.withColumn(\"partition_id\", f.spark_partition_id()).orderBy(\"partition_id_before\")\n",
    "sdf_part.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Hash Partitioning\n",
    "- How it works:\n",
    "    - During repartitioning each value of the column to be rapartitioned by is hashed to an integer\n",
    "    - Spark uses the Murmur3_x86_32 \n",
    "    - Applying Module % num_target_partitions results into returning the assigned target partition\n",
    "- How to screw it up:\n",
    "    - If you are unlucky the values you have in a column don't distribute the same way. E.g. you have 10 distinct values but only 4 output partitions\n",
    "    - Especially for a low distinct value range this might be the case\n",
    "    - Increasing the number of rows per partition does not help\n",
    "    - adjusting the num of target partitions influences the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "|  0|2024-01-31|2024-01-31 23:28:...|       0|      0|     0|         0|                  0|\n",
      "|  1|2024-01-31|2024-01-31 23:28:...|       1|      1|     1|         1|                  0|\n",
      "|  2|2024-01-31|2024-01-31 23:28:...|       2|      2|     2|         2|                  0|\n",
      "|  3|2024-01-31|2024-01-31 23:28:...|       3|      3|     3|         3|                  0|\n",
      "|  4|2024-01-31|2024-01-31 23:28:...|       4|      4|     4|         4|                  0|\n",
      "|  5|2024-01-31|2024-01-31 23:28:...|       5|      5|     5|         5|                  1|\n",
      "|  6|2024-01-31|2024-01-31 23:28:...|       6|      6|     6|         6|                  1|\n",
      "|  7|2024-01-31|2024-01-31 23:28:...|       7|      7|     7|         7|                  1|\n",
      "|  8|2024-01-31|2024-01-31 23:28:...|       8|      8|     8|         8|                  1|\n",
      "|  9|2024-01-31|2024-01-31 23:28:...|       9|      9|     9|         9|                  1|\n",
      "| 10|2024-01-31|2024-01-31 23:28:...|      10|      1|     0|        10|                  2|\n",
      "| 11|2024-01-31|2024-01-31 23:28:...|      11|      1|     1|        11|                  2|\n",
      "| 12|2024-01-31|2024-01-31 23:28:...|      12|      1|     2|        12|                  2|\n",
      "| 13|2024-01-31|2024-01-31 23:28:...|      13|      1|     3|        13|                  2|\n",
      "| 14|2024-01-31|2024-01-31 23:28:...|      14|      1|     4|        14|                  2|\n",
      "| 15|2024-01-31|2024-01-31 23:28:...|      15|      1|     5|        15|                  3|\n",
      "| 16|2024-01-31|2024-01-31 23:28:...|      16|      1|     6|        16|                  3|\n",
      "| 17|2024-01-31|2024-01-31 23:28:...|      17|      1|     7|        17|                  3|\n",
      "| 18|2024-01-31|2024-01-31 23:28:...|      18|      1|     8|        18|                  3|\n",
      "| 19|2024-01-31|2024-01-31 23:28:...|      19|      1|     9|        19|                  3|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(20, 4)\n",
    "sdf = sdf.withColumn(\"partition_id_before\", f.spark_partition_id())\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           1|    5|      25.0|\n",
      "|           3|    5|      25.0|\n",
      "|           2|    5|      25.0|\n",
      "|           0|    5|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           1|    4|      20.0|\n",
      "|           3|    4|      20.0|\n",
      "|           4|    8|      40.0|\n",
      "|           0|    4|      20.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_part = sdf.repartition(5, \"idlast\")\n",
    "rows_per_partition(sdf_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|partition_id|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "|  0|2024-01-31|2024-01-31 23:00:...|       0|      0|     0|         0|                  0|           0|\n",
      "|  1|2024-01-31|2024-01-31 23:00:...|       1|      1|     1|         1|                  0|           4|\n",
      "|  2|2024-01-31|2024-01-31 23:00:...|       2|      2|     2|         2|                  0|           4|\n",
      "|  3|2024-01-31|2024-01-31 23:00:...|       3|      3|     3|         3|                  0|           3|\n",
      "|  4|2024-01-31|2024-01-31 23:00:...|       4|      4|     4|         4|                  0|           1|\n",
      "|  5|2024-01-31|2024-01-31 23:00:...|       5|      5|     5|         5|                  1|           4|\n",
      "|  6|2024-01-31|2024-01-31 23:00:...|       6|      6|     6|         6|                  1|           0|\n",
      "|  7|2024-01-31|2024-01-31 23:00:...|       7|      7|     7|         7|                  1|           3|\n",
      "|  8|2024-01-31|2024-01-31 23:00:...|       8|      8|     8|         8|                  1|           1|\n",
      "|  9|2024-01-31|2024-01-31 23:00:...|       9|      9|     9|         9|                  1|           4|\n",
      "| 10|2024-01-31|2024-01-31 23:00:...|      10|      1|     0|        10|                  2|           0|\n",
      "| 11|2024-01-31|2024-01-31 23:00:...|      11|      1|     1|        11|                  2|           4|\n",
      "| 12|2024-01-31|2024-01-31 23:00:...|      12|      1|     2|        12|                  2|           4|\n",
      "| 13|2024-01-31|2024-01-31 23:00:...|      13|      1|     3|        13|                  2|           3|\n",
      "| 14|2024-01-31|2024-01-31 23:00:...|      14|      1|     4|        14|                  2|           1|\n",
      "| 15|2024-01-31|2024-01-31 23:00:...|      15|      1|     5|        15|                  3|           4|\n",
      "| 16|2024-01-31|2024-01-31 23:00:...|      16|      1|     6|        16|                  3|           0|\n",
      "| 17|2024-01-31|2024-01-31 23:00:...|      17|      1|     7|        17|                  3|           3|\n",
      "| 18|2024-01-31|2024-01-31 23:00:...|      18|      1|     8|        18|                  3|           1|\n",
      "| 19|2024-01-31|2024-01-31 23:00:...|      19|      1|     9|        19|                  3|           4|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part = sdf_part.withColumn(\"partition_id\", f.spark_partition_id()).orderBy(\"id\")\n",
    "sdf_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+-----------+----+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|       hash|part|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+-----------+----+\n",
      "|  0|2024-01-31|2024-01-31 23:25:...|       0|      0|     0|         0|                  0|  735846435|   0|\n",
      "|  1|2024-01-31|2024-01-31 23:25:...|       1|      1|     1|         1|                  0| 1625004744|   4|\n",
      "|  2|2024-01-31|2024-01-31 23:25:...|       2|      2|     2|         2|                  0|  870267989|   4|\n",
      "|  3|2024-01-31|2024-01-31 23:25:...|       3|      3|     3|         3|                  0|-1756013582|   2|\n",
      "|  4|2024-01-31|2024-01-31 23:25:...|       4|      4|     4|         4|                  0|-2142269034|   4|\n",
      "|  5|2024-01-31|2024-01-31 23:25:...|       5|      5|     5|         5|                  1|  135093849|   4|\n",
      "|  6|2024-01-31|2024-01-31 23:25:...|       6|      6|     6|         6|                  1|-1929623325|   0|\n",
      "|  7|2024-01-31|2024-01-31 23:25:...|       7|      7|     7|         7|                  1|  141897003|   3|\n",
      "|  8|2024-01-31|2024-01-31 23:25:...|       8|      8|     8|         8|                  1|-1276280174|   4|\n",
      "|  9|2024-01-31|2024-01-31 23:25:...|       9|      9|     9|         9|                  1|  541481139|   4|\n",
      "| 10|2024-01-31|2024-01-31 23:25:...|      10|      1|     0|        10|                  2|  735846435|   0|\n",
      "| 11|2024-01-31|2024-01-31 23:25:...|      11|      1|     1|        11|                  2| 1625004744|   4|\n",
      "| 12|2024-01-31|2024-01-31 23:25:...|      12|      1|     2|        12|                  2|  870267989|   4|\n",
      "| 13|2024-01-31|2024-01-31 23:25:...|      13|      1|     3|        13|                  2|-1756013582|   2|\n",
      "| 14|2024-01-31|2024-01-31 23:25:...|      14|      1|     4|        14|                  2|-2142269034|   4|\n",
      "| 15|2024-01-31|2024-01-31 23:25:...|      15|      1|     5|        15|                  3|  135093849|   4|\n",
      "| 16|2024-01-31|2024-01-31 23:25:...|      16|      1|     6|        16|                  3|-1929623325|   0|\n",
      "| 17|2024-01-31|2024-01-31 23:25:...|      17|      1|     7|        17|                  3|  141897003|   3|\n",
      "| 18|2024-01-31|2024-01-31 23:25:...|      18|      1|     8|        18|                  3|-1276280174|   4|\n",
      "| 19|2024-01-31|2024-01-31 23:25:...|      19|      1|     9|        19|                  3|  541481139|   4|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_hash = sdf.withColumn(\"hash\", f.hash(\"idlast\"))\n",
    "sdf_hash = sdf_hash.withColumn(\"part\", f.abs(f.col(\"hash\") % 5))\n",
    "sdf_hash = sdf_hash.orderBy(\"id\")\n",
    "sdf_hash.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|part|count|\n",
      "+----+-----+\n",
      "|   3|    2|\n",
      "|   4|   12|\n",
      "|   2|    2|\n",
      "|   0|    4|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_hash.groupBy(\"part\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Repartition by range\n",
    "- How it works:\n",
    "  - In range partitioning spark creates ranges based on the values\n",
    "  - Sampling is used to identify the ranges which then are divided into the target partitions\n",
    "  - e.g. id col below is split into 5 partitions 1-4, 5-8, 9-12, 13-16, 17-20\n",
    "  - it usually used for continious values\n",
    "  - If no number of partitions is defined the default value depends on spark.sql.shuffle.partitions which defaults to 200 (important later when evaluating wide transformations in later episodes)\n",
    "- How to screw it up:\n",
    "  - Using columns with descrete values and some values repeat more often than others. Basically a skewed dataset\n",
    "\n",
    "- Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartitionByRange.html\n",
    "- Reference: https://stackoverflow.com/questions/65809909/spark-what-is-the-difference-between-repartition-and-repartitionbyrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "|  0|2024-01-31|2024-01-31 23:29:...|       0|      0|     0|         0|                  0|\n",
      "|  1|2024-01-31|2024-01-31 23:29:...|       1|      1|     1|         1|                  0|\n",
      "|  2|2024-01-31|2024-01-31 23:29:...|       2|      2|     2|         2|                  0|\n",
      "|  3|2024-01-31|2024-01-31 23:29:...|       3|      3|     3|         3|                  0|\n",
      "|  4|2024-01-31|2024-01-31 23:29:...|       4|      4|     4|         4|                  0|\n",
      "|  5|2024-01-31|2024-01-31 23:29:...|       5|      5|     5|         5|                  1|\n",
      "|  6|2024-01-31|2024-01-31 23:29:...|       6|      6|     6|         6|                  1|\n",
      "|  7|2024-01-31|2024-01-31 23:29:...|       7|      7|     7|         7|                  1|\n",
      "|  8|2024-01-31|2024-01-31 23:29:...|       8|      8|     8|         8|                  1|\n",
      "|  9|2024-01-31|2024-01-31 23:29:...|       9|      9|     9|         9|                  1|\n",
      "| 10|2024-01-31|2024-01-31 23:29:...|      10|      1|     0|        10|                  2|\n",
      "| 11|2024-01-31|2024-01-31 23:29:...|      11|      1|     1|        11|                  2|\n",
      "| 12|2024-01-31|2024-01-31 23:29:...|      12|      1|     2|        12|                  2|\n",
      "| 13|2024-01-31|2024-01-31 23:29:...|      13|      1|     3|        13|                  2|\n",
      "| 14|2024-01-31|2024-01-31 23:29:...|      14|      1|     4|        14|                  2|\n",
      "| 15|2024-01-31|2024-01-31 23:29:...|      15|      1|     5|        15|                  3|\n",
      "| 16|2024-01-31|2024-01-31 23:29:...|      16|      1|     6|        16|                  3|\n",
      "| 17|2024-01-31|2024-01-31 23:29:...|      17|      1|     7|        17|                  3|\n",
      "| 18|2024-01-31|2024-01-31 23:29:...|      18|      1|     8|        18|                  3|\n",
      "| 19|2024-01-31|2024-01-31 23:29:...|      19|      1|     9|        19|                  3|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(20, 4)\n",
    "sdf = sdf.withColumn(\"partition_id_before\", f.spark_partition_id()) #.where((f.col(\"id\") < 10) | (f.col(\"id\") > 14))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           1|    5|      25.0|\n",
      "|           3|    5|      25.0|\n",
      "|           2|    5|      25.0|\n",
      "|           0|    5|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           1|    1|       5.0|\n",
      "|           3|    2|      10.0|\n",
      "|           4|    4|      20.0|\n",
      "|           2|    1|       5.0|\n",
      "|           0|   12|      60.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_id: int, count: bigint, count_perc: double]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_part = sdf.repartitionByRange(5, \"idfirst\")\n",
    "rows_per_partition(sdf_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|idfirsttwo|partition_id_before|partition_id|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "|  0|2024-01-31|2024-01-31 23:00:...|       0|      0|     0|         0|                  0|           0|\n",
      "|  1|2024-01-31|2024-01-31 23:00:...|       1|      1|     1|         1|                  0|           0|\n",
      "|  2|2024-01-31|2024-01-31 23:00:...|       2|      2|     2|         2|                  0|           1|\n",
      "|  3|2024-01-31|2024-01-31 23:00:...|       3|      3|     3|         3|                  0|           1|\n",
      "|  4|2024-01-31|2024-01-31 23:00:...|       4|      4|     4|         4|                  0|           2|\n",
      "|  5|2024-01-31|2024-01-31 23:00:...|       5|      5|     5|         5|                  1|           2|\n",
      "|  6|2024-01-31|2024-01-31 23:00:...|       6|      6|     6|         6|                  1|           3|\n",
      "|  7|2024-01-31|2024-01-31 23:00:...|       7|      7|     7|         7|                  1|           3|\n",
      "|  8|2024-01-31|2024-01-31 23:00:...|       8|      8|     8|         8|                  1|           4|\n",
      "|  9|2024-01-31|2024-01-31 23:00:...|       9|      9|     9|         9|                  1|           4|\n",
      "| 10|2024-01-31|2024-01-31 23:00:...|      10|      1|     0|        10|                  2|           0|\n",
      "| 11|2024-01-31|2024-01-31 23:00:...|      11|      1|     1|        11|                  2|           0|\n",
      "| 12|2024-01-31|2024-01-31 23:00:...|      12|      1|     2|        12|                  2|           1|\n",
      "| 13|2024-01-31|2024-01-31 23:00:...|      13|      1|     3|        13|                  2|           1|\n",
      "| 14|2024-01-31|2024-01-31 23:00:...|      14|      1|     4|        14|                  2|           2|\n",
      "| 15|2024-01-31|2024-01-31 23:00:...|      15|      1|     5|        15|                  3|           2|\n",
      "| 16|2024-01-31|2024-01-31 23:00:...|      16|      1|     6|        16|                  3|           3|\n",
      "| 17|2024-01-31|2024-01-31 23:00:...|      17|      1|     7|        17|                  3|           3|\n",
      "| 18|2024-01-31|2024-01-31 23:00:...|      18|      1|     8|        18|                  3|           4|\n",
      "| 19|2024-01-31|2024-01-31 23:00:...|      19|      1|     9|        19|                  3|           4|\n",
      "+---+----------+--------------------+--------+-------+------+----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part = sdf_part.withColumn(\"partition_id\", f.spark_partition_id()).orderBy(\"id\")\n",
    "sdf_part.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Summary\n",
    "- Use Round Robin Shuffling if you want to have equal partition sizes not depending on column grouping\n",
    "- Use Hash Partitioning to repartition data based on cols. Take care to have a bigger number of distinct values. It might create a skew even though the col distribution is not skewed\n",
    "- Use Range partitioning for continious increasing columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
