{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Discover Sparks Algorithm to calculate the number of partitions loading parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need this?\n",
    "- Understand how Spark creates partitioning and what influences it makes performance and debugging better\n",
    "- You learned how the number of partitions, empty partitions and distribution of data within partitions influences the performance (added the previous comments below)\n",
    "- Coalesce and especially repartition are expensive operation. If we can influence the spark partitions during loading already is a big win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n",
    "results_dict = {}\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What influences the no. of partitions when loading parquet files\n",
    "- Num Cores in the cluster,\n",
    "  - Correctly speaking it's the \"spark.sql.files.minPartitionNum\" config\n",
    "  - It defaults to the default parallism which is our num of cores = 4\n",
    "- File Size\n",
    "- Num parquet files\n",
    "- Num of blocks/rowgroups within a parquet file\n",
    "- Max Partition Size:\n",
    "  - Influences the size of a partition  \n",
    "  - based on the config\"spark.sql.files.maxPartitionBytes\"\n",
    "  - defaults to 128 MB \n",
    "- Max Cost Per Bytes\n",
    "  - Represents the cost of creating a new partition\n",
    "  - based on the config \"spark.sql.files.openCostInBytes\"\n",
    "  - defaults to 4 MB\n",
    "  - Technically it adds the cost, e.g. 4 MB, to each file which is called padding\n",
    "  - Through this less but bigger partitions are created around the size of the open cost value\n",
    "  - Usually no influence, except of smaller files, default of 4MB works\n",
    "  - Official description: The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimate, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first). This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "- https://stackoverflow.com/questions/69034543/number-of-tasks-while-reading-hdfs-in-spark\n",
    "- https://stackoverflow.com/questions/75924368/skewed-partitions-when-setting-spark-sql-files-maxpartitionbytes\n",
    "- https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "- https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summary: Why do we need this?\n",
    "- Understand how Spark creates partitioning and what influences it makes performance and debugging better\n",
    "- You learned how the number of partitions, empty partitions and distribution of data within partitions influences the performance (added the previous comments below)\n",
    "- Coalesce and especially repartition are expensive operation. If we can influence the spark partitions during loading already is a big win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Our experiments\n",
    "- Experiment 1: 4 files, a 64.8 MB, sum 259.3 MB\n",
    "- Experiment 2: 8 files, a 64.9 MB MB, sum 518.9 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: D:/Spark/Data/4_files_32000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00002-7d709add-425e-4fed-b58f-7bdcb680635c-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00003-7d709add-425e-4fed-b58f-7bdcb680635c-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00001-7d709add-425e-4fed-b58f-7bdcb680635c-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/4_files_32000000_rows.parquet/part-00000-7d709add-425e-4fed-b58f-7bdcb680635c-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Num files: 4\n",
      "Avg file Size: 64.8 MB or 67965922.75 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4\n",
      "Num rows written: 32000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 20 MB or 20971520 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Size Per Core: 64.8 MB or 67965922.75 bytes\n",
      "Partionsize: 20.0 MB or 20971520 bytes\n",
      "EstimatedPartitions: 13, unrounded: 12.96347098350525\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 14\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|       67726805|        64.6|8000000|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|              0|         0.0|      0|\n",
      "|        4|       1|       68008198|        64.9|8000000|\n",
      "|        5|       1|              0|         0.0|      0|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|       68064947|        64.9|8000000|\n",
      "|        8|       1|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "|       10|       1|       68063741|        64.9|8000000|\n",
      "|       11|       1|              0|         0.0|      0|\n",
      "|       12|       2|              0|         0.0|      0|\n",
      "|       13|       2|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/4_files_32000000_rows.parquet\"\n",
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=20, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "#basic_maxSplitBytes(size, num_files)\n",
    "#max_split_bytes = maxSplitBytes(size, num_files)\n",
    "#file_size_list = get_files_as_list(sdf_meta_data)\n",
    "#file_size_list_dummy = split_files(file_size_list, max_split_bytes)\n",
    "#res = getFilePartitions(file_size_list, max_split_bytes)\n",
    "get_actual_num_partitions(path)\n",
    "#noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 8\n",
      "Saved Path: D:/Spark/Data/8_files_64000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows   |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00000-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |67726805       |8000000|64.6        |0.5046875    |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00001-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68008198       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00003-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68063741       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00002-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68064947       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00007-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68063635       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00006-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68063235       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00005-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68063528       |8000000|64.9        |0.50703125   |\n",
      "|file:/D:/Spark/Data/8_files_64000000_rows.parquet/part-00004-30835968-e404-46b8-8f5a-deae687628a4-c000.snappy.parquet|1     |68063118       |8000000|64.9        |0.50703125   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+-------+------------+-------------+\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 5 MB or 5242880 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Size Per Core: 129.7 MB or 136029301.75 bytes\n",
      "Partionsize: 5.0 MB or 5242880 bytes\n",
      "EstimatedPartitions: 104, unrounded: 103.78212108612061\n",
      " \n",
      "******** EXTENDED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS  ********\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxPartionsize: 5.0 MB or 5242880 bytes\n",
      "EstimatedPartitions: 111, unrounded: 110.1821210861206\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxPartionsize: 5.0 MB or 5242880 bytes\n",
      "EstimatedPartitionsAvg: 111, unrounded: 110.1821210861206\n",
      "Number calculated Partitions: 104\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 104\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|              0|         0.0|      0|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|              0|         0.0|      0|\n",
      "|        4|       1|              0|         0.0|      0|\n",
      "|        5|       1|              0|         0.0|      0|\n",
      "|        6|       1|       67726805|        64.6|8000000|\n",
      "|        7|       1|              0|         0.0|      0|\n",
      "|        8|       1|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "|       10|       1|              0|         0.0|      0|\n",
      "|       11|       1|              0|         0.0|      0|\n",
      "|       12|       1|              0|         0.0|      0|\n",
      "|       13|       1|              0|         0.0|      0|\n",
      "|       14|       1|              0|         0.0|      0|\n",
      "|       15|       1|              0|         0.0|      0|\n",
      "|       16|       1|              0|         0.0|      0|\n",
      "|       17|       1|              0|         0.0|      0|\n",
      "|       18|       1|       68008198|        64.9|8000000|\n",
      "|       19|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/8_files_64000000_rows.parquet\"\n",
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "sdf_meta_data = get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=5, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "basic_maxSplitBytes(size, num_files)\n",
    "max_split_bytes = maxSplitBytes(size, num_files)\n",
    "file_size_list = get_files_as_list(sdf_meta_data)\n",
    "file_size_list_dummy = split_files(file_size_list, max_split_bytes)\n",
    "res = getFilePartitions(file_size_list_dummy, max_split_bytes)\n",
    "get_actual_num_partitions(path)\n",
    "#noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic algorithm\n",
    "def basic_algorithm(file_size):\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])    \n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    size_per_core = file_size/minPartitionNum\n",
    "    partition_size = min(maxPartitionBytes, size_per_core)\n",
    "    no_partitions = file_size/partition_size #round up for no_partitions\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"******** BASIC ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Size Per Core: {round(size_per_core/1024/1024, 1)} MB or {size_per_core} bytes\")\n",
    "    print(f\"Partionsize: {round(partition_size/1024/1024, 1)} MB or {partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions)}, unrounded: {no_partitions}\")\n",
    "\n",
    "#Reference: https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    \n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    partition_size = min(maxPartitionBytes, size_per_core_padded)\n",
    "    no_partitions_padded = file_size_padded/partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** EXTENDED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS  ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(partition_size/1024/1024, 1)} MB or {partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    \n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    max_partition_size = min(maxPartitionBytes, max(openCostInBytes, size_per_core_padded))\n",
    "    no_partitions_padded = file_size_padded/max_partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(max_partition_size/1024/1024, 1)} MB or {max_partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitionsAvg: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")\n",
    "    return max_partition_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_as_list(parquet_meta_data_sdf):\n",
    "    return list(parquet_meta_data_sdf.select(\"compressedBytes\").toPandas()[\"compressedBytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67726805,\n",
       " 68008198,\n",
       " 68063741,\n",
       " 68064947,\n",
       " 68063635,\n",
       " 68063235,\n",
       " 68063528,\n",
       " 68063118]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [50, 73, 35, 22, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(files_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtil.scala#L45\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    for file_size in files_list:\n",
    "        remaining = file_size\n",
    "        while remaining > 0:\n",
    "            if remaining > max_split_bytes:\n",
    "                result_list.append(max_split_bytes)\n",
    "            else:\n",
    "                result_list.append(remaining)  \n",
    "            remaining = remaining - max_split_bytes\n",
    "    result_list.sort(reverse=True)\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result2 = split_files(test_list, 5)\n",
    "test_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilePartitions(file_size_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n",
    "    \"\"\"\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    partitions = []\n",
    "    current_files = []\n",
    "    current_size = 0\n",
    "\n",
    "    def close_partition():\n",
    "        if current_files:\n",
    "            partition_details = {\n",
    "                \"files\": current_files.copy(),\n",
    "                \"num_files\": len(current_files),\n",
    "                \"partition_size\": sum(current_files)\n",
    "            }\n",
    "        else:\n",
    "            partition_details = {}\n",
    "        partitions.append(partition_details)\n",
    "        current_files.clear()\n",
    "\n",
    "    for file_size in file_size_list:\n",
    "        if current_size + file_size > max_split_bytes:\n",
    "            close_partition()\n",
    "            current_size = 0\n",
    "        current_size += file_size + openCostInBytes\n",
    "        current_files.append(file_size)\n",
    "    close_partition()\n",
    "    print(f\"Number calculated Partitions: {len(partitions)}\")\n",
    "    return partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number calculated Partitions: 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [5], 'num_files': 1, 'partition_size': 5},\n",
       " {'files': [3], 'num_files': 1, 'partition_size': 3},\n",
       " {'files': [2], 'num_files': 1, 'partition_size': 2}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFilePartitions(test_result2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
