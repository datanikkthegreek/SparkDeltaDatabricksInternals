{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference gresearch:\\n- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\\n- GitHub Spark extension: https://github.com/G-Research/spark-extension\\n- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-03-24|2024-03-24 12:46:...|       0|      0|     0|\n",
      "|  1|2024-03-24|2024-03-24 12:46:...|       1|      1|     1|\n",
      "|  2|2024-03-24|2024-03-24 12:46:...|       2|      2|     2|\n",
      "|  3|2024-03-24|2024-03-24 12:46:...|       3|      3|     3|\n",
      "|  4|2024-03-24|2024-03-24 12:46:...|       4|      4|     4|\n",
      "|  5|2024-03-24|2024-03-24 12:46:...|       5|      5|     5|\n",
      "|  6|2024-03-24|2024-03-24 12:46:...|       6|      6|     6|\n",
      "|  7|2024-03-24|2024-03-24 12:46:...|       7|      7|     7|\n",
      "|  8|2024-03-24|2024-03-24 12:46:...|       8|      8|     8|\n",
      "|  9|2024-03-24|2024-03-24 12:46:...|       9|      9|     9|\n",
      "| 10|2024-03-24|2024-03-24 12:46:...|      10|      1|     0|\n",
      "| 11|2024-03-24|2024-03-24 12:46:...|      11|      1|     1|\n",
      "| 12|2024-03-24|2024-03-24 12:46:...|      12|      1|     2|\n",
      "| 13|2024-03-24|2024-03-24 12:46:...|      13|      1|     3|\n",
      "| 14|2024-03-24|2024-03-24 12:46:...|      14|      1|     4|\n",
      "| 15|2024-03-24|2024-03-24 12:46:...|      15|      1|     5|\n",
      "| 16|2024-03-24|2024-03-24 12:46:...|      16|      1|     6|\n",
      "| 17|2024-03-24|2024-03-24 12:46:...|      17|      1|     7|\n",
      "| 18|2024-03-24|2024-03-24 12:46:...|      18|      1|     8|\n",
      "| 19|2024-03-24|2024-03-24 12:46:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(100, 8)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_max_partitions_bytes(maxPartitionsMB):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_schema = \"id bigint, date date, timestamp timestamp, idstring string, idfirst string, idlast string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: 8 files, 80.000.000 rows, a 81 MB\n",
    "- A file size factor of num of cores 4 and equal file sizes create well distributed partitions\n",
    "- reducing the maxPartitionBytes does not change anything as long as above the file size\n",
    "- Doubling the maxPartitionBytes does reduce the number of partitions to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 1: 8 files, 80.000.000 rows, 81 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_1.parquet\"\n",
    "sdf = sdf_generator(80000000, 8)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(90)\n",
    "sc.setJobDescription(\"Load Parquet with 90 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(180)\n",
    "sc.setJobDescription(\"Load Parquet with 180 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: 16 files, 160.000.000 rows, a 81 MB\n",
    "- Again we have a well distrubuted dataset and partitions. But the file size is not 100 % equal distributed\n",
    "- Choosing a wrong number of maxPartition bytes leads to an undistrubuted number of partitions\n",
    "- Interestingly 4 partitions cannot be reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 2: 16 files, 160.000.000 rows, 81 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_2.parquet\"\n",
    "sdf = sdf_generator(160000000, 16)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(90)\n",
    "sc.setJobDescription(\"Load Parquet with 90 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(180)\n",
    "sc.setJobDescription(\"Load Parquet with 180 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(170)\n",
    "sc.setJobDescription(\"Load Parquet with 170 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(320)\n",
    "sc.setJobDescription(\"Load Parquet with 320 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(900)\n",
    "sc.setJobDescription(\"Load Parquet with 900 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: 30 files, 110.000.000 rows, a 30 MB\n",
    "- The default value creates 10 partitions\n",
    "- By slightly increasing it we can create 8 partitions and reduce the execution time from 5 to 4s\n",
    "- Reducing with 12 partitions had a similar effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 3: 30 files, 110.000.000 rows, 30 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_3.parquet\"\n",
    "sdf = sdf_generator(110000000, 30)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(140)\n",
    "sc.setJobDescription(\"Load Parquet with 140 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(97)\n",
    "sc.setJobDescription(\"Load Parquet with 97 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: 10 files, 100.000.000 rows, a 81 MB\n",
    "- Reduction of max Partition Bytes below file size can have a negative effect of empty partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 4: 10 files, 100.000.000 rows, 80 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_4.parquet\"\n",
    "sdf = sdf_generator(100000000, 10)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(60)\n",
    "sc.setJobDescription(\"Load Parquet with 60 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(10)\n",
    "sc.setJobDescription(\"Load Parquet with 10 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: 1 file, 100.000.000 rows, a 809 MB\n",
    "- Here we have a skewed partition set as the last row group is smaller. Also we have 7 row groups\n",
    "- We still can improve performance by increasing max partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 5: 1 file, 100.000.000 rows, 809 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_5.parquet\"\n",
    "sdf = sdf_generator(100000000, 1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(280)\n",
    "sc.setJobDescription(\"Load Parquet with 280 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: 10 files, 1.000.000.000 rows, a 850 MB\n",
    "- Here we have files with 7 row groups each and the last one is smaller\n",
    "- In this case it didnt speed up peformance but we could reduce the no. partitions for a better distrubution\n",
    "- Risk of empty partitions after filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 6: 10 files, 1.000.000.000 rows, 850 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_6.parquet\"\n",
    "sdf = sdf_generator(1000000000, 10)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(150)\n",
    "sc.setJobDescription(\"Load Parquet with 150 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(270)\n",
    "sc.setJobDescription(\"Load Parquet with 270 MB\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: 10 files, 1.000.000.000 rows, a 81 MB with filter id < 100.000.000\n",
    "- filters can create empty partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 7: 10 files, 1.000.000.000 rows, 850 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_7.parquet\"\n",
    "sdf = sdf_generator(1000000000, 10)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "#sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB id < 100.000.000\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 100000000)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(260)\n",
    "sc.setJobDescription(\"Load Parquet with 260 MB id < 100.000.000\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 100000000)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(270)\n",
    "sc.setJobDescription(\"Load Parquet with 270 MB id < 100.000.000\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 100000000)\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: 10 files, 1.000.000.000 rows, a 81 MB with filter idlast = 1\n",
    "- filter based on key through all tables this can be avoided but predicate pushdown cannot be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Save Parquet Exp 8: 10 files, 1.000.000.000 rows, 850 MB\")\n",
    "path_parquet = \"D:/Spark/Data/parquet_8.parquet\"\n",
    "sdf = sdf_generator(1000000000, 10)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(128)\n",
    "sc.setJobDescription(\"Load Parquet with 128 MB idlast 1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"idlast\") == \"1\")\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(270)\n",
    "sc.setJobDescription(\"Load Parquet with 270 MB idlast 1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"idlast\") == \"1\")\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "set_max_partitions_bytes(370)\n",
    "sc.setJobDescription(\"Load Parquet with 370 MB idlast 1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"idlast\") == \"1\")\n",
    "print(sdf_parquet.rdd.getNumPartitions())\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-3.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
