{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark Parititions influence saving data with parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: \n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n",
    "results_dict = {}\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10**decimals\n",
    "    return math.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "#source: https://realpython.com/python-rounding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_num_partitions(file_size, num_files):\n",
    "    #get spark values\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    #Calculate maxSpliPartitionBytes\n",
    "    # a) If we have bigger files bytesPerCorePadded will be bigger then openCostInBytes but also maxPartitionBytes. \n",
    "    # In this case we would limit the size as mayPartitionBytes will be the result of maxSplitPartitionBytes. e.g. 1 GB dataset, 4 cores, maxPartitions 128 MB\n",
    "    # b) If bytesPerCorePadded is the result of maxSplitPartitionBytes we have a fair split of the data over all cores, e.g. 1 GB dataset, 4 cores, maxPartitions 300 MB\n",
    "    # c) If bytesPerCore is to small we want to limit amount of Partitions to be opened. This is the cost here.\n",
    "    paddedFileSize = file_size + num_files * openCostInBytes\n",
    "    bytesPerCorePadded = paddedFileSize / minPartitionNum\n",
    "    maxSplitPartitionBytes = min(maxPartitionBytes, max(openCostInBytes, bytesPerCorePadded))\n",
    "    #Estimation of partitions from Internet\n",
    "    estimated_num_partitions_int = paddedFileSize/maxSplitPartitionBytes\n",
    "    #Own Estimator\n",
    "    avg_file_size_padded = paddedFileSize/num_files\n",
    "    bytesPerCore = file_size / minPartitionNum\n",
    "    #Calculate number of files fitting into one partitions. Then calculate the number of partitions\n",
    "    files_per_partition = max(1, math.floor(maxSplitPartitionBytes/avg_file_size_padded))\n",
    "    estimated_num_partitions = num_files/files_per_partition\n",
    "    print(\" \")\n",
    "    print(\"******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\")\n",
    "    print(f\"Avg file Size Padded: {round(avg_file_size_padded/1024/1024, 1)} MB or {avg_file_size_padded} bytes\")\n",
    "    print(f\"Padded File Size: {round(paddedFileSize/1024/1024, 1)} MB or {paddedFileSize} bytes\")\n",
    "    print(f\"SizePerCore: {round(bytesPerCore/1024/1024, 1)} MB or {bytesPerCore} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(bytesPerCorePadded/1024/1024, 1)} MB or {bytesPerCorePadded} bytes\")\n",
    "    print(f\"MaxSplitPartitionBytes: {round(maxSplitPartitionBytes/1024/1024, 1)} MB or {maxSplitPartitionBytes} bytes\")\n",
    "    print(f\"MaxFilesPerPartition {files_per_partition}\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(estimated_num_partitions)}, unrounded: {estimated_num_partitions}\")\n",
    "    print(f\"EstimatedPartitionsInternet: {math.ceil(estimated_num_partitions_int)}, unrounded: {estimated_num_partitions_int}\")\n",
    "\n",
    "    results_dict[\"paddedFileSize\"] = round(paddedFileSize/1024/1024, 1)\n",
    "    results_dict[\"MBPerCore\"] = round(bytesPerCore/1024/1024, 1)\n",
    "    results_dict[\"MBPerCorePadded\"] = round(bytesPerCorePadded/1024/1024, 1)\n",
    "    results_dict[\"maxSplitPartitionBytes\"] = round(maxSplitPartitionBytes/1024/1024, 1)\n",
    "    results_dict[\"avg_file_size_padded\"] = round(avg_file_size_padded/1024/1024, 1)\n",
    "    results_dict[\"Maxfiles_per_partition\"] = files_per_partition\n",
    "    results_dict[\"MyEstimationPartitions\"] = math.ceil(estimated_num_partitions)\n",
    "    results_dict[\"InternetEstimationPartitions\"] = math.ceil(estimated_num_partitions_int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_bytes_rows_partition(sdf):\n",
    "    sdf = (\n",
    "        sdf.select(f.mean(\"numFiles\"), f.mean(\"compressedBytes\"), f.mean(\"rows\"))\n",
    "        .withColumn(\"avg(compressedMB)\", f.round(f.col(\"avg(compressedBytes)\")/1024/1204, 1))\n",
    "        .select(\"avg(numFiles)\", \"avg(compressedBytes)\", \"avg(compressedMB)\", \"avg(rows)\")\n",
    "    )\n",
    "    sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What influences the no. of partitions when loading parquet files\n",
    "- Num Cores\n",
    "- File Size\n",
    "- Num parquet files\n",
    "- Num of blocks/rowgroups within a parquet file\n",
    "- Max Partition Size\n",
    "- Max Cost Per Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First simple experiment\n",
    "- Experiment 1: 4 files, a 64 MB\n",
    "- Experiment 2: 8 files, a 64 MB (128 MB; 140 MB max size)\n",
    "- Experiment 3: 8 files, a 50 MB\n",
    "\n",
    "Set-Up:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: D:/Spark/Data/4_files_32000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Num files: 4\n",
      "Avg file Size: 64.8 MB or 67965922.75 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4\n",
      "Num rows written: 32000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 50 MB or 52428800 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 68.8 MB or 72160226.75 bytes\n",
      "Padded File Size: 275.3 MB or 288640907 bytes\n",
      "SizePerCore: 64.8 MB or 67965922.75 bytes\n",
      "SizePerCorePadded: 68.8 MB or 72160226.75 bytes\n",
      "MaxSplitPartitionBytes: 50.0 MB or 52428800 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 6, unrounded: 5.505388393402099\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 6\n",
      "Duration: 2.29 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       67726805|        64.6|8000000|\n",
      "|        1|       1|       68008198|        64.9|8000000|\n",
      "|        2|       1|       68064947|        64.9|8000000|\n",
      "|        3|       1|       68063741|        64.9|8000000|\n",
      "|        4|       2|              0|         0.0|      0|\n",
      "|        5|       2|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=50, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 8\n",
      "Saved Path: D:/Spark/Data/8_files_64000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 60 MB or 62914560 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 68.9 MB or 72208954.875 bytes\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCore: 129.7 MB or 136029301.75 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxSplitPartitionBytes: 60.0 MB or 62914560 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 8, unrounded: 8.0\n",
      "EstimatedPartitionsInternet: 10, unrounded: 9.181843423843384\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 10\n",
      "Duration: 4.51 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       67726805|        64.6|8000000|\n",
      "|        1|       1|       68008198|        64.9|8000000|\n",
      "|        2|       1|       68064947|        64.9|8000000|\n",
      "|        3|       1|       68063741|        64.9|8000000|\n",
      "|        4|       1|       68063118|        64.9|8000000|\n",
      "|        5|       1|       68063528|        64.9|8000000|\n",
      "|        6|       1|       68063235|        64.9|8000000|\n",
      "|        7|       1|       68063635|        64.9|8000000|\n",
      "|        8|       7|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=60, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
