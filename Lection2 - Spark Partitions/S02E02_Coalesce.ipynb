{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen = sdf_generator(20)\n",
    "sdf_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-01-13|2024-01-13 13:13:...|       0|      0|     0|\n",
      "|  1|2024-01-13|2024-01-13 13:13:...|       1|      1|     1|\n",
      "|  2|2024-01-13|2024-01-13 13:13:...|       2|      2|     2|\n",
      "|  3|2024-01-13|2024-01-13 13:13:...|       3|      3|     3|\n",
      "|  4|2024-01-13|2024-01-13 13:13:...|       4|      4|     4|\n",
      "|  5|2024-01-13|2024-01-13 13:13:...|       5|      5|     5|\n",
      "|  6|2024-01-13|2024-01-13 13:13:...|       6|      6|     6|\n",
      "|  7|2024-01-13|2024-01-13 13:13:...|       7|      7|     7|\n",
      "|  8|2024-01-13|2024-01-13 13:13:...|       8|      8|     8|\n",
      "|  9|2024-01-13|2024-01-13 13:13:...|       9|      9|     9|\n",
      "| 10|2024-01-13|2024-01-13 13:13:...|      10|      1|     0|\n",
      "| 11|2024-01-13|2024-01-13 13:13:...|      11|      1|     1|\n",
      "| 12|2024-01-13|2024-01-13 13:13:...|      12|      1|     2|\n",
      "| 13|2024-01-13|2024-01-13 13:13:...|      13|      1|     3|\n",
      "| 14|2024-01-13|2024-01-13 13:13:...|      14|      1|     4|\n",
      "| 15|2024-01-13|2024-01-13 13:13:...|      15|      1|     5|\n",
      "| 16|2024-01-13|2024-01-13 13:13:...|      16|      1|     6|\n",
      "| 17|2024-01-13|2024-01-13 13:13:...|      17|      1|     7|\n",
      "| 18|2024-01-13|2024-01-13 13:13:...|      18|      1|     8|\n",
      "| 19|2024-01-13|2024-01-13 13:13:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How coalesce works \n",
    "- Documentation: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html\n",
    "- Narrow transformation\n",
    "- Can only increase not reduce partitions. It does not give you and error but it just ignores a value higher than the initially available partitions\n",
    "- Coalesce can skew the data within each partition which leads to lower performance and some tasks running way longer\n",
    "- Coalesce can help with efficiently reducing high number of small partitions and improve performance\n",
    "- A to small number of partitions (bigger partitions) can result to OOM or other issues. A factor of 2-4 of the number of cors is recommended. But really depends on the memory available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 2000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1 = sdf_generator(num_rows, 4)\n",
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000000\n"
     ]
    }
   ],
   "source": [
    "row_count1 = sdf1.count()\n",
    "print(row_count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+\n",
      "|partition_id|    count|count_perc|\n",
      "+------------+---------+----------+\n",
      "|           0|500000000|      25.0|\n",
      "|           1|500000000|      25.0|\n",
      "|           2|500000000|      25.0|\n",
      "|           3|500000000|      25.0|\n",
      "+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part1 = sdf1.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "sdf_part_count1 = sdf_part1.groupBy(\"partition_id\").count()\n",
    "sdf_part_count1 = sdf_part_count1.withColumn(\"count_perc\", 100*f.col(\"count\")/row_count1)\n",
    "sdf_part_count1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Baseline 4 partitions\")\n",
    "sdf1.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.coalesce(12).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = sdf1.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000000\n"
     ]
    }
   ],
   "source": [
    "row_count2 = sdf2.count()\n",
    "print(row_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|partition_id|     count|count_perc|\n",
      "+------------+----------+----------+\n",
      "|           0| 500000000|      25.0|\n",
      "|           1| 500000000|      25.0|\n",
      "|           2|1000000000|      50.0|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part2 = sdf2.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "sdf_part_count2 = sdf_part2.groupBy(\"partition_id\").count()\n",
    "sdf_part_count2 = sdf_part_count2.withColumn(\"count_perc\", 100*f.col(\"count\")/row_count2)\n",
    "sdf_part_count2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Coalesce from 4 to 3\")\n",
    "sdf2.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sdf3 = sdf_generator(num_rows, 3)\n",
    "print(sdf3.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"3 Partitions\")\n",
    "sdf3.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sdf4 = sdf_generator(num_rows, 8)\n",
    "print(sdf4.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"8 Partitions\")\n",
    "sdf4.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf5 = sdf4.coalesce(4)\n",
    "print(sdf5.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce from 8 to 4\")\n",
    "sdf5.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n"
     ]
    }
   ],
   "source": [
    "sdf6 = sdf_generator(num_rows, 200001)\n",
    "print(sdf6.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"200001 partitions\")\n",
    "sdf6.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf7 = sdf6.coalesce(4)\n",
    "print(sdf7.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce from 200001 to 4\")\n",
    "sdf7.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sdf8 = sdf_generator(num_rows, 40)\n",
    "print(sdf8.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"40 partitions\")\n",
    "sdf8.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
