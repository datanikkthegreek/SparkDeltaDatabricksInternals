{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark ui happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [\n",
    "    {\"a\":\"a\", \"b\": 1},\n",
    "    {\"a\":\"b\", \"b\": 2},\n",
    "    {\"a\":\"c\", \"b\": 3},\n",
    "    {\"a\":\"d\", \"b\": 4},\n",
    "    {\"a\":\"e\", \"b\": 5},\n",
    "    {\"a\":\"e\", \"b\": 6},\n",
    "    {\"a\":\"f\", \"b\": 7},\n",
    "    {\"a\":\"g\", \"b\": 8},\n",
    "    {\"a\":\"h\", \"b\": 9},\n",
    "    {\"a\":\"i\", \"b\": 10},\n",
    "    {\"a\":\"j\", \"b\": 11},\n",
    "    {\"a\":\"k\", \"b\": 12},\n",
    "    {\"a\":\"a\", \"b\": 13},\n",
    "    {\"a\":\"b\", \"b\": 13},\n",
    "]\n",
    "ddl_schema = \"a string, b int\"\n",
    "sdf = spark.createDataFrame(d, schema=ddl_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lazy Execution and actions\n",
    "\n",
    "You can see here the following things:\n",
    "- The simple filter operation runs within ms\n",
    "- In the Spark UI under jobs you will not see any job when running only the filter operation\n",
    "- Only once you run a Job as here count or later write Spark becomes active and runs the actual calculation\n",
    "- Once count is executed you will see the Job in the Spark UI\n",
    "- The data has been paritioned into 4 partitions (more in the next episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_lazy = sdf.filter(f.col(\"b\") > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setJobDescription(\"LazyExecution\")\n",
    "sdf_lazy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_lazy.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Noop write\n",
    "\n",
    "- The same behavior as before. Write is an action and Spark is running a job. \n",
    "- We use a noop write which does not have any side effects and spark optimizations while executing an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"NoopWrite\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Narrow transformation with noop write\n",
    "- A narrow transformation as you hopefully already know does not have any shuffle operations\n",
    "- This means if no Spark or Databricks optimizations active we will see only one Job with one Stage and the number of tasks related to the Spark partition size of the data.\n",
    "- Hint: the number of stages can also vary depending on the action. E.g. with count as you see next. That's why a noop write is great to analyze without side effects\n",
    "- Another indicator is the number partitions before and after the transformation which does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"FilterNoopWrite\")\n",
    "sdf_narrowNoop = sdf.filter(f.col(\"b\") > 5)\n",
    "sdf_narrowNoop.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_narrowNoop.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Count\n",
    "- We see that the count creates two stages\n",
    "- The first stage has the same number of tasks as before. Here on each executer the partial counts of each partitions are calculated\n",
    "- Afterwards we have an exchange of all those informations into one last executer to caclualte the final count. That's why the second stage only has one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Count\")\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Wide transformation\n",
    "- For wide transformations we have usually shuffle operations and an exchange for the data and thus two stages similar to a count. Those operations often are reason for unefficiencies. \n",
    "- A hint for a wide transformation is the change of partitions as the shuffle creates a repartioning during this process\n",
    "- You can see that the number of partitions depends on the value of \"spark.sql.shuffle.partitions\". Default 200\n",
    "- In the Spark UI under SQL and then Details we can see in the physical plan the Hash partitioning which usually happens during re-shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "sc.setJobDescription(\"Wide\")\n",
    "sdf_w = sdf.groupBy(\"a\").count()\n",
    "sdf_w.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  g|    1|\n",
      "|  f|    1|\n",
      "|  k|    1|\n",
      "|  e|    2|\n",
      "|  h|    1|\n",
      "|  d|    1|\n",
      "|  c|    1|\n",
      "|  i|    1|\n",
      "|  j|    1|\n",
      "|  b|    2|\n",
      "|  a|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"WideShow\")\n",
    "sdf_w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_w.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Wide Transformations with AQE\n",
    "- Similar to before we run a wide transformation here just to showcase why we turned off AQE\n",
    "- You probably already assumed that for 14 rows 200 partitions is not benefitial\n",
    "- One of AQEs feature is to coalesce a lot of small partitions into bigger once. In this case one partion is left at the end\n",
    "- This can be seen in the Job DAG visualisation in the second stage as an AQE Shuffle Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning on AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "sc.setJobDescription(\"WideAQE\")\n",
    "sdf_w_aqe = sdf.groupBy(\"a\").count()\n",
    "sdf_w_aqe.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  c|    1|\n",
      "|  b|    2|\n",
      "|  a|    2|\n",
      "|  e|    2|\n",
      "|  d|    1|\n",
      "|  g|    1|\n",
      "|  f|    1|\n",
      "|  h|    1|\n",
      "|  k|    1|\n",
      "|  i|    1|\n",
      "|  j|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"WideShowAQE\")\n",
    "sdf_w_aqe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_w_aqe.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
