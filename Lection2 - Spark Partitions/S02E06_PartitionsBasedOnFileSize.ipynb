{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark Parititions influence saving data with parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: \n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    print(f\"File Size: {round(size/1024/1024, 1)} MB or {size} bytes\")\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10**decimals\n",
    "    return math.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "#source: https://realpython.com/python-rounding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_num_partitions(file_size, num_files):\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    padded_file_size = file_size + num_files * openCostInBytes\n",
    "    avg_file_size = file_size/num_files\n",
    "    avg_file_size_padded = avg_file_size + openCostInBytes\n",
    "    bytesPerCore = file_size / minPartitionNum\n",
    "    bytesPerCorePadded = padded_file_size / minPartitionNum\n",
    "    maxSplitPartitionBytes = min(maxPartitionBytes, max(openCostInBytes, bytesPerCorePadded))\n",
    "    files_per_partition = round_half_up(maxSplitPartitionBytes/avg_file_size_padded, 0)\n",
    "    estimated_num_partitions = math.ceil(num_files/files_per_partition)\n",
    "    estimated_num_partitions_int = math.ceil(padded_file_size/maxSplitPartitionBytes)\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    print(f\"Avg file Size Padded: {round(avg_file_size_padded/1024/1024, 1)} MB or {avg_file_size_padded} bytes\")\n",
    "    print(f\"Padded File Size: {round(padded_file_size/1024/1024, 1)} MB or {padded_file_size} bytes\")\n",
    "    print(f\"SizePerCore: {round(bytesPerCore/1024/1024, 1)} MB or {bytesPerCore} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(bytesPerCorePadded/1024/1024, 1)} MB or {bytesPerCorePadded} bytes\")\n",
    "    print(f\"MaxSplitPartitionBytes: {round(maxSplitPartitionBytes/1024/1024, 1)} MB or {maxSplitPartitionBytes} bytes\")\n",
    "    print(f\"MaxFilesPerPartition {files_per_partition}\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(estimated_num_partitions)}, unrounded: {estimated_num_partitions}\")\n",
    "    print(f\"EstimatedPartitionsInternet: {math.ceil(estimated_num_partitions_int)}, unrounded: {estimated_num_partitions_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_half_up(2.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    sc.setJobDescription(\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files written: 20\n",
      "Num rows written: 6000000\n",
      "Num partitions written: 20\n",
      "Saved Path: D:/Spark/Data/20_files_6000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 20\n",
    "num_rows = 6000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPartitionSize 60 MB or 62914560 bytes\n",
      "OpenCostInBytes 12 MB or 12582912 bytes\n",
      "Min Partitions: 2\n",
      "File Size: 48.4 MB or 50768643 bytes\n",
      "Avg file Size: 2.4 MB or 2538432.15 bytes\n",
      "Avg file Size Padded: 14.4 MB or 15121344.15 bytes\n",
      "Padded File Size: 288.4 MB or 302426883 bytes\n",
      "SizePerCore: 24.2 MB or 25384321.5 bytes\n",
      "SizePerCorePadded: 144.2 MB or 151213441.5 bytes\n",
      "MaxSplitPartitionBytes: 60.0 MB or 62914560 bytes\n",
      "MaxFilesPerPartition 4.0\n",
      "EstimatedPartitions: 5, unrounded: 5\n",
      "EstimatedPartitionsInternet: 5, unrounded: 5\n",
      "ActualNumPartitions: 5\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+------+------------+-------------+\n",
      "|filename                                                                                                             |blocks|compressedBytes|rows  |compressedMB|calcNumBlocks|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+------+------------+-------------+\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00006-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544164        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00015-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544707        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00004-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544193        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00000-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2511273        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00018-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544198        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00013-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544156        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00001-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2508612        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00016-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544152        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00005-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544194        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00010-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544181        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00009-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544180        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00012-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544212        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00002-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2508627        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00008-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544168        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00007-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544678        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00019-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544177        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00003-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2532273        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00011-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544178        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00017-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544144        |300000|2.4         |0.01875      |\n",
      "|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00014-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|1     |2544176        |300000|2.4         |0.01875      |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------+---------------+------+------------+-------------+\n",
      "\n",
      "+---------+-----+-------+-------+------+---------------+------------+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|partition|start|end    |length |blocks|compressedBytes|compressedMB|rows  |filename                                                                                                             |\n",
      "+---------+-----+-------+-------+------+---------------+------------+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0        |0    |2548235|2548235|1     |2544707        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00015-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|0        |0    |2548206|2548206|1     |2544678        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00007-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|0        |0    |2547740|2547740|1     |2544212        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00012-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|0        |0    |2547726|2547726|1     |2544198        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00018-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|1        |0    |2547722|2547722|1     |2544194        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00005-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|1        |0    |2547721|2547721|1     |2544193        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00004-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|1        |0    |2547709|2547709|1     |2544181        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00010-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|1        |0    |2547708|2547708|1     |2544180        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00009-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|2        |0    |2547706|2547706|1     |2544178        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00011-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|2        |0    |2547705|2547705|1     |2544177        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00019-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|2        |0    |2547704|2547704|1     |2544176        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00014-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|2        |0    |2547696|2547696|1     |2544168        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00008-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|3        |0    |2547686|2547686|1     |2544164        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00006-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|3        |0    |2547678|2547678|1     |2544156        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00013-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|3        |0    |2547674|2547674|1     |2544152        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00016-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|3        |0    |2547672|2547672|1     |2544144        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00017-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|4        |0    |2535784|2535784|1     |2532273        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00003-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|4        |0    |2514743|2514743|1     |2511273        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00000-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|4        |0    |2512117|2512117|1     |2508627        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00002-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "|4        |0    |2512102|2512102|1     |2508612        |2.4         |300000|file:/D:/Spark/Data/20_files_6000000_rows.parquet/part-00001-2939f2ee-2fd2-45a7-90b9-9c78c832bcce-c000.snappy.parquet|\n",
      "+---------+-----+-------+-------+------+---------------+------------+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_configs(maxPartitionsMB=60, openCostInMB=12, minPartitions=2)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "#get_parquet_window_length(path)\n",
    "get_parquet_meta_data(path)\n",
    "#get_parquet_blocks(path)\n",
    "get_spark_partitions(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def calPartitions(splitbytes, paddedfilesize):\n",
    "    \n",
    "\n",
    "calPartitions(64.2, 6.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mparquet_partitions(path)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#s.groupBy(\"partition\").sum(\"compressedBytes\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[1;34m(self, spark)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte"
     ]
    }
   ],
   "source": [
    "s = spark.read.parquet_partitions(path)\n",
    "#s.groupBy(\"partition\").sum(\"compressedBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.015625"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32.1/6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169986942.25"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 679947769/4\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.234375"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 87165529 \n",
    "\n",
    "diff = t-c\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.234375"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42496735.5625"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 679947769/16\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086028.4375"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 43582764\n",
    "diff = t-c\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.109375"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169986942.25"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "679947769/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.49373435974121"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43509334/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.08097457885742"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87116716/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|partition|start    |end      |length   |blocks|compressedBytes|compressedMB|rows    |filename                                                                                                             |\n",
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0        |0        |107202443|107202443|1     |133949814      |127.7       |15790100|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|1        |107202443|214404886|107202443|1     |133895545      |127.7       |15770100|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|2        |214404886|321607329|107202443|0     |0              |0.0         |0       |file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|3        |321607329|424615468|103008139|2     |156390510      |149.1       |18439800|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 1 File - \n",
    "\n",
    "https://mageswaran1989.medium.com/a-dive-into-apache-spark-parquet-reader-for-small-file-sizes-fabb9c35f64e#:~:text=maxPartitionBytes%3A%20128MB%20(The%20maximum%20number,sql.\n",
    "https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPartitionsMB = 70\n",
    "openCostInMB = 4\n",
    "minPartitions = 4\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInMB*1024*1024)+\"b\")\n",
    "spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "#https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "#https://issues.apache.org/jira/browse/SPARK-17998\n",
    "#spark.sql.files.maxPartitionNum = None\t\n",
    "#https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "#https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example\n",
    "#https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134217728"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n",
      "4.0\n",
      "4\n",
      "484.9\n",
      "121.225\n",
      "70.0\n",
      "6.927142857142857\n"
     ]
    }
   ],
   "source": [
    "data_size = 404.9*1024*1024\n",
    "number_files = \n",
    "defaultMaxSplitBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "totalBytes = data_size + (number_files * openCostInBytes)\n",
    "bytesPerCore = totalBytes / minPartitionNum\n",
    "targetSizePerPartition = min(defaultMaxSplitBytes, max(openCostInBytes, bytesPerCore))\n",
    "noPartitions = totalBytes/targetSizePerPartition\n",
    "print(defaultMaxSplitBytes/1024/1024)\n",
    "print(openCostInBytes/1024/1024)\n",
    "print(minPartitionNum)\n",
    "print(totalBytes/1024/1024)\n",
    "print(bytesPerCore/1024/1024)\n",
    "print(targetSizePerPartition/1024/1024)\n",
    "print(noPartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(\n",
    "      sparkSession: SparkSession,\n",
    "      selectedPartitions: Seq[PartitionDirectory]): Long = {\n",
    "defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes\n",
    "    val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes\n",
    "    val minPartitionNum = sparkSession.sessionState.conf.filesMinPartitionNum\n",
    "      .getOrElse(sparkSession.leafNodeDefaultParallelism)\n",
    "    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum\n",
    "    val bytesPerCore = totalBytes / minPartitionNum\n",
    "\n",
    "    Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.leafNodeDefaultParallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.minPartitionNum\", \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 1 file, 404,9 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_1_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "sc.setJobDescription(\"Read 1 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"70m\")\n",
    "sc.setJobDescription(\"Read 1 file with 70m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"51m\")\n",
    "sc.setJobDescription(\"Read 1 file with 51m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"34m\")\n",
    "sc.setJobDescription(\"Read 1 file with 34m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"200m\")\n",
    "sc.setJobDescription(\"Read 1 file with 200m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"400m\")\n",
    "sc.setJobDescription(\"Read 1 file with 400m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"500m\")\n",
    "sc.setJobDescription(\"Read 1 file with 400m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 4 files a 101,5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_experiment(maxPartitionsMB, path, file_size, num_files):\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "    sdf_load = spark.read.parquet(path)\n",
    "    print(f\"Number partitions: {sdf_load.rdd.getNumPartitions()}\")\n",
    "    sc.setJobDescription(f\"Read {num_files} file with {maxPartitionsMB} MB\")\n",
    "    sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    partition_size = (file_size)/4\n",
    "    target_partition_size = min(maxPartitionsMB, partition_size)\n",
    "    print(target_partition_size)\n",
    "    print((file_size)/target_partition_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_files = 1\n",
    "num_rows = 10000000\n",
    "size = 406\n",
    "sdf = sdf_generator(num_rows, 1)\n",
    "path = f\"{base_dir}/test_{num_files}_file.parquet\"\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(f\"Write {num_files} files, {size} MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files written: 3\n",
      "Num rows written: 100\n",
      "Num partitions written: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/Spark/Data/3_files_100_rows.parquet'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_generator(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', blocks=1, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0, createdBy='parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)', schema='message spark_schema {\\n  required int64 id;\\n  required int32 date (DATE);\\n  required int96 timestamp;\\n  required binary idstring (STRING);\\n  required binary idfirst (STRING);\\n  required binary idlast (STRING);\\n}\\n', encryption='UNENCRYPTED', keyValues={'org.apache.spark.version': '3.5.0', 'org.apache.spark.sql.parquet.row.metadata': '{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"date\",\"type\":\"date\",\"nullable\":false,\"metadata\":{}},{\"name\":\"timestamp\",\"type\":\"timestamp\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idstring\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idfirst\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idlast\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}}]}'})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_metadata(path).dropDuplicates([\"filename\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.76440238952637"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "84687614/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(partition=0, start=0, end=107202443, length=107202443, blocks=1, compressedBytes=133949814, uncompressedBytes=312755950, rows=15790100, columns=6, values=94740600, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=1, start=107202443, end=214404886, length=107202443, blocks=1, compressedBytes=133895545, uncompressedBytes=323456519, rows=15770100, columns=6, values=94620600, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=2, start=214404886, end=321607329, length=107202443, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=3, start=321607329, end=424615468, length=103008139, blocks=2, compressedBytes=156390510, uncompressedBytes=378213943, rows=18439800, columns=6, values=110638800, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_partitions(path).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', block=1, blockStart=4, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_blocks(path).dropDuplicates([\"filename\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration.setInt(\"parquet.block.size\",blockSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.sql.parquet.block.size\" cannot be found. Please verify that the config exists.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\r\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5234)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.parquet.block.size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.sql.parquet.block.size\" cannot be found. Please verify that the config exists.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\r\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5234)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.parquet.block.size\")\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125829120"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 120*1024*1024\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424235872\n"
     ]
    }
   ],
   "source": [
    "b1 = 4\n",
    "b2 = 133949818\n",
    "b3 = 267845363\n",
    "b4 = 401778296\n",
    "e = b4 + 22457577 -1\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66974911.0\n",
      "200897590.5\n",
      "334811829.5\n",
      "413007084.0\n"
     ]
    }
   ],
   "source": [
    "m1 = (b1+b2)/2\n",
    "m2 = (b2+b3)/2\n",
    "m3 = (b3+b4)/2\n",
    "m4 = (b4+e)/2\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "print(m3)\n",
    "print(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503316480"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377487360"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8120698"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949818-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 4\n",
      "110.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "read_experiment(120, path, 442, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.74449729919434"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949814/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(partition=0, start=0, end=22238867, length=22238867, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=1, start=22238867, end=44477734, length=22238867, blocks=1, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=2, start=44477734, end=66716601, length=22238867, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=3, start=66716601, end=84761164, length=18044563, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_partitions(path).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.1456127166748"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "156390510/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.74449729919434"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949814/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404.5828523635864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "424235869/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 9\n",
      "2.5\n",
      "4.0\n",
      "Number partitions: 5\n",
      "5.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "7.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "17.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "25.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "32.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "50.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [10, 20, 30, 70, 100, 128, 200]:\n",
    "    read_experiment(size, path, size, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 4 file, 101,5 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_4_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 4 file, 20 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_20_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 52\n",
      "8\n",
      "52.75\n",
      "Number partitions: 26\n",
      "16\n",
      "26.375\n",
      "Number partitions: 14\n",
      "32\n",
      "13.1875\n",
      "Number partitions: 8\n",
      "64\n",
      "6.59375\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [8, 16, 32, 64, 128, 256, 512]:\n",
    "    read_experiment(size, f\"{base_dir}/test_4_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 60\n",
      "8\n",
      "50.75\n",
      "Number partitions: 30\n",
      "16\n",
      "25.375\n",
      "Number partitions: 20\n",
      "32\n",
      "12.6875\n",
      "Number partitions: 10\n",
      "64\n",
      "6.34375\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [8, 16, 32, 64, 128, 256, 512]:\n",
    "    read_experiment(size, f\"{base_dir}/test_20_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_experiment(128, f\"{base_dir}/test_4_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_experiment(64, f\"{base_dir}/test_4_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "sc.setJobDescription(\"Read 4 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"70m\")\n",
    "sc.setJobDescription(\"Read 4 file with 70m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"51m\")\n",
    "sc.setJobDescription(\"Read 4 file with 51m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 20 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 20 file, 20,3 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_20_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "sc.setJobDescription(\"Read 20 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"70m\")\n",
    "sc.setJobDescription(\"Read 20 file with 70m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"51m\")\n",
    "sc.setJobDescription(\"Read 20 file with 51m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"200m\")\n",
    "sc.setJobDescription(\"Read 20 file with 200m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"400m\")\n",
    "sc.setJobDescription(\"Read 20 file with 400m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
