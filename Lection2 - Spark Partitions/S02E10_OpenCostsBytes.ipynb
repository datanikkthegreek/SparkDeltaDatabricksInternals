{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How to set your configs right: Open Cost In Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    return sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path, desc = \"\"):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(f\"WRITE {desc}\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    print(f\"Duration: {duration} sec\")\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    \n",
    "    file_size_padded = file_size + num_files * openCostInBytes\n",
    "    size_per_core_padded = file_size_padded / minPartitionNum\n",
    "    max_partition_size = int(min(maxPartitionBytes, max(openCostInBytes, size_per_core_padded)))\n",
    "    no_partitions_padded = file_size_padded/max_partition_size\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\")\n",
    "    print(f\"Padded File Size: {round(file_size_padded/1024/1024, 1)} MB or {file_size_padded} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(size_per_core_padded/1024/1024, 1)} MB or {size_per_core_padded} bytes\")\n",
    "    print(f\"MaxPartionsize: {round(max_partition_size/1024/1024, 1)} MB or {max_partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitionsAvg: {math.ceil(no_partitions_padded)}, unrounded: {no_partitions_padded}\")\n",
    "    return max_partition_size, size_per_core_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_as_list(parquet_meta_data_sdf):\n",
    "    return list(parquet_meta_data_sdf.select(\"compressedBytes\").toPandas()[\"compressedBytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(files_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtil.scala#L45\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    for file_size in files_list:\n",
    "        remaining = file_size - max_split_bytes\n",
    "        f = {\n",
    "            \"start\": 0,\n",
    "            \"length\": min(max_split_bytes, file_size),\n",
    "            \"file_size\": file_size\n",
    "        }\n",
    "        result_list.append(f)\n",
    "        while remaining > 0:\n",
    "            if remaining > max_split_bytes:\n",
    "                f = {\n",
    "                    \"start\": file_size - remaining,\n",
    "                    \"length\": max_split_bytes,\n",
    "                    \"file_size\": 0\n",
    "                }\n",
    "                result_list.append(f)\n",
    "            else:\n",
    "                f = {\n",
    "                    \"start\": file_size - remaining,\n",
    "                    \"length\": remaining,\n",
    "                    \"file_size\": 0\n",
    "                }\n",
    "                result_list.append(f)  \n",
    "            remaining = remaining - max_split_bytes\n",
    "    return sorted(result_list, key=lambda d: d['length'], reverse=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilePartitions(splitted_files_list, max_split_bytes):\n",
    "    \"\"\"\n",
    "    Reference to code in GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n",
    "    \"\"\"\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    partitions = []\n",
    "    current_files = []\n",
    "    current_size = 0\n",
    "\n",
    "    def close_partition():\n",
    "        if current_files:\n",
    "            partition_details = {\n",
    "                \"files\": current_files.copy(),\n",
    "                \"num_files\": len(current_files),\n",
    "            }\n",
    "        else:\n",
    "            partition_details = {}\n",
    "        partitions.append(partition_details)\n",
    "        current_files.clear()\n",
    "\n",
    "    for file in splitted_files_list:\n",
    "        if current_size + file[\"length\"] > max_split_bytes:\n",
    "            close_partition()\n",
    "            current_size = 0\n",
    "        current_size += file[\"length\"] + openCostInBytes\n",
    "        current_files.append(file)\n",
    "    close_partition()\n",
    "    print(f\"Number calculated Partitions: {len(partitions)}\")\n",
    "    return partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_partitions_analysis(file_partitions):\n",
    "    pdf = pd.DataFrame(file_partitions)\n",
    "    pdf[\"partition\"] = pdf.index\n",
    "    sdf_partitions = spark.createDataFrame(pdf)\n",
    "    sdf_partitions = sdf_partitions.withColumn(\"files\", f.explode(sdf_partitions.files))\n",
    "    sdf_partitions = (sdf_partitions\n",
    "                    .withColumn(\"start\", f.col(\"files\").start)\n",
    "                    .withColumn(\"length\", f.col(\"files\").length)\n",
    "                    .withColumn(\"file_size\", f.col(\"files\").file_size)\n",
    "                    .drop(\"files\", \"num_files\")\n",
    "                    .withColumn(\"virt_num_files\", f.lit(1))\n",
    "                    .withColumn(\"real_num_files\", f.when(f.col(\"file_size\") > 0, f.lit(1)).otherwise(f.lit(0)))\n",
    "                    .select(\"partition\", \"start\", \"length\", \"file_size\", \"real_num_files\", \"virt_num_files\")\n",
    "    )\n",
    "    #sdf_partitions.show()\n",
    "    sdf_agg = sdf_partitions.groupBy(\"partition\").agg(f.sum(\"file_size\")).withColumnRenamed(\"sum(file_size)\", \"file_size\")\n",
    "    sdf_agg = sdf_agg.orderBy(\"partition\")\n",
    "    sdf_agg = sdf_agg.withColumn(\"file_size_mb\", f.col(\"file_size\")/1024/1024)\n",
    "    #sdf_agg.show()\n",
    "    max_size = sdf_agg.select(f.max(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    min_size = sdf_agg.select(f.min(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    median_size = sdf_agg.select(f.median(sdf_agg[\"file_size\"])).collect()[0][0]\n",
    "    sdf_empty = sdf_agg.filter(f.col(\"file_size\") == 0)\n",
    "    empty_partitions = sdf_empty.count()\n",
    "    return {\n",
    "        \"max_size\": max_size,\n",
    "        \"min_size\": min_size,\n",
    "        \"median_size\": median_size,\n",
    "        \"num_partitions\": len(file_partitions),\n",
    "        \"empty_partition\": empty_partitions\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Max Cost Per Bytes\n",
    "  - Represents the cost of creating a new partition\n",
    "  - based on the config \"spark.sql.files.openCostInBytes\"\n",
    "  - defaults to 4 MB\n",
    "  - Technically it adds the cost, e.g. 4 MB, to each file which is called padding\n",
    "  - Through this less but bigger partitions are created around the size of the open cost value\n",
    "  - Usually no influence, except of smaller files, default of 4MB works\n",
    "  - Official description: The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimate, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first). This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "- https://stackoverflow.com/questions/69034543/number-of-tasks-while-reading-hdfs-in-spark\n",
    "- https://stackoverflow.com/questions/75924368/skewed-partitions-when-setting-spark-sql-files-maxpartitionbytes\n",
    "- https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "- https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4500\n",
      "Saved Path: D:/Spark/Data/4500_files_256000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 4500\n",
    "num_rows = 256000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------+------+---------------+-----+------------+-------------+\n",
      "|filename                                                                                                                 |blocks|compressedBytes|rows |compressedMB|calcNumBlocks|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+------+---------------+-----+------------+-------------+\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-04275-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |509992         |56888|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-01861-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |509449         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03992-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |508983         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03272-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |508909         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03746-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |507754         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03611-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |507628         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-02391-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |506860         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-04342-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |506712         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-01965-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |505502         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03554-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |504450         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03424-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |504295         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03835-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |504132         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-03347-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |504035         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-02556-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |503823         |56888|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-01445-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |483484         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-00696-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |482868         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-00793-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |481493         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-01160-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |480442         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-01430-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |479998         |56889|0.5         |0.00390625   |\n",
      "|file:/D:/Spark/Data/4500_files_256000000_rows.parquet/part-00821-b0e09747-0806-42af-a3e5-3397ed107c14-c000.snappy.parquet|1     |479732         |56889|0.5         |0.00390625   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+------+---------------+-----+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 2130.1 MB or 2233524433 bytes\n",
      "Num files: 4500\n",
      "Avg file Size: 0.5 MB or 496338.7628888889 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4500\n",
      "Num rows written: 256000000\n",
      "Num rows per file: 56888\n"
     ]
    }
   ],
   "source": [
    "path = \"D:/Spark/Data/4500_files_256000000_rows.parquet\"\n",
    "num_files = 4500\n",
    "num_rows = 256000000\n",
    "sdf_meta_data = get_parquet_meta_data(path)\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "file_size = get_parquet_file_size(path)\n",
    "file_size_list = get_files_as_list(sdf_meta_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Open Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 20 MB or 20971520 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 92130.1 MB or 96605364433 bytes\n",
      "SizePerCorePadded: 23032.5 MB or 24151341108.25 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 720, unrounded: 719.7660538032651\n",
      "Number calculated Partitions: 643\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 643\n",
      "Duration: 31.99 sec\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 10 MB or 10485760 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 47130.1 MB or 49419444433 bytes\n",
      "SizePerCorePadded: 11782.5 MB or 12354861108.25 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 369, unrounded: 368.2035538032651\n",
      "Number calculated Partitions: 347\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 347\n",
      "Duration: 27.57 sec\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 8 MB or 8388608 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 38130.1 MB or 39982260433 bytes\n",
      "SizePerCorePadded: 9532.5 MB or 9995565108.25 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 298, unrounded: 297.8910538032651\n",
      "Number calculated Partitions: 282\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 282\n",
      "Duration: 28.65 sec\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 5 MB or 5242880 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ADVANCED ALGORITHM TO ESTIMATE Partition SIZE AND NO PARTITIONS ********\n",
      "Padded File Size: 24630.1 MB or 25826484433 bytes\n",
      "SizePerCorePadded: 6157.5 MB or 6456621108.25 bytes\n",
      "MaxPartionsize: 128.0 MB or 134217728 bytes\n",
      "EstimatedPartitionsAvg: 193, unrounded: 192.4223038032651\n",
      "Number calculated Partitions: 188\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 188\n",
      "Duration: 27.8 sec\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_dict = {}\n",
    "for openCostMB in [20, 10, 8, 5, 4, 3, 2, 1, 0.5, 0.25, 0.1, 0.01, 0.001, 0]:\n",
    "    maxMB = 128\n",
    "    minPartitions = 4\n",
    "    set_configs(maxPartitionsMB=maxMB, openCostInMB=openCostMB, minPartitions=minPartitions)\n",
    "    max_split_bytes, size_per_core_padded = maxSplitBytes(file_size, num_files)\n",
    "    splitted_files = split_files(file_size_list, max_split_bytes)\n",
    "    file_partitions = getFilePartitions(splitted_files, max_split_bytes)\n",
    "    results_dict = file_partitions_analysis(file_partitions)\n",
    "    results_dict[\"maxPartitionMB\"] = maxMB\n",
    "    results_dict[\"openCosts\"] = openCostMB\n",
    "    results_dict[\"size_per_core_padded\"] = round(size_per_core_padded/1024/1024,1)\n",
    "    results_dict[\"maxSplitBytes\"] = round(max_split_bytes/1024/1024,1)\n",
    "    results_dict[\"actualNumberPartitions\"] = get_actual_num_partitions(path)\n",
    "    results_dict[\"ExecutionTime\"] = noop_write(path, str(maxMB))\n",
    "    #bytes_rows_per_partition(path)\n",
    "    results.append(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "|maxPartitionMB|openCosts|size_per_core_padded|maxSplitBytes|num_partitions|actualNumberPartitions|empty_partition|min_size|median_size|max_size|ExecutionTime|\n",
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "|           128|       20|             23032.5|        128.0|           643|                   643|              0|     2.7|        3.4|     3.4|        31.99|\n",
      "|           128|       10|             11782.5|        128.0|           347|                   347|              0|     0.9|        6.3|     6.3|        27.57|\n",
      "|           128|        8|              9532.5|        128.0|           282|                   282|              0|     1.8|        7.7|     7.8|        28.65|\n",
      "|           128|        5|              6157.5|        128.0|           188|                   188|              0|     5.4|       11.5|    11.7|         27.8|\n",
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "sdf_results = spark.createDataFrame(pdf)\n",
    "sdf_results = sdf_results.withColumn(\"min_size\", f.round(f.col(\"min_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"median_size\", f.round(f.col(\"median_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.withColumn(\"max_size\", f.round(f.col(\"max_size\")/1024/1024,1))\n",
    "sdf_results = sdf_results.select(\"maxPartitionMB\", \"openCosts\", \"size_per_core_padded\", \"maxSplitBytes\", \"num_partitions\", \"actualNumberPartitions\", \"empty_partition\", \"min_size\", \"median_size\", \"max_size\", \"ExecutionTime\")\n",
    "sdf_results.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"D:/Spark/Data/ResultsOpenCosts/open3.parquet\"\n",
    "sdf_results.write.format(\"parquet\").save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "|maxPartitionMB|openCosts|size_per_core_padded|maxSplitBytes|num_partitions|actualNumberPartitions|empty_partition|min_size|median_size|max_size|ExecutionTime|\n",
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "|           128|     20.0|             23032.5|        128.0|           643|                   643|              0|     2.7|        3.4|     3.4|        31.99|\n",
      "|           128|     10.0|             11782.5|        128.0|           347|                   347|              0|     0.9|        6.3|     6.3|        27.57|\n",
      "|           128|      8.0|              9532.5|        128.0|           282|                   282|              0|     1.8|        7.7|     7.8|        28.65|\n",
      "|           128|      5.0|              6157.5|        128.0|           188|                   188|              0|     5.4|       11.5|    11.7|         27.8|\n",
      "|           128|      4.0|              5032.5|        128.0|           156|                   156|              0|     2.3|       14.0|    14.1|        27.94|\n",
      "|           128|      3.0|              3907.5|        128.0|           122|                   122|              0|    10.4|       17.8|    18.0|        29.44|\n",
      "|           128|      2.0|              2782.5|        128.0|            87|                    87|              0|    12.7|       25.0|    25.3|        29.62|\n",
      "|           128|      1.0|              1657.5|        128.0|            52|                    52|              0|    26.4|       41.6|    42.0|        29.15|\n",
      "|           128|      0.5|              1095.0|        128.0|            35|                    35|              0|    14.5|       62.6|    63.2|        29.96|\n",
      "|           128|     0.25|               813.8|        128.0|            26|                    26|              0|    37.3|       84.1|    84.6|        30.83|\n",
      "|           128|      0.1|               645.0|        128.0|            21|                    21|              0|    19.1|      105.8|   106.2|        28.89|\n",
      "|           128|     0.01|               543.8|        128.0|            18|                    18|              0|     2.3|      125.1|   125.4|         31.1|\n",
      "|           128|    0.001|               533.6|        128.0|            17|                    17|              0|    89.8|      127.5|   127.7|        30.77|\n",
      "|           128|      0.0|               532.5|        128.0|            17|                    17|              0|    85.2|      127.8|   128.0|        30.43|\n",
      "+--------------+---------+--------------------+-------------+--------------+----------------------+---------------+--------+-----------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_path = f\"D:/Spark/Data/ResultsOpenCosts/open3.parquet\"\n",
    "sdf = spark.read.parquet(load_path)\n",
    "sdf.orderBy(f.col(\"openCosts\"), ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.998444790046657"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4500/643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
