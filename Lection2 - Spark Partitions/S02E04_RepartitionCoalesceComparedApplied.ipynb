{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Spark Partitioning in action with Repartion and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen = sdf_generator(20)\n",
    "sdf_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-01-16|2024-01-16 21:08:...|       0|      0|     0|\n",
      "|  1|2024-01-16|2024-01-16 21:08:...|       1|      1|     1|\n",
      "|  2|2024-01-16|2024-01-16 21:08:...|       2|      2|     2|\n",
      "|  3|2024-01-16|2024-01-16 21:08:...|       3|      3|     3|\n",
      "|  4|2024-01-16|2024-01-16 21:08:...|       4|      4|     4|\n",
      "|  5|2024-01-16|2024-01-16 21:08:...|       5|      5|     5|\n",
      "|  6|2024-01-16|2024-01-16 21:08:...|       6|      6|     6|\n",
      "|  7|2024-01-16|2024-01-16 21:08:...|       7|      7|     7|\n",
      "|  8|2024-01-16|2024-01-16 21:08:...|       8|      8|     8|\n",
      "|  9|2024-01-16|2024-01-16 21:08:...|       9|      9|     9|\n",
      "| 10|2024-01-16|2024-01-16 21:08:...|      10|      1|     0|\n",
      "| 11|2024-01-16|2024-01-16 21:08:...|      11|      1|     1|\n",
      "| 12|2024-01-16|2024-01-16 21:08:...|      12|      1|     2|\n",
      "| 13|2024-01-16|2024-01-16 21:08:...|      13|      1|     3|\n",
      "| 14|2024-01-16|2024-01-16 21:08:...|      14|      1|     4|\n",
      "| 15|2024-01-16|2024-01-16 21:08:...|      15|      1|     5|\n",
      "| 16|2024-01-16|2024-01-16 21:08:...|      16|      1|     6|\n",
      "| 17|2024-01-16|2024-01-16 21:08:...|      17|      1|     7|\n",
      "| 18|2024-01-16|2024-01-16 21:08:...|      18|      1|     8|\n",
      "| 19|2024-01-16|2024-01-16 21:08:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recap partitioning\n",
    "\n",
    "## The most important thing you want a good parallisation. \n",
    "- This means your number of partitions should always depend on the number of cores you have available. In spark language: spark.sparkContext.defaultParallelism. Recommendations are a factor of 2-4. But really depends on memory and data size. Small data sizes run perfectly with a factor 1x.\n",
    "- To have a good parallisation you should also have a well (best uniform, worst case normal) distributed dataset. Data skew can even in narrow transformations already make your whole execution dependend on one partition or task as we saw before\n",
    "## Partition size\n",
    "- If your partition size is really big > 1GB you might have OOM (out of memory), Garbage collection (GC) and other errors\n",
    "- Recommendations in the internet say anything between 100-1000 MB. Spark sets his max partition bytes parameter for example to 128 MB. It really depends on your machine and available memory of course. Definitly don't scratch the limits of available memory.\n",
    "## Distribution overhead\n",
    "- As we saw in previous experiments a to high number of partitions leads to a lot of scheduling and distribution overhead.\n",
    "- A good sign is if your actual aexecution time makes not at least 90 % of the total task time. Also if your tasks are below 100 ms it's usually to short\n",
    "\n",
    "See also here: https://stackoverflow.com/questions/64600212/how-to-determine-the-partition-size-in-an-apache-spark-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How repartitioning works\n",
    "- Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html#pyspark.sql.DataFrame.repartition\n",
    "- Repartition allows to increase and decrease the number of partitions\n",
    "- Repartition requires shuffling data which can be more unefficient than Coalesce\n",
    "- On the other hand it creates uniform distributions unlike coalesce which only unions partions together\n",
    "- Instead of partition based on the number of partitions you can partition based on a number of rows\n",
    "- If no number of partitions is defined the default value depends on spark.sql.shuffle.partitions which defaults to 200 (important later when evaluating wide transformations in later episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How coalesce works\n",
    "- Documentation: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html\n",
    "- Narrow transformation\n",
    "- Can only reduce not inrcease no. partitions. It does not give you and error but it just ignores a value higher than the initially available partitions\n",
    "- Coalesce can skew the data within each partition which leads to lower performance and some tasks running way longer. Reason it just unions the partitions together.\n",
    "- Coalesce can help with efficiently reducing high number of small partitions and improve performance. Remember a too high number of partitions leads to a lot of scheduling overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. When to use repartioning and coalesce\n",
    "- You want rebalance your data across another number of paritions as explained before to depend on the number of cores, e.g. initially you have 7 partitions but 4 cores you would increase to 8 (today)\n",
    "- You have data skew which leads to one partition/task running longer than the others. It also can affect joins and other wide transformations. Repartitioning will handle this. (today)\n",
    "- Join operations can benefit of repartioning the data beforehand. Repartioning reduces the shuffling of data during the join. But also other shuffle operations based on a column key like order by (will learn this in a later episode)\n",
    "- Filter operations can become more efficient. (today)\n",
    "- bigger filter operations can lead to a lot of empty partitions. E.g. you have 10 Mio of rows and 1000 partitions. After a filter 10 rows are left. This would be suddenly an overhead for any following operation e.g. a count. (today)\n",
    "- Exploding of structured fields in a dataframe can increase the partition size. (later)\n",
    "- Optimize or influence your writes. You will learn later writes depend on the number of partitions. High number of partitions can make your writes unoptimal. The file creation e.g. with parquet depends also on the number of partitions just before the write. (will learn this in a later episode)\n",
    "\n",
    "See also here: https://medium.com/@zaiderikat/apache-spark-repartitioning-101-f2b37e7d8301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reducing the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 200000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Scenario 1\n",
    "\n",
    "12 is our target partition size but we have 13 partitions as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 13)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 13\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 13).coalesce(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce 13 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 13).repartition(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition 13 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Scenario 2\n",
    "\n",
    "12 is our target partition size but we have 20001 partitions as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20001\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20001)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 20001\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20001).coalesce(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce 20001 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20001).repartition(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition 20001 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scenario 3\n",
    "\n",
    "40 is our target partition size but we have 90 partitions as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 40)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 40\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 90)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 90\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 90).coalesce(40)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce 90 to 40\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 90).repartition(40)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition 90 to 40\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Increasing the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 1\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 10)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 10\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 1).repartition(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition 1 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 10).repartition(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition 10 to 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Skew\n",
    "\n",
    "filter or coalesce to generate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 15)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 15\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 15)\n",
    "sdf = sdf.coalesce(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line skewed 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 15)\n",
    "sdf = sdf.coalesce(12)\n",
    "sdf = sdf.coalesce(8)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce for Skew 8\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 15)\n",
    "sdf = sdf.coalesce(12)\n",
    "sdf = sdf.repartition(12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition for Skew 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 15)\n",
    "sdf = sdf.coalesce(12)\n",
    "sdf = sdf.repartition(8)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition for Skew 8\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filter operations become more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.filter(f.col(\"id\") < 1000)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12 with filter id\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.repartition(12, \"id\")\n",
    "sdf = sdf.filter(f.col(\"id\") < 1000)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Repartition filter 12 id\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.filter(f.col(\"idfirst\") == \"1\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12 with filter idfirst\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.repartition(12, \"idfirst\")\n",
    "sdf = sdf.filter(f.col(\"idfirst\") == \"1\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Reaprtition filter 12 idfirst\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.filter(f.col(\"idlast\") == \"1\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 12 with filter idlast\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.repartition(12, \"idlast\")\n",
    "sdf = sdf.filter(f.col(\"idlast\") == \"1\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Reaprtition filter 12 idlast\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Bigger filter operations and empty partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 20\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+\n",
      "|partition_id|   count|count_perc|\n",
      "+------------+--------+----------+\n",
      "|           0|10000000|       5.0|\n",
      "|           1|10000000|       5.0|\n",
      "|           2|10000000|       5.0|\n",
      "|           3|10000000|       5.0|\n",
      "|           4|10000000|       5.0|\n",
      "|           5|10000000|       5.0|\n",
      "|           6|10000000|       5.0|\n",
      "|           7|10000000|       5.0|\n",
      "|           8|10000000|       5.0|\n",
      "|           9|10000000|       5.0|\n",
      "|          10|10000000|       5.0|\n",
      "|          11|10000000|       5.0|\n",
      "|          12|10000000|       5.0|\n",
      "|          13|10000000|       5.0|\n",
      "|          14|10000000|       5.0|\n",
      "|          15|10000000|       5.0|\n",
      "|          16|10000000|       5.0|\n",
      "|          17|10000000|       5.0|\n",
      "|          18|10000000|       5.0|\n",
      "|          19|10000000|       5.0|\n",
      "+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 4\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+\n",
      "|partition_id|   count|count_perc|\n",
      "+------------+--------+----------+\n",
      "|           0|50000000|      25.0|\n",
      "|           1|50000000|      25.0|\n",
      "|           2|50000000|      25.0|\n",
      "|           3|50000000|      25.0|\n",
      "+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line 200 filter\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|  200|     100.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "sdf = sdf.coalesce(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"4 coalesce\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|  200|     100.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "sdf = sdf.repartition(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"4 repartition\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|   50|      25.0|\n",
      "|           1|   50|      25.0|\n",
      "|           2|   50|      25.0|\n",
      "|           3|   50|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Bigger filtering and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "200000000\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line Count 20\")\n",
    "print(sdf.count())\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "200000000\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line Count 4\")\n",
    "print(sdf.count())\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Base line count 20 filter\")\n",
    "print(sdf.count())\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "sdf = sdf.coalesce(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"4 coalesce count\")\n",
    "print(sdf.count())\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "sdf = sdf.repartition(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"4 repartition count\")\n",
    "print(sdf.count())\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Final comments\n",
    "- We have to balance performance of the current process vs the data distribution. That's where following processes e.g. by saving the data for data queries or operations like joins, sorts etc. will benefit from.\n",
    "- Even if we deep dive here. Some operations make sense when you realize performance significant bottle necks. E.g. if you gain 10 sec execution time is it worth? But if a small amount of data runs 3h daily and you reduce it to 15 min then it's worth to improve. \n",
    "- Something you should ways have a look on with some quick look in the Spark UI:\n",
    "    - Is a lot of driver memory consumed, meaning driver execution like collect()? (later)\n",
    "    - Are all cores used?\n",
    "    - Do you have spill to disk? (later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
