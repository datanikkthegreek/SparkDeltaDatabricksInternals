{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark Parititions influence saving data with parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen = sdf_generator(20)\n",
    "sdf_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-01-17|2024-01-17 22:05:...|       0|      0|     0|\n",
      "|  1|2024-01-17|2024-01-17 22:05:...|       1|      1|     1|\n",
      "|  2|2024-01-17|2024-01-17 22:05:...|       2|      2|     2|\n",
      "|  3|2024-01-17|2024-01-17 22:05:...|       3|      3|     3|\n",
      "|  4|2024-01-17|2024-01-17 22:05:...|       4|      4|     4|\n",
      "|  5|2024-01-17|2024-01-17 22:05:...|       5|      5|     5|\n",
      "|  6|2024-01-17|2024-01-17 22:05:...|       6|      6|     6|\n",
      "|  7|2024-01-17|2024-01-17 22:05:...|       7|      7|     7|\n",
      "|  8|2024-01-17|2024-01-17 22:05:...|       8|      8|     8|\n",
      "|  9|2024-01-17|2024-01-17 22:05:...|       9|      9|     9|\n",
      "| 10|2024-01-17|2024-01-17 22:05:...|      10|      1|     0|\n",
      "| 11|2024-01-17|2024-01-17 22:05:...|      11|      1|     1|\n",
      "| 12|2024-01-17|2024-01-17 22:05:...|      12|      1|     2|\n",
      "| 13|2024-01-17|2024-01-17 22:05:...|      13|      1|     3|\n",
      "| 14|2024-01-17|2024-01-17 22:05:...|      14|      1|     4|\n",
      "| 15|2024-01-17|2024-01-17 22:05:...|      15|      1|     5|\n",
      "| 16|2024-01-17|2024-01-17 22:05:...|      16|      1|     6|\n",
      "| 17|2024-01-17|2024-01-17 22:05:...|      17|      1|     7|\n",
      "| 18|2024-01-17|2024-01-17 22:05:...|      18|      1|     8|\n",
      "| 19|2024-01-17|2024-01-17 22:05:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"D:/Spark/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How many files are written based on the number of partitions\n",
    "- Answer is Easy. Parquet devides the data in multiple snappy files based on the spark partitions. That's how it's able to write those files in parallel\n",
    "- Here you can find details on parquet: https://learncsdesigns.medium.com/understanding-apache-parquet-d722645cfe74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 1 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_1_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 4 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_4_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 12 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_12_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What does the Spark UI say?\n",
    "- We have 1 job with 1 stage per write\n",
    "- Also for writing the rule counts how to split into partitions. 1 partition is lower than 4 or 12 partitions on 4 cores. 12 partitions seems to have some overhead with 8 MB of data written only\n",
    "- In the SQL Tab we can also see how many files and how much data has been written. \n",
    "- In the stages we can see the number of rows and size written and the execution times of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using coalesce and Repartition to save data\n",
    "- Remember: repartitioning is an expense operation but might be helpfull for following processes.\n",
    "- For going down to one partition repartion(1) can be better than coalesce(1) as all cores are used for the processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.coalesce(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write with Coalesce 12 to 4 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_12_to_4_coalesce_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.coalesce(1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write with Coalesce 12 to 1 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_12_to_1_coalesce_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.repartition(4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write with Repartition 12 to 4 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_12_to_4_repartition_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 12)\n",
    "sdf = sdf.repartition(1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write with Repartition 12 to 1 file\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_12_to_1_repartition_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Empty partitions problem when writing\n",
    "- Spark is smart enough to only write partitions with actual record/data in it. E.g. after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.filter(f.col(\"id\") < 200)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Empty rows\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/emptyRows.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|  200|     100.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Repartitioning by column idfirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.repartition(10, \"idfirst\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"repartition 10 idfirst\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/repartition_10_idfirst.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "|partition_id| count|count_perc|\n",
      "+------------+------+----------+\n",
      "|           3|111111|   11.1111|\n",
      "|           4|111111|   11.1111|\n",
      "|           5|111112|   11.1112|\n",
      "|           6|222222|   22.2222|\n",
      "|           8|111111|   11.1111|\n",
      "|           9|333333|   33.3333|\n",
      "+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+--------+-------+------+\n",
      "| id|date|timestamp|idstring|idfirst|idlast|\n",
      "+---+----+---------+--------+-------+------+\n",
      "+---+----+---------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"{base_dir}/repartition_10_idfirst.parquet/part-00000-16469030-23e0-4ecb-91d1-7110f3967395-c000.snappy.parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(num_rows, 20)\n",
    "sdf = sdf.repartition(8, \"idfirst\")\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"repartition 8 idfirst\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/repartition_8_idfirst.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "|partition_id| count|count_perc|\n",
      "+------------+------+----------+\n",
      "|           0|111111|   11.1111|\n",
      "|           1|111111|   11.1111|\n",
      "|           2|222222|   22.2222|\n",
      "|           3|333334|   33.3334|\n",
      "|           5|111111|   11.1111|\n",
      "|           6|111111|   11.1111|\n",
      "+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
