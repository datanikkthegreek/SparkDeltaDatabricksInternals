{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: Repartition (and partition recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen = sdf_generator(20)\n",
    "sdf_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-01-14|2024-01-14 09:40:...|       0|      0|     0|\n",
      "|  1|2024-01-14|2024-01-14 09:40:...|       1|      1|     1|\n",
      "|  2|2024-01-14|2024-01-14 09:40:...|       2|      2|     2|\n",
      "|  3|2024-01-14|2024-01-14 09:40:...|       3|      3|     3|\n",
      "|  4|2024-01-14|2024-01-14 09:40:...|       4|      4|     4|\n",
      "|  5|2024-01-14|2024-01-14 09:40:...|       5|      5|     5|\n",
      "|  6|2024-01-14|2024-01-14 09:40:...|       6|      6|     6|\n",
      "|  7|2024-01-14|2024-01-14 09:40:...|       7|      7|     7|\n",
      "|  8|2024-01-14|2024-01-14 09:40:...|       8|      8|     8|\n",
      "|  9|2024-01-14|2024-01-14 09:40:...|       9|      9|     9|\n",
      "| 10|2024-01-14|2024-01-14 09:40:...|      10|      1|     0|\n",
      "| 11|2024-01-14|2024-01-14 09:40:...|      11|      1|     1|\n",
      "| 12|2024-01-14|2024-01-14 09:40:...|      12|      1|     2|\n",
      "| 13|2024-01-14|2024-01-14 09:40:...|      13|      1|     3|\n",
      "| 14|2024-01-14|2024-01-14 09:40:...|      14|      1|     4|\n",
      "| 15|2024-01-14|2024-01-14 09:40:...|      15|      1|     5|\n",
      "| 16|2024-01-14|2024-01-14 09:40:...|      16|      1|     6|\n",
      "| 17|2024-01-14|2024-01-14 09:40:...|      17|      1|     7|\n",
      "| 18|2024-01-14|2024-01-14 09:40:...|      18|      1|     8|\n",
      "| 19|2024-01-14|2024-01-14 09:40:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\", num_rows: int) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recap partitioning\n",
    "\n",
    "## The most important thing you want a good parallisation. \n",
    "- This means your number of partitions should always depend on the number of cores you have available. In spark language: spark.sparkContext.defaultParallelism. Recommendations are a factor of 2-4. But really depends on memory and data size. Small data sizes run perfectly with a factor 1x.\n",
    "- To have a good parallisation you should also have a well (best uniform, worst case normal) distributed dataset. Data skew can even in narrow transformations already make your whole execution dependend on one partition or task as we saw before\n",
    "## Partition size\n",
    "- If your partition size is really big > 1GB you might have OOM (out of memory), Garbage collection (GC) and other errors\n",
    "- Recommendations in the internet say anything between 100-1000 MB. Spark sets his max partition bytes parameter for example to 128 MB. It really depends on your machine and available memory of course. Definitly don't scratch the limits of available memory.\n",
    "## Distribution overhead\n",
    "- As we saw in previous experiments a to high number of partitions leads to a lot of scheduling and distribution overhead.\n",
    "- A good sign is if your actual aexecution time makes not at least 90 % of the total task time. Also if your tasks are below 100 ms it's usually to short\n",
    "\n",
    "See also here: https://stackoverflow.com/questions/64600212/how-to-determine-the-partition-size-in-an-apache-spark-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How repartitioning works\n",
    "- Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html#pyspark.sql.DataFrame.repartition\n",
    "- Repartition allows to increase and decrease the number of partitions\n",
    "- Repartition requires shuffling data which can be more unefficient than Coalesce\n",
    "- On the other hand it creates uniform distributions unlike coalesce which only unions partions together\n",
    "- Instead of partition based on the number of partitions you can partition based on a number of rows\n",
    "- If no number of partitions is defined the default value depends on spark.sql.shuffle.partitions which defaults to 200 (important later when evaluating wide transformations in later episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1 = sdf_generator(num_rows, 4)\n",
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "row_count = sdf1.count()\n",
    "print(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 5000|      25.0|\n",
      "|           1| 5000|      25.0|\n",
      "|           2| 5000|      25.0|\n",
      "|           3| 5000|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf1, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           0|      0|    1|     0.005|\n",
      "|           0|      1| 1111|     5.555|\n",
      "|           0|      2| 1111|     5.555|\n",
      "|           0|      3| 1111|     5.555|\n",
      "|           0|      4| 1111|     5.555|\n",
      "|           0|      5|  111|     0.555|\n",
      "|           0|      6|  111|     0.555|\n",
      "|           0|      7|  111|     0.555|\n",
      "|           0|      8|  111|     0.555|\n",
      "|           0|      9|  111|     0.555|\n",
      "|           1|      5| 1000|       5.0|\n",
      "|           1|      6| 1000|       5.0|\n",
      "|           1|      7| 1000|       5.0|\n",
      "|           1|      8| 1000|       5.0|\n",
      "|           1|      9| 1000|       5.0|\n",
      "|           2|      1| 5000|      25.0|\n",
      "|           3|      1| 5000|      25.0|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf1, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Baseline 4 partitions\")\n",
    "sdf1.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_3 = sdf1.repartition(3)\n",
    "sdf_3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 6667|    33.335|\n",
      "|           1| 6667|    33.335|\n",
      "|           2| 6666|     33.33|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_3, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_12 = sdf1.repartition(12)\n",
    "sdf_12.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 1667|     8.335|\n",
      "|           1| 1666|      8.33|\n",
      "|           2| 1666|      8.33|\n",
      "|           3| 1666|      8.33|\n",
      "|           4| 1667|     8.335|\n",
      "|           5| 1667|     8.335|\n",
      "|           6| 1667|     8.335|\n",
      "|           7| 1667|     8.335|\n",
      "|           8| 1666|      8.33|\n",
      "|           9| 1667|     8.335|\n",
      "|          10| 1667|     8.335|\n",
      "|          11| 1667|     8.335|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_12, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "sdf_col_200 = sdf1.repartition(\"idfirst\")\n",
    "sdf_col_200.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|          18| 1111|     5.555|\n",
      "|          26| 1111|     5.555|\n",
      "|          35|    1|     0.005|\n",
      "|          49| 1111|     5.555|\n",
      "|          75| 1111|     5.555|\n",
      "|         139| 1111|     5.555|\n",
      "|         144|11111|    55.555|\n",
      "|         166| 1111|     5.555|\n",
      "|         189| 1111|     5.555|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_200, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|          18|      3| 1111|     5.555|\n",
      "|          26|      8| 1111|     5.555|\n",
      "|          35|      0|    1|     0.005|\n",
      "|          49|      5| 1111|     5.555|\n",
      "|          75|      6| 1111|     5.555|\n",
      "|         139|      9| 1111|     5.555|\n",
      "|         144|      1|11111|    55.555|\n",
      "|         166|      4| 1111|     5.555|\n",
      "|         189|      2| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_200, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)\n",
    "sdf_col_20 = sdf1.repartition(\"idfirst\")\n",
    "sdf_col_20.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|           4|11111|    55.555|\n",
      "|           6| 2222|     11.11|\n",
      "|           9| 2222|     11.11|\n",
      "|          15| 1112|      5.56|\n",
      "|          18| 1111|     5.555|\n",
      "|          19| 1111|     5.555|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_20, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           6|      4| 1111|     5.555|\n",
      "|           6|      8| 1111|     5.555|\n",
      "|           9|      2| 1111|     5.555|\n",
      "|           9|      5| 1111|     5.555|\n",
      "|          15|      0|    1|     0.005|\n",
      "|          15|      6| 1111|     5.555|\n",
      "|          18|      3| 1111|     5.555|\n",
      "|          19|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_20, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_col_10 = sdf1.repartition(10, \"idfirst\")\n",
    "sdf_col_10.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|           4|11111|    55.555|\n",
      "|           5| 1112|      5.56|\n",
      "|           6| 2222|     11.11|\n",
      "|           8| 1111|     5.555|\n",
      "|           9| 3333|    16.665|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_10, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           5|      0|    1|     0.005|\n",
      "|           5|      6| 1111|     5.555|\n",
      "|           6|      4| 1111|     5.555|\n",
      "|           6|      8| 1111|     5.555|\n",
      "|           8|      3| 1111|     5.555|\n",
      "|           9|      2| 1111|     5.555|\n",
      "|           9|      5| 1111|     5.555|\n",
      "|           9|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_10, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_col_5 = sdf1.repartition(5, \"idfirst\")\n",
    "sdf_col_5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 1112|      5.56|\n",
      "|           1| 2222|     11.11|\n",
      "|           3| 2222|     11.11|\n",
      "|           4|14444|     72.22|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_5, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           0|      0|    1|     0.005|\n",
      "|           0|      6| 1111|     5.555|\n",
      "|           1|      4| 1111|     5.555|\n",
      "|           1|      8| 1111|     5.555|\n",
      "|           3|      3| 1111|     5.555|\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           4|      2| 1111|     5.555|\n",
      "|           4|      5| 1111|     5.555|\n",
      "|           4|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_5, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 3\")\n",
    "sdf_3.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 12\")\n",
    "sdf_12.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 5 with col\")\n",
    "sdf_col_5.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. When to use repartioning (and partly coalesce)\n",
    "- You want rebalance your data across another number of paritions as explained before to depend on the number of cores, e.g. initially you have 7 partitions but 4 cores you would increase to 8 (today)\n",
    "- You have data skew which leads to one partition/task running longer than the others. It also can affect joins and other wide transformations. Repartitioning will handle this. (today)\n",
    "- Join operations can benefit of repartioning the data beforehand. Repartioning reduces the shuffling of data during the join. (will learn this in a later episode)\n",
    "- Any operations doing a shuffle based on a column key, e.g. a join as explained but also an orderby can benefit of it. (today)\n",
    "- bigger filter operations can lead to a lot of empty partitions. E.g. you have 10 Mio of rows and 1000 partitions. After a filter 10 rows are left. This would be suddenly an overhead for any following operation. (today)\n",
    "- Optimize or influence your writes. You will learn later writes depend on the number of partitions. High number of partitions can make your writes optimal. The file creation e.g. with parquet depends also on the number of partitions just before the write. (will learn this in a later episode)\n",
    "\n",
    "See also here: https://medium.com/@zaiderikat/apache-spark-repartitioning-101-f2b37e7d8301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance comparison with Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Filter and Sorting\n",
    "orderby\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sdf3 = sdf_generator(num_rows, 3)\n",
    "print(sdf3.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"3 Partitions\")\n",
    "sdf3.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sdf4 = sdf_generator(num_rows, 8)\n",
    "print(sdf4.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"8 Partitions\")\n",
    "sdf4.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf5 = sdf4.coalesce(4)\n",
    "print(sdf5.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce from 8 to 4\")\n",
    "sdf5.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n"
     ]
    }
   ],
   "source": [
    "sdf6 = sdf_generator(num_rows, 200001)\n",
    "print(sdf6.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"200001 partitions\")\n",
    "sdf6.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf7 = sdf6.coalesce(4)\n",
    "print(sdf7.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Coalesce from 200001 to 4\")\n",
    "sdf7.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sdf8 = sdf_generator(num_rows, 40)\n",
    "print(sdf8.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"40 partitions\")\n",
    "sdf8.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. References\n",
    "- Interesting discussion on Stackoverflow of Coalesce vs Repartition speed: https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce\n",
    "- When to use repartition: https://medium.com/@zaiderikat/apache-spark-repartitioning-101-f2b37e7d8301\n",
    "- Factors to consider for no. of partitions: https://stackoverflow.com/questions/64600212/how-to-determine-the-partition-size-in-an-apache-spark-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
