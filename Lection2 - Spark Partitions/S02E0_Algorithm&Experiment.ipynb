{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark Parititions influence saving data with parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-Ups\n",
    "\n",
    "General hints for this notebook:\n",
    "- Spark UI usually accesible by http://localhost:4040/ or http://localhost:4041/\n",
    "- Deep dive Spark UI happens in later episodes\n",
    "- sc.setJobDescription(\"Description\") replaces the Job Description of an action in the Spark UI with your own\n",
    "- sdf.rdd.getNumPartitions() returns the number partitions of the current Spark DataFrame\n",
    "- sdf.write.format(\"noop\").mode(\"overwrite\").save() is a good way to analyze and initiate actions for transformations without side effects during an actual write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import gresearch.spark.parquet\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: \n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache datafrimes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\") -> None:\n",
    "    num_rows = sdf.count()\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n",
    "results_dict = {}\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_meta_data(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .withColumn(\"calcNumBlocks\", f.col(\"compressedMB\")/128)\n",
    "    )\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_blocks(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_blocks(path)\n",
    "        .dropDuplicates([\"filename\",\"block\"])\n",
    "        .orderBy(\"filename\", \"block\")\n",
    "        .withColumn(\"blockEnd\", f.col(\"blockStart\") + f.col(\"compressedBytes\") - 1)\n",
    "        .withColumn(\"blockMiddle\", f.col(\"blockStart\") + 0.5 * f.col(\"compressedBytes\"))\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"filename\", \"block\", \"blockStart\", \"blockEnd\", \"blockMiddle\", \"compressedBytes\", \"compressedMB\", \"rows\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_partitions(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"start\", \"end\", \"length\", \"blocks\", \"compressedBytes\", \"compressedMB\", \"rows\", \"filename\")\n",
    "    )\n",
    "\n",
    "    sdf.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_window_length(path):\n",
    "    sdf = spark.read.parquet_partitions(path)\n",
    "    val = sdf.select(f.max(sdf[\"length\"]))\n",
    "    max_length = val.collect()[0][0]\n",
    "    print(f\"Max Parquet window length: {round(max_length/1024/1024, 1)} MB or {max_length} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10**decimals\n",
    "    return math.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "#source: https://realpython.com/python-rounding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_num_partitions(file_size, num_files):\n",
    "    #get spark values\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    #Calculate maxSpliPartitionBytes\n",
    "    # a) If we have bigger files bytesPerCorePadded will be bigger then openCostInBytes but also maxPartitionBytes. \n",
    "    # In this case we would limit the size as mayPartitionBytes will be the result of maxSplitPartitionBytes. e.g. 1 GB dataset, 4 cores, maxPartitions 128 MB\n",
    "    # b) If bytesPerCorePadded is the result of maxSplitPartitionBytes we have a fair split of the data over all cores, e.g. 1 GB dataset, 4 cores, maxPartitions 300 MB\n",
    "    # c) If bytesPerCore is to small we want to limit amount of Partitions to be opened. This is the cost here.\n",
    "    paddedFileSize = file_size + num_files * openCostInBytes\n",
    "    bytesPerCorePadded = paddedFileSize / minPartitionNum\n",
    "    maxSplitPartitionBytes = min(maxPartitionBytes, max(openCostInBytes, bytesPerCorePadded))\n",
    "    #Estimation of partitions from Internet\n",
    "    estimated_num_partitions_int = paddedFileSize/maxSplitPartitionBytes\n",
    "    #Own Estimator\n",
    "    avg_file_size_padded = paddedFileSize/num_files\n",
    "    bytesPerCore = file_size / minPartitionNum\n",
    "    #Calculate number of files fitting into one partitions. Then calculate the number of partitions\n",
    "    files_per_partition = max(1, math.floor(maxSplitPartitionBytes/avg_file_size_padded))\n",
    "    estimated_num_partitions = num_files/files_per_partition\n",
    "    print(\" \")\n",
    "    print(\"******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\")\n",
    "    print(f\"Avg file Size Padded: {round(avg_file_size_padded/1024/1024, 1)} MB or {avg_file_size_padded} bytes\")\n",
    "    print(f\"Padded File Size: {round(paddedFileSize/1024/1024, 1)} MB or {paddedFileSize} bytes\")\n",
    "    print(f\"SizePerCore: {round(bytesPerCore/1024/1024, 1)} MB or {bytesPerCore} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(bytesPerCorePadded/1024/1024, 1)} MB or {bytesPerCorePadded} bytes\")\n",
    "    print(f\"MaxSplitPartitionBytes: {round(maxSplitPartitionBytes/1024/1024, 1)} MB or {maxSplitPartitionBytes} bytes\")\n",
    "    print(f\"MaxFilesPerPartition {files_per_partition}\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(estimated_num_partitions)}, unrounded: {estimated_num_partitions}\")\n",
    "    print(f\"EstimatedPartitionsInternet: {math.ceil(estimated_num_partitions_int)}, unrounded: {estimated_num_partitions_int}\")\n",
    "\n",
    "    results_dict[\"paddedFileSize\"] = round(paddedFileSize/1024/1024, 1)\n",
    "    results_dict[\"MBPerCore\"] = round(bytesPerCore/1024/1024, 1)\n",
    "    results_dict[\"MBPerCorePadded\"] = round(bytesPerCorePadded/1024/1024, 1)\n",
    "    results_dict[\"maxSplitPartitionBytes\"] = round(maxSplitPartitionBytes/1024/1024, 1)\n",
    "    results_dict[\"avg_file_size_padded\"] = round(avg_file_size_padded/1024/1024, 1)\n",
    "    results_dict[\"Maxfiles_per_partition\"] = files_per_partition\n",
    "    results_dict[\"MyEstimationPartitions\"] = math.ceil(estimated_num_partitions)\n",
    "    results_dict[\"InternetEstimationPartitions\"] = math.ceil(estimated_num_partitions_int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_bytes_rows_partition(sdf):\n",
    "    sdf = (\n",
    "        sdf.select(f.mean(\"numFiles\"), f.mean(\"compressedBytes\"), f.mean(\"rows\"))\n",
    "        .withColumn(\"avg(compressedMB)\", f.round(f.col(\"avg(compressedBytes)\")/1024/1204, 1))\n",
    "        .select(\"avg(numFiles)\", \"avg(compressedBytes)\", \"avg(compressedMB)\", \"avg(rows)\")\n",
    "    )\n",
    "    sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What influences the no. of partitions when loading parquet files\n",
    "- Num Cores\n",
    "- File Size\n",
    "- Num parquet files\n",
    "- Num of blocks/rowgroups within a parquet file\n",
    "- Max Partition Size\n",
    "- Max Cost Per Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First simple experiment\n",
    "- Experiment 1: 4 files, a 64 MB\n",
    "- Experiment 2: 8 files, a 64 MB (128 MB; 140 MB max size)\n",
    "- Experiment 3: 8 files, a 50 MB\n",
    "\n",
    "Set-Up:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: D:/Spark/Data/4_files_32000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 259.3 MB or 271863691 bytes\n",
      "Num files: 4\n",
      "Avg file Size: 64.8 MB or 67965922.75 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4\n",
      "Num rows written: 32000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 50 MB or 52428800 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 68.8 MB or 72160226.75 bytes\n",
      "Padded File Size: 275.3 MB or 288640907 bytes\n",
      "SizePerCore: 64.8 MB or 67965922.75 bytes\n",
      "SizePerCorePadded: 68.8 MB or 72160226.75 bytes\n",
      "MaxSplitPartitionBytes: 50.0 MB or 52428800 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 6, unrounded: 5.505388393402099\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 6\n",
      "Duration: 2.29 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       67726805|        64.6|8000000|\n",
      "|        1|       1|       68008198|        64.9|8000000|\n",
      "|        2|       1|       68064947|        64.9|8000000|\n",
      "|        3|       1|       68063741|        64.9|8000000|\n",
      "|        4|       2|              0|         0.0|      0|\n",
      "|        5|       2|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=50, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 8\n",
      "Saved Path: D:/Spark/Data/8_files_64000000_rows.parquet\n"
     ]
    }
   ],
   "source": [
    "num_files = 8\n",
    "num_rows = 64000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 518.9 MB or 544117207 bytes\n",
      "Num files: 8\n",
      "Avg file Size: 64.9 MB or 68014650.875 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 8\n",
      "Num rows written: 64000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 60 MB or 62914560 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 68.9 MB or 72208954.875 bytes\n",
      "Padded File Size: 550.9 MB or 577671639 bytes\n",
      "SizePerCore: 129.7 MB or 136029301.75 bytes\n",
      "SizePerCorePadded: 137.7 MB or 144417909.75 bytes\n",
      "MaxSplitPartitionBytes: 60.0 MB or 62914560 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 8, unrounded: 8.0\n",
      "EstimatedPartitionsInternet: 10, unrounded: 9.181843423843384\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 10\n",
      "Duration: 4.51 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       67726805|        64.6|8000000|\n",
      "|        1|       1|       68008198|        64.9|8000000|\n",
      "|        2|       1|       68064947|        64.9|8000000|\n",
      "|        3|       1|       68063741|        64.9|8000000|\n",
      "|        4|       1|       68063118|        64.9|8000000|\n",
      "|        5|       1|       68063528|        64.9|8000000|\n",
      "|        6|       1|       68063235|        64.9|8000000|\n",
      "|        7|       1|       68063635|        64.9|8000000|\n",
      "|        8|       7|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=60, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The algorithm\n",
    "\n",
    "With examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The experiment\n",
    "\n",
    "Experiment results\n",
    "\n",
    "e.g. first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 2007.6 MB or 2105075364 bytes\n",
      "Num files: 60\n",
      "Avg file Size: 33.5 MB or 35084589.4 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 60\n",
      "Num rows written: 240000000\n",
      "Num rows per file: 4000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 100 MB or 104857600 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 100.0 MB or 104857600 bytes\n",
      "MaxFilesPerPartition 2\n",
      "EstimatedPartitions: 30, unrounded: 30.0\n",
      "EstimatedPartitionsInternet: 23, unrounded: 22.475563087463378\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 30\n",
      "Duration: 21.12 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       2|       71696791|        68.4|8000000|\n",
      "|        1|       2|       71696497|        68.4|8000000|\n",
      "|        2|       2|       71696347|        68.4|8000000|\n",
      "|        3|       2|       71696335|        68.4|8000000|\n",
      "|        4|       2|       71696124|        68.4|8000000|\n",
      "|        5|       2|       71695861|        68.4|8000000|\n",
      "|        6|       2|       71695735|        68.4|8000000|\n",
      "|        7|       2|       71695702|        68.4|8000000|\n",
      "|        8|       2|       71695672|        68.4|8000000|\n",
      "|        9|       2|       71695565|        68.4|8000000|\n",
      "|       10|       2|       71695551|        68.4|8000000|\n",
      "|       11|       2|       71695428|        68.4|8000000|\n",
      "|       12|       2|       71695317|        68.4|8000000|\n",
      "|       13|       2|       71694833|        68.4|8000000|\n",
      "|       14|       2|       71694707|        68.4|8000000|\n",
      "|       15|       2|       71694593|        68.4|8000000|\n",
      "|       16|       2|       71694257|        68.4|8000000|\n",
      "|       17|       2|       69879696|        66.6|8000000|\n",
      "|       18|       2|       68064803|        64.9|8000000|\n",
      "|       19|       2|       68064542|        64.9|8000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "results_list = []\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=100, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)\n",
    "results_list.append(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 2007.6 MB or 2105075364 bytes\n",
      "Num files: 60\n",
      "Avg file Size: 33.5 MB or 35084589.4 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 60\n",
      "Num rows written: 240000000\n",
      "Num rows per file: 4000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxPartitionsBytes</th>\n",
       "      <th>paddedFileSize</th>\n",
       "      <th>MBPerCore</th>\n",
       "      <th>MBPerCorePadded</th>\n",
       "      <th>maxSplitPartitionBytes</th>\n",
       "      <th>avg_file_size_padded</th>\n",
       "      <th>Maxfiles_per_partition</th>\n",
       "      <th>MyEstimationPartitions</th>\n",
       "      <th>InternetEstimationPartitions</th>\n",
       "      <th>ActualNumPartitions</th>\n",
       "      <th>ExecutionTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>128.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>21.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maxPartitionsBytes  paddedFileSize  MBPerCore  MBPerCorePadded  \\\n",
       "0                 128          2247.6      501.9            561.9   \n",
       "\n",
       "   maxSplitPartitionBytes  avg_file_size_padded  Maxfiles_per_partition  \\\n",
       "0                   128.0                  37.5                     3.0   \n",
       "\n",
       "   MyEstimationPartitions  InternetEstimationPartitions  ActualNumPartitions  \\\n",
       "0                      20                            18                   20   \n",
       "\n",
       "   ExecutionTime  \n",
       "0          21.36  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "pd.DataFrame.from_dict(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 2007.6 MB or 2105075364 bytes\n",
      "Num files: 60\n",
      "Avg file Size: 33.5 MB or 35084589.4 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 60\n",
      "Num rows written: 240000000\n",
      "Num rows per file: 4000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 2 MB or 2097152 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 2.0 MB or 2097152 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 1124, unrounded: 1123.778154373169\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 1055\n",
      "Duration: 20.4 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|              0|         0.0|      0|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|              0|         0.0|      0|\n",
      "|        4|       1|              0|         0.0|      0|\n",
      "|        5|       1|              0|         0.0|      0|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|              0|         0.0|      0|\n",
      "|        8|       1|       33805282|        32.2|4000000|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "|       10|       1|              0|         0.0|      0|\n",
      "|       11|       1|              0|         0.0|      0|\n",
      "|       12|       1|              0|         0.0|      0|\n",
      "|       13|       1|              0|         0.0|      0|\n",
      "|       14|       1|              0|         0.0|      0|\n",
      "|       15|       1|              0|         0.0|      0|\n",
      "|       16|       1|              0|         0.0|      0|\n",
      "|       17|       1|              0|         0.0|      0|\n",
      "|       18|       1|              0|         0.0|      0|\n",
      "|       19|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 4 MB or 4194304 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 4.0 MB or 4194304 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 562, unrounded: 561.8890771865845\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 540\n",
      "Duration: 19.7 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|              0|         0.0|      0|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|              0|         0.0|      0|\n",
      "|        4|       1|       33805282|        32.2|4000000|\n",
      "|        5|       1|              0|         0.0|      0|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|              0|         0.0|      0|\n",
      "|        8|       1|              0|         0.0|      0|\n",
      "|        9|       1|              0|         0.0|      0|\n",
      "|       10|       1|              0|         0.0|      0|\n",
      "|       11|       1|              0|         0.0|      0|\n",
      "|       12|       1|       33921637|        32.4|4000000|\n",
      "|       13|       1|              0|         0.0|      0|\n",
      "|       14|       1|              0|         0.0|      0|\n",
      "|       15|       1|              0|         0.0|      0|\n",
      "|       16|       1|              0|         0.0|      0|\n",
      "|       17|       1|              0|         0.0|      0|\n",
      "|       18|       1|              0|         0.0|      0|\n",
      "|       19|       1|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 16 MB or 16777216 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 16.0 MB or 16777216 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 141, unrounded: 140.47226929664612\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 138\n",
      "Duration: 19.35 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|              0|         0.0|      0|\n",
      "|        1|       1|       33805282|        32.2|4000000|\n",
      "|        2|       1|              0|         0.0|      0|\n",
      "|        3|       1|       33921637|        32.4|4000000|\n",
      "|        4|       1|              0|         0.0|      0|\n",
      "|        5|       1|       33976681|        32.4|4000000|\n",
      "|        6|       1|              0|         0.0|      0|\n",
      "|        7|       1|       34031466|        32.5|4000000|\n",
      "|        8|       1|              0|         0.0|      0|\n",
      "|        9|       1|       34032314|        32.5|4000000|\n",
      "|       10|       1|              0|         0.0|      0|\n",
      "|       11|       1|       34032577|        32.5|4000000|\n",
      "|       12|       1|              0|         0.0|      0|\n",
      "|       13|       1|       34031401|        32.5|4000000|\n",
      "|       14|       1|              0|         0.0|      0|\n",
      "|       15|       1|       34032489|        32.5|4000000|\n",
      "|       16|       1|              0|         0.0|      0|\n",
      "|       17|       1|       34031404|        32.5|4000000|\n",
      "|       18|       1|              0|         0.0|      0|\n",
      "|       19|       1|       34031863|        32.5|4000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 20 MB or 20971520 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 20.0 MB or 20971520 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 113, unrounded: 112.3778154373169\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 120\n",
      "Duration: 20.57 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       33805282|        32.2|4000000|\n",
      "|        1|       1|       33921637|        32.4|4000000|\n",
      "|        2|       1|       33976681|        32.4|4000000|\n",
      "|        3|       1|       34031466|        32.5|4000000|\n",
      "|        4|       1|       34032314|        32.5|4000000|\n",
      "|        5|       1|       34032577|        32.5|4000000|\n",
      "|        6|       1|       34031401|        32.5|4000000|\n",
      "|        7|       1|       34032489|        32.5|4000000|\n",
      "|        8|       1|       34031404|        32.5|4000000|\n",
      "|        9|       1|       34031863|        32.5|4000000|\n",
      "|       10|       1|       34031947|        32.5|4000000|\n",
      "|       11|       1|       34031730|        32.5|4000000|\n",
      "|       12|       1|       34031876|        32.5|4000000|\n",
      "|       13|       1|       34031308|        32.5|4000000|\n",
      "|       14|       1|       34032233|        32.5|4000000|\n",
      "|       15|       1|       34031346|        32.5|4000000|\n",
      "|       16|       1|       34031755|        32.5|4000000|\n",
      "|       17|       1|       34031859|        32.5|4000000|\n",
      "|       18|       1|       34031339|        32.5|4000000|\n",
      "|       19|       1|       34032309|        32.5|4000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 32 MB or 33554432 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 32.0 MB or 33554432 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 71, unrounded: 70.23613464832306\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 70\n",
      "Duration: 21.13 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       33805282|        32.2|4000000|\n",
      "|        1|       1|       33921637|        32.4|4000000|\n",
      "|        2|       1|       33976681|        32.4|4000000|\n",
      "|        3|       1|       34031466|        32.5|4000000|\n",
      "|        4|       1|       34032314|        32.5|4000000|\n",
      "|        5|       1|       34032577|        32.5|4000000|\n",
      "|        6|       1|       34031401|        32.5|4000000|\n",
      "|        7|       1|       34032489|        32.5|4000000|\n",
      "|        8|       1|       34031404|        32.5|4000000|\n",
      "|        9|       1|       34031863|        32.5|4000000|\n",
      "|       10|       1|       34031947|        32.5|4000000|\n",
      "|       11|       1|       34031730|        32.5|4000000|\n",
      "|       12|       1|       34031876|        32.5|4000000|\n",
      "|       13|       1|       34031308|        32.5|4000000|\n",
      "|       14|       1|       34032233|        32.5|4000000|\n",
      "|       15|       1|       34031346|        32.5|4000000|\n",
      "|       16|       1|       34031755|        32.5|4000000|\n",
      "|       17|       1|       34031859|        32.5|4000000|\n",
      "|       18|       1|       34031339|        32.5|4000000|\n",
      "|       19|       1|       34032309|        32.5|4000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 40 MB or 41943040 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 40.0 MB or 41943040 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 57, unrounded: 56.18890771865845\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 60\n",
      "Duration: 21.08 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       35848430|        34.2|4000000|\n",
      "|        1|       1|       35848361|        34.2|4000000|\n",
      "|        2|       1|       35848274|        34.2|4000000|\n",
      "|        3|       1|       35848223|        34.2|4000000|\n",
      "|        4|       1|       35848174|        34.2|4000000|\n",
      "|        5|       1|       35848173|        34.2|4000000|\n",
      "|        6|       1|       35848170|        34.2|4000000|\n",
      "|        7|       1|       35848165|        34.2|4000000|\n",
      "|        8|       1|       35848121|        34.2|4000000|\n",
      "|        9|       1|       35848003|        34.2|4000000|\n",
      "|       10|       1|       35847964|        34.2|4000000|\n",
      "|       11|       1|       35847897|        34.2|4000000|\n",
      "|       12|       1|       35847873|        34.2|4000000|\n",
      "|       13|       1|       35847862|        34.2|4000000|\n",
      "|       14|       1|       35847853|        34.2|4000000|\n",
      "|       15|       1|       35847849|        34.2|4000000|\n",
      "|       16|       1|       35847844|        34.2|4000000|\n",
      "|       17|       1|       35847828|        34.2|4000000|\n",
      "|       18|       1|       35847785|        34.2|4000000|\n",
      "|       19|       1|       35847780|        34.2|4000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 64 MB or 67108864 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 64.0 MB or 67108864 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 60, unrounded: 60.0\n",
      "EstimatedPartitionsInternet: 36, unrounded: 35.11806732416153\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 60\n",
      "Duration: 20.85 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       35848430|        34.2|4000000|\n",
      "|        1|       1|       35848361|        34.2|4000000|\n",
      "|        2|       1|       35848274|        34.2|4000000|\n",
      "|        3|       1|       35848223|        34.2|4000000|\n",
      "|        4|       1|       35848174|        34.2|4000000|\n",
      "|        5|       1|       35848173|        34.2|4000000|\n",
      "|        6|       1|       35848170|        34.2|4000000|\n",
      "|        7|       1|       35848165|        34.2|4000000|\n",
      "|        8|       1|       35848121|        34.2|4000000|\n",
      "|        9|       1|       35848003|        34.2|4000000|\n",
      "|       10|       1|       35847964|        34.2|4000000|\n",
      "|       11|       1|       35847897|        34.2|4000000|\n",
      "|       12|       1|       35847873|        34.2|4000000|\n",
      "|       13|       1|       35847862|        34.2|4000000|\n",
      "|       14|       1|       35847853|        34.2|4000000|\n",
      "|       15|       1|       35847849|        34.2|4000000|\n",
      "|       16|       1|       35847844|        34.2|4000000|\n",
      "|       17|       1|       35847828|        34.2|4000000|\n",
      "|       18|       1|       35847785|        34.2|4000000|\n",
      "|       19|       1|       35847780|        34.2|4000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 80 MB or 83886080 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 80.0 MB or 83886080 bytes\n",
      "MaxFilesPerPartition 2\n",
      "EstimatedPartitions: 30, unrounded: 30.0\n",
      "EstimatedPartitionsInternet: 29, unrounded: 28.094453859329224\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 30\n",
      "Duration: 21.64 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       2|       71696791|        68.4|8000000|\n",
      "|        1|       2|       71696497|        68.4|8000000|\n",
      "|        2|       2|       71696347|        68.4|8000000|\n",
      "|        3|       2|       71696335|        68.4|8000000|\n",
      "|        4|       2|       71696124|        68.4|8000000|\n",
      "|        5|       2|       71695861|        68.4|8000000|\n",
      "|        6|       2|       71695735|        68.4|8000000|\n",
      "|        7|       2|       71695702|        68.4|8000000|\n",
      "|        8|       2|       71695672|        68.4|8000000|\n",
      "|        9|       2|       71695565|        68.4|8000000|\n",
      "|       10|       2|       71695551|        68.4|8000000|\n",
      "|       11|       2|       71695428|        68.4|8000000|\n",
      "|       12|       2|       71695317|        68.4|8000000|\n",
      "|       13|       2|       71694833|        68.4|8000000|\n",
      "|       14|       2|       71694707|        68.4|8000000|\n",
      "|       15|       2|       71694593|        68.4|8000000|\n",
      "|       16|       2|       71694257|        68.4|8000000|\n",
      "|       17|       2|       69879696|        66.6|8000000|\n",
      "|       18|       2|       68064803|        64.9|8000000|\n",
      "|       19|       2|       68064542|        64.9|8000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 100 MB or 104857600 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 100.0 MB or 104857600 bytes\n",
      "MaxFilesPerPartition 2\n",
      "EstimatedPartitions: 30, unrounded: 30.0\n",
      "EstimatedPartitionsInternet: 23, unrounded: 22.475563087463378\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 30\n",
      "Duration: 21.7 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       2|       71696791|        68.4|8000000|\n",
      "|        1|       2|       71696497|        68.4|8000000|\n",
      "|        2|       2|       71696347|        68.4|8000000|\n",
      "|        3|       2|       71696335|        68.4|8000000|\n",
      "|        4|       2|       71696124|        68.4|8000000|\n",
      "|        5|       2|       71695861|        68.4|8000000|\n",
      "|        6|       2|       71695735|        68.4|8000000|\n",
      "|        7|       2|       71695702|        68.4|8000000|\n",
      "|        8|       2|       71695672|        68.4|8000000|\n",
      "|        9|       2|       71695565|        68.4|8000000|\n",
      "|       10|       2|       71695551|        68.4|8000000|\n",
      "|       11|       2|       71695428|        68.4|8000000|\n",
      "|       12|       2|       71695317|        68.4|8000000|\n",
      "|       13|       2|       71694833|        68.4|8000000|\n",
      "|       14|       2|       71694707|        68.4|8000000|\n",
      "|       15|       2|       71694593|        68.4|8000000|\n",
      "|       16|       2|       71694257|        68.4|8000000|\n",
      "|       17|       2|       69879696|        66.6|8000000|\n",
      "|       18|       2|       68064803|        64.9|8000000|\n",
      "|       19|       2|       68064542|        64.9|8000000|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 128.0 MB or 134217728 bytes\n",
      "MaxFilesPerPartition 3\n",
      "EstimatedPartitions: 20, unrounded: 20.0\n",
      "EstimatedPartitionsInternet: 18, unrounded: 17.559033662080765\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 20\n",
      "Duration: 20.85 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|       3|      107545065|       102.6|12000000|\n",
      "|        1|       3|      107544570|       102.6|12000000|\n",
      "|        2|       3|      107544456|       102.6|12000000|\n",
      "|        3|       3|      107543864|       102.6|12000000|\n",
      "|        4|       3|      107543588|       102.6|12000000|\n",
      "|        5|       3|      107543521|       102.6|12000000|\n",
      "|        6|       3|      107543343|       102.6|12000000|\n",
      "|        7|       3|      107543201|       102.6|12000000|\n",
      "|        8|       3|      107542757|       102.6|12000000|\n",
      "|        9|       3|      107542100|       102.6|12000000|\n",
      "|       10|       3|      107541728|       102.6|12000000|\n",
      "|       11|       3|      105726818|       100.8|12000000|\n",
      "|       12|       3|      102097112|        97.4|12000000|\n",
      "|       13|       3|      102096242|        97.4|12000000|\n",
      "|       14|       3|      102095598|        97.4|12000000|\n",
      "|       15|       3|      102095302|        97.4|12000000|\n",
      "|       16|       3|      102094526|        97.4|12000000|\n",
      "|       17|       3|      102094151|        97.4|12000000|\n",
      "|       18|       3|      102093822|        97.4|12000000|\n",
      "|       19|       3|      101703600|        97.0|12000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 150 MB or 157286400 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 150.0 MB or 157286400 bytes\n",
      "MaxFilesPerPartition 4\n",
      "EstimatedPartitions: 15, unrounded: 15.0\n",
      "EstimatedPartitionsInternet: 15, unrounded: 14.983708724975585\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 15\n",
      "Duration: 21.83 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|       4|      143393288|       136.8|16000000|\n",
      "|        1|       4|      143392682|       136.7|16000000|\n",
      "|        2|       4|      143391985|       136.7|16000000|\n",
      "|        3|       4|      143391437|       136.7|16000000|\n",
      "|        4|       4|      143391237|       136.7|16000000|\n",
      "|        5|       4|      143390979|       136.7|16000000|\n",
      "|        6|       4|      143390150|       136.7|16000000|\n",
      "|        7|       4|      143389300|       136.7|16000000|\n",
      "|        8|       4|      141573953|       135.0|16000000|\n",
      "|        9|       4|      136129345|       129.8|16000000|\n",
      "|       10|       4|      136127748|       129.8|16000000|\n",
      "|       11|       4|      136127161|       129.8|16000000|\n",
      "|       12|       4|      136125930|       129.8|16000000|\n",
      "|       13|       4|      136125394|       129.8|16000000|\n",
      "|       14|       4|      135734775|       129.4|16000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 200 MB or 209715200 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 200.0 MB or 209715200 bytes\n",
      "MaxFilesPerPartition 5\n",
      "EstimatedPartitions: 12, unrounded: 12.0\n",
      "EstimatedPartitionsInternet: 12, unrounded: 11.237781543731689\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 12\n",
      "Duration: 20.92 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|       5|      179241462|       170.9|20000000|\n",
      "|        1|       5|      179240632|       170.9|20000000|\n",
      "|        2|       5|      179239449|       170.9|20000000|\n",
      "|        3|       5|      179239086|       170.9|20000000|\n",
      "|        4|       5|      179238661|       170.9|20000000|\n",
      "|        5|       5|      179237175|       170.9|20000000|\n",
      "|        6|       5|      179235969|       170.9|20000000|\n",
      "|        7|       5|      170161922|       162.3|20000000|\n",
      "|        8|       5|      170159607|       162.3|20000000|\n",
      "|        9|       5|      170158398|       162.3|20000000|\n",
      "|       10|       5|      170156920|       162.3|20000000|\n",
      "|       11|       5|      169766083|       161.9|20000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 256 MB or 268435456 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 256.0 MB or 268435456 bytes\n",
      "MaxFilesPerPartition 6\n",
      "EstimatedPartitions: 10, unrounded: 10.0\n",
      "EstimatedPartitionsInternet: 9, unrounded: 8.779516831040382\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 10\n",
      "Duration: 24.26 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|       6|      215089635|       205.1|24000000|\n",
      "|        1|       6|      215088320|       205.1|24000000|\n",
      "|        2|       6|      215087109|       205.1|24000000|\n",
      "|        3|       6|      215086544|       205.1|24000000|\n",
      "|        4|       6|      215084857|       205.1|24000000|\n",
      "|        5|       6|      213268546|       203.4|24000000|\n",
      "|        6|       7|      238225230|       227.2|28000000|\n",
      "|        7|       7|      238222120|       227.2|28000000|\n",
      "|        8|       7|      238219403|       227.2|28000000|\n",
      "|        9|       3|      101703600|        97.0|12000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 512 MB or 536870912 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 512.0 MB or 536870912 bytes\n",
      "MaxFilesPerPartition 13\n",
      "EstimatedPartitions: 5, unrounded: 4.615384615384615\n",
      "EstimatedPartitionsInternet: 5, unrounded: 4.389758415520191\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 5\n",
      "Duration: 24.85 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|      13|      466025828|       444.4|52000000|\n",
      "|        1|      13|      466021097|       444.4|52000000|\n",
      "|        2|      13|      458755198|       437.5|52000000|\n",
      "|        3|      14|      476444473|       454.4|56000000|\n",
      "|        4|       7|      237828768|       226.8|28000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 1024 MB or 1073741824 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 561.9 MB or 589183401.0 bytes\n",
      "MaxFilesPerPartition 15\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 4, unrounded: 4.0\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 5\n",
      "Duration: 22.91 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|      14|      501873690|       478.6|56000000|\n",
      "|        1|      14|      501868068|       478.6|56000000|\n",
      "|        2|      15|      523188483|       499.0|60000000|\n",
      "|        3|      15|      510418204|       486.8|60000000|\n",
      "|        4|       2|       67726919|        64.6| 8000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 1500 MB or 1572864000 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 561.9 MB or 589183401.0 bytes\n",
      "MaxFilesPerPartition 15\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 4, unrounded: 4.0\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 5\n",
      "Duration: 21.28 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|      14|      501873690|       478.6|56000000|\n",
      "|        1|      14|      501868068|       478.6|56000000|\n",
      "|        2|      15|      523188483|       499.0|60000000|\n",
      "|        3|      15|      510418204|       486.8|60000000|\n",
      "|        4|       2|       67726919|        64.6| 8000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 2048 MB or 2147483648 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 37.5 MB or 39278893.4 bytes\n",
      "Padded File Size: 2247.6 MB or 2356733604 bytes\n",
      "SizePerCore: 501.9 MB or 526268841.0 bytes\n",
      "SizePerCorePadded: 561.9 MB or 589183401.0 bytes\n",
      "MaxSplitPartitionBytes: 561.9 MB or 589183401.0 bytes\n",
      "MaxFilesPerPartition 15\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 4, unrounded: 4.0\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 5\n",
      "Duration: 23.6 sec\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|    rows|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "|        0|      14|      501873690|       478.6|56000000|\n",
      "|        1|      14|      501868068|       478.6|56000000|\n",
      "|        2|      15|      523188483|       499.0|60000000|\n",
      "|        3|      15|      510418204|       486.8|60000000|\n",
      "|        4|       2|       67726919|        64.6| 8000000|\n",
      "+---------+--------+---------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "for maxPartitionSize in [2, 4, 16, 20 ,32, 40, 64, 80, 100, 128, 150, 200, 256, 512, 1024, 1500, 2048]:\n",
    "    results_dict = {}\n",
    "    set_configs(maxPartitionsMB=maxPartitionSize, openCostInMB=4, minPartitions=4)\n",
    "    size = get_parquet_file_size(path)\n",
    "    estimate_num_partitions(size, num_files)\n",
    "    get_actual_num_partitions(path)\n",
    "    noop_write(path)\n",
    "    bytes_rows_per_partition(path)\n",
    "    results_list.append(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 2007.6 MB or 2105075364 bytes\n",
      "Num files: 60\n",
      "Avg file Size: 33.5 MB or 35084589.4 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 60\n",
      "Num rows written: 240000000\n",
      "Num rows per file: 4000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxPartitionsBytes</th>\n",
       "      <th>paddedFileSize</th>\n",
       "      <th>MBPerCore</th>\n",
       "      <th>MBPerCorePadded</th>\n",
       "      <th>maxSplitPartitionBytes</th>\n",
       "      <th>avg_file_size_padded</th>\n",
       "      <th>Maxfiles_per_partition</th>\n",
       "      <th>MyEstimationPartitions</th>\n",
       "      <th>InternetEstimationPartitions</th>\n",
       "      <th>ActualNumPartitions</th>\n",
       "      <th>ExecutionTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1124</td>\n",
       "      <td>1055</td>\n",
       "      <td>23.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>562</td>\n",
       "      <td>540</td>\n",
       "      <td>22.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>141</td>\n",
       "      <td>138</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>113</td>\n",
       "      <td>120</td>\n",
       "      <td>21.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>20.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>64.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>60</td>\n",
       "      <td>22.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>22.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>128.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>22.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>22.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>200.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>22.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>256.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>24.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>512</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>512.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1024</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>37.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>23.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1500</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>37.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>21.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2048</td>\n",
       "      <td>2247.6</td>\n",
       "      <td>501.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>561.9</td>\n",
       "      <td>37.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>21.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    maxPartitionsBytes  paddedFileSize  MBPerCore  MBPerCorePadded  \\\n",
       "0                    2          2247.6      501.9            561.9   \n",
       "1                    4          2247.6      501.9            561.9   \n",
       "2                   16          2247.6      501.9            561.9   \n",
       "3                   20          2247.6      501.9            561.9   \n",
       "4                   32          2247.6      501.9            561.9   \n",
       "5                   40          2247.6      501.9            561.9   \n",
       "6                   64          2247.6      501.9            561.9   \n",
       "7                   80          2247.6      501.9            561.9   \n",
       "8                  100          2247.6      501.9            561.9   \n",
       "9                  128          2247.6      501.9            561.9   \n",
       "10                 150          2247.6      501.9            561.9   \n",
       "11                 200          2247.6      501.9            561.9   \n",
       "12                 256          2247.6      501.9            561.9   \n",
       "13                 512          2247.6      501.9            561.9   \n",
       "14                1024          2247.6      501.9            561.9   \n",
       "15                1500          2247.6      501.9            561.9   \n",
       "16                2048          2247.6      501.9            561.9   \n",
       "\n",
       "    maxSplitPartitionBytes  avg_file_size_padded  Maxfiles_per_partition  \\\n",
       "0                      2.0                  37.5                     1.0   \n",
       "1                      4.0                  37.5                     1.0   \n",
       "2                     16.0                  37.5                     1.0   \n",
       "3                     20.0                  37.5                     1.0   \n",
       "4                     32.0                  37.5                     1.0   \n",
       "5                     40.0                  37.5                     1.0   \n",
       "6                     64.0                  37.5                     2.0   \n",
       "7                     80.0                  37.5                     2.0   \n",
       "8                    100.0                  37.5                     3.0   \n",
       "9                    128.0                  37.5                     3.0   \n",
       "10                   150.0                  37.5                     4.0   \n",
       "11                   200.0                  37.5                     5.0   \n",
       "12                   256.0                  37.5                     7.0   \n",
       "13                   512.0                  37.5                    14.0   \n",
       "14                   561.9                  37.5                    15.0   \n",
       "15                   561.9                  37.5                    15.0   \n",
       "16                   561.9                  37.5                    15.0   \n",
       "\n",
       "    MyEstimationPartitions  InternetEstimationPartitions  ActualNumPartitions  \\\n",
       "0                       60                          1124                 1055   \n",
       "1                       60                           562                  540   \n",
       "2                       60                           141                  138   \n",
       "3                       60                           113                  120   \n",
       "4                       60                            71                   70   \n",
       "5                       60                            57                   60   \n",
       "6                       30                            36                   60   \n",
       "7                       30                            29                   30   \n",
       "8                       20                            23                   30   \n",
       "9                       20                            18                   20   \n",
       "10                      15                            15                   15   \n",
       "11                      12                            12                   12   \n",
       "12                       9                             9                   10   \n",
       "13                       5                             5                    5   \n",
       "14                       4                             4                    5   \n",
       "15                       4                             4                    5   \n",
       "16                       4                             4                    5   \n",
       "\n",
       "    ExecutionTime  \n",
       "0           23.83  \n",
       "1           22.88  \n",
       "2           23.20  \n",
       "3           21.15  \n",
       "4           21.32  \n",
       "5           20.32  \n",
       "6           22.69  \n",
       "7           22.06  \n",
       "8           22.91  \n",
       "9           22.17  \n",
       "10          22.38  \n",
       "11          22.19  \n",
       "12          24.45  \n",
       "13          25.15  \n",
       "14          23.84  \n",
       "15          21.99  \n",
       "16          21.82  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_per_partition = max(1, math.floor(64/37.5))\n",
    "files_per_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.826666666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/37.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEXT:\n",
    "- Smal Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------------+------+\n",
      "|partition|compressedBytes|compressedMB|  rows|\n",
      "+---------+---------------+------------+------+\n",
      "|        0|        1261784|         1.2|150000|\n",
      "|        1|        1258358|         1.2|150000|\n",
      "|        2|        1257550|         1.2|150000|\n",
      "|        3|        1256699|         1.2|150000|\n",
      "+---------+---------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+\n",
      "|partition|compressedBytes|sum(rows)|\n",
      "+---------+---------------+---------+\n",
      "|        1|        1258358|   150000|\n",
      "|        3|        1256699|   150000|\n",
      "|        2|        1257550|   150000|\n",
      "|        0|        1261784|   150000|\n",
      "+---------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = spark.read.parquet_partitions(path)\n",
    "s.groupBy(\"partition\").sum(\"compressedBytes\",\"rows\").withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.015625"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32.1/6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169986942.25"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 679947769/4\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.234375"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 87165529 \n",
    "\n",
    "diff = t-c\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.234375"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42496735.5625"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 679947769/16\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086028.4375"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 43582764\n",
    "diff = t-c\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271507.109375"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169986942.25"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "679947769/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.49373435974121"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43509334/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.08097457885742"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "87116716/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|partition|start    |end      |length   |blocks|compressedBytes|compressedMB|rows    |filename                                                                                                             |\n",
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0        |0        |107202443|107202443|1     |133949814      |127.7       |15790100|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|1        |107202443|214404886|107202443|1     |133895545      |127.7       |15770100|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|2        |214404886|321607329|107202443|0     |0              |0.0         |0       |file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "|3        |321607329|424615468|103008139|2     |156390510      |149.1       |18439800|file:/D:/Spark/Data/1_files_50000000_rows.parquet/part-00000-6cde06e8-a2fb-4a79-9488-b3a53bd5e038-c000.snappy.parquet|\n",
      "+---------+---------+---------+---------+------+---------------+------------+--------+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 1 File - \n",
    "\n",
    "https://mageswaran1989.medium.com/a-dive-into-apache-spark-parquet-reader-for-small-file-sizes-fabb9c35f64e#:~:text=maxPartitionBytes%3A%20128MB%20(The%20maximum%20number,sql.\n",
    "https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPartitionsMB = 70\n",
    "openCostInMB = 4\n",
    "minPartitions = 4\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInMB*1024*1024)+\"b\")\n",
    "spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "#https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "#https://issues.apache.org/jira/browse/SPARK-17998\n",
    "#spark.sql.files.maxPartitionNum = None\t\n",
    "#https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "#https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example\n",
    "#https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134217728"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n",
      "4.0\n",
      "4\n",
      "484.9\n",
      "121.225\n",
      "70.0\n",
      "6.927142857142857\n"
     ]
    }
   ],
   "source": [
    "data_size = 404.9*1024*1024\n",
    "number_files = \n",
    "defaultMaxSplitBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "totalBytes = data_size + (number_files * openCostInBytes)\n",
    "bytesPerCore = totalBytes / minPartitionNum\n",
    "targetSizePerPartition = min(defaultMaxSplitBytes, max(openCostInBytes, bytesPerCore))\n",
    "noPartitions = totalBytes/targetSizePerPartition\n",
    "print(defaultMaxSplitBytes/1024/1024)\n",
    "print(openCostInBytes/1024/1024)\n",
    "print(minPartitionNum)\n",
    "print(totalBytes/1024/1024)\n",
    "print(bytesPerCore/1024/1024)\n",
    "print(targetSizePerPartition/1024/1024)\n",
    "print(noPartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSplitBytes(\n",
    "      sparkSession: SparkSession,\n",
    "      selectedPartitions: Seq[PartitionDirectory]): Long = {\n",
    "defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes\n",
    "    val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes\n",
    "    val minPartitionNum = sparkSession.sessionState.conf.filesMinPartitionNum\n",
    "      .getOrElse(sparkSession.leafNodeDefaultParallelism)\n",
    "    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum\n",
    "    val bytesPerCore = totalBytes / minPartitionNum\n",
    "\n",
    "    Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.leafNodeDefaultParallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.minPartitionNum\", \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 1)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 1 file, 404,9 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_1_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "sc.setJobDescription(\"Read 1 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"70m\")\n",
    "sc.setJobDescription(\"Read 1 file with 70m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"51m\")\n",
    "sc.setJobDescription(\"Read 1 file with 51m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"34m\")\n",
    "sc.setJobDescription(\"Read 1 file with 34m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"200m\")\n",
    "sc.setJobDescription(\"Read 1 file with 200m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"400m\")\n",
    "sc.setJobDescription(\"Read 1 file with 400m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"500m\")\n",
    "sc.setJobDescription(\"Read 1 file with 400m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_1_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 4 files a 101,5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_experiment(maxPartitionsMB, path, file_size, num_files):\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsMB*1024*1024)+\"b\")\n",
    "    sdf_load = spark.read.parquet(path)\n",
    "    print(f\"Number partitions: {sdf_load.rdd.getNumPartitions()}\")\n",
    "    sc.setJobDescription(f\"Read {num_files} file with {maxPartitionsMB} MB\")\n",
    "    sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    partition_size = (file_size)/4\n",
    "    target_partition_size = min(maxPartitionsMB, partition_size)\n",
    "    print(target_partition_size)\n",
    "    print((file_size)/target_partition_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_files = 1\n",
    "num_rows = 10000000\n",
    "size = 406\n",
    "sdf = sdf_generator(num_rows, 1)\n",
    "path = f\"{base_dir}/test_{num_files}_file.parquet\"\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(f\"Write {num_files} files, {size} MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files written: 3\n",
      "Num rows written: 100\n",
      "Num partitions written: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/Spark/Data/3_files_100_rows.parquet'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_generator(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', blocks=1, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0, createdBy='parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)', schema='message spark_schema {\\n  required int64 id;\\n  required int32 date (DATE);\\n  required int96 timestamp;\\n  required binary idstring (STRING);\\n  required binary idfirst (STRING);\\n  required binary idlast (STRING);\\n}\\n', encryption='UNENCRYPTED', keyValues={'org.apache.spark.version': '3.5.0', 'org.apache.spark.sql.parquet.row.metadata': '{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"date\",\"type\":\"date\",\"nullable\":false,\"metadata\":{}},{\"name\":\"timestamp\",\"type\":\"timestamp\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idstring\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idfirst\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"idlast\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}}]}'})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_metadata(path).dropDuplicates([\"filename\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.76440238952637"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "84687614/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(partition=0, start=0, end=107202443, length=107202443, blocks=1, compressedBytes=133949814, uncompressedBytes=312755950, rows=15790100, columns=6, values=94740600, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=1, start=107202443, end=214404886, length=107202443, blocks=1, compressedBytes=133895545, uncompressedBytes=323456519, rows=15770100, columns=6, values=94620600, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=2, start=214404886, end=321607329, length=107202443, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468),\n",
       " Row(partition=3, start=321607329, end=424615468, length=103008139, blocks=2, compressedBytes=156390510, uncompressedBytes=378213943, rows=18439800, columns=6, values=110638800, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-fc9294ce-8c57-4be8-a157-fba53a12f902-c000.snappy.parquet', fileLength=424615468)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_partitions(path).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', block=1, blockStart=4, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_blocks(path).dropDuplicates([\"filename\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration.setInt(\"parquet.block.size\",blockSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.sql.parquet.block.size\" cannot be found. Please verify that the config exists.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\r\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5234)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.parquet.block.size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.sql.parquet.block.size\" cannot be found. Please verify that the config exists.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\r\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5234)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5234)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.parquet.block.size\")\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125829120"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 120*1024*1024\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424235872\n"
     ]
    }
   ],
   "source": [
    "b1 = 4\n",
    "b2 = 133949818\n",
    "b3 = 267845363\n",
    "b4 = 401778296\n",
    "e = b4 + 22457577 -1\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66974911.0\n",
      "200897590.5\n",
      "334811829.5\n",
      "413007084.0\n"
     ]
    }
   ],
   "source": [
    "m1 = (b1+b2)/2\n",
    "m2 = (b2+b3)/2\n",
    "m3 = (b3+b4)/2\n",
    "m4 = (b4+e)/2\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "print(m3)\n",
    "print(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503316480"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377487360"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8120698"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949818-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 4\n",
      "110.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "read_experiment(120, path, 442, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.74449729919434"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949814/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(partition=0, start=0, end=22238867, length=22238867, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=1, start=22238867, end=44477734, length=22238867, blocks=1, compressedBytes=84687614, uncompressedBytes=193996611, rows=10000000, columns=6, values=60000000, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=2, start=44477734, end=66716601, length=22238867, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164),\n",
       " Row(partition=3, start=66716601, end=84761164, length=18044563, blocks=0, compressedBytes=0, uncompressedBytes=0, rows=0, columns=0, values=0, nulls=0, filename='file:/D:/Spark/Data/test_1_file.parquet/part-00000-6ceb4061-1dcd-458e-8453-36d8001bbdd1-c000.snappy.parquet', fileLength=84761164)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet_partitions(path).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.1456127166748"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "156390510/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.74449729919434"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133949814/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404.5828523635864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "424235869/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 9\n",
      "2.5\n",
      "4.0\n",
      "Number partitions: 5\n",
      "5.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "7.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "17.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "25.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "32.0\n",
      "4.0\n",
      "Number partitions: 4\n",
      "50.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [10, 20, 30, 70, 100, 128, 200]:\n",
    "    read_experiment(size, path, size, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 4)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 4 file, 101,5 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_4_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 4 file, 20 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_20_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 52\n",
      "8\n",
      "52.75\n",
      "Number partitions: 26\n",
      "16\n",
      "26.375\n",
      "Number partitions: 14\n",
      "32\n",
      "13.1875\n",
      "Number partitions: 8\n",
      "64\n",
      "6.59375\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "105.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [8, 16, 32, 64, 128, 256, 512]:\n",
    "    read_experiment(size, f\"{base_dir}/test_4_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number partitions: 60\n",
      "8\n",
      "50.75\n",
      "Number partitions: 30\n",
      "16\n",
      "25.375\n",
      "Number partitions: 20\n",
      "32\n",
      "12.6875\n",
      "Number partitions: 10\n",
      "64\n",
      "6.34375\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n",
      "Number partitions: 4\n",
      "101.5\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for size in [8, 16, 32, 64, 128, 256, 512]:\n",
    "    read_experiment(size, f\"{base_dir}/test_20_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_experiment(128, f\"{base_dir}/test_4_file.parquet\", 406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_experiment(64, f\"{base_dir}/test_4_file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "sc.setJobDescription(\"Read 4 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"70m\")\n",
    "sc.setJobDescription(\"Read 4 file with 70m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"51m\")\n",
    "sc.setJobDescription(\"Read 4 file with 51m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_4_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 20 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(50000000, 20)\n",
    "print(sdf.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Write 20 file, 20,3 MB\")\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(f\"{base_dir}/test_20_file.parquet\")\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "sc.setJobDescription(\"Read 20 file with 128m\")\n",
    "sdf_load = spark.read.parquet(f\"{base_dir}/test_20_file.parquet\")\n",
    "print(sdf_load.rdd.getNumPartitions())\n",
    "sdf_load.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2.5.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
