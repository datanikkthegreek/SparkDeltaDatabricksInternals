{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference gresearch:\\n- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\\n- GitHub Spark extension: https://github.com/G-Research/spark-extension\\n- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning off AQE as it generates more jobs which might be confusing for this scenario here. \n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "#to not cache dataframes... this may not create repeatable results\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-03-15|2024-03-15 06:32:...|       0|      0|     0|\n",
      "|  1|2024-03-15|2024-03-15 06:32:...|       1|      1|     1|\n",
      "|  2|2024-03-15|2024-03-15 06:32:...|       2|      2|     2|\n",
      "|  3|2024-03-15|2024-03-15 06:32:...|       3|      3|     3|\n",
      "|  4|2024-03-15|2024-03-15 06:32:...|       4|      4|     4|\n",
      "|  5|2024-03-15|2024-03-15 06:32:...|       5|      5|     5|\n",
      "|  6|2024-03-15|2024-03-15 06:32:...|       6|      6|     6|\n",
      "|  7|2024-03-15|2024-03-15 06:32:...|       7|      7|     7|\n",
      "|  8|2024-03-15|2024-03-15 06:32:...|       8|      8|     8|\n",
      "|  9|2024-03-15|2024-03-15 06:32:...|       9|      9|     9|\n",
      "| 10|2024-03-15|2024-03-15 06:32:...|      10|      1|     0|\n",
      "| 11|2024-03-15|2024-03-15 06:32:...|      11|      1|     1|\n",
      "| 12|2024-03-15|2024-03-15 06:32:...|      12|      1|     2|\n",
      "| 13|2024-03-15|2024-03-15 06:32:...|      13|      1|     3|\n",
      "| 14|2024-03-15|2024-03-15 06:32:...|      14|      1|     4|\n",
      "| 15|2024-03-15|2024-03-15 06:32:...|      15|      1|     5|\n",
      "| 16|2024-03-15|2024-03-15 06:32:...|      16|      1|     6|\n",
      "| 17|2024-03-15|2024-03-15 06:32:...|      17|      1|     7|\n",
      "| 18|2024-03-15|2024-03-15 06:32:...|      18|      1|     8|\n",
      "| 19|2024-03-15|2024-03-15 06:32:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf_generator(10000000, 8)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_schema = \"id bigint, date date, timestamp timestamp, idstring string, idfirst string, idlast string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How predicate pushdown works on Spark since Spark 3.1.0: https://www.waitingforcode.com/apache-spark-sql/what-new-apache-spark-3.1-predicate-pushdown-json-csv-apache-avro/read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 Initial state JSON\n",
    "\n",
    "How it works: https://github.com/jerryshao/apache-spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L323\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Save Json\")\n",
    "path_json = \"D:/Spark/Data/format_json_large.json\"\n",
    "sdf.write.format(\"json\").mode(\"overwrite\").save(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set maxPartitions MB for a fair comparison of 8 partitions also during read \n",
    "maxPartitionsMB = 160\n",
    "maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Load Json all data\")\n",
    "sdf_json = spark.read.format(\"json\").schema(sdf_schema).load(path_json)\n",
    "sdf_json.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Column filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"JSON Column Filter\")\n",
    "sdf_json = spark.read.format(\"json\").schema(sdf_schema).load(path_json)\n",
    "sdf_json = sdf_json.select(\"id\", \"idstring\")\n",
    "sdf_json.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Row filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"JSON Row Filter id\")\n",
    "sdf_json = spark.read.format(\"json\").schema(sdf_schema).load(path_json)\n",
    "sdf_json = sdf_json.filter(f.col(\"id\") < 300)\n",
    "sdf_json.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - CSV with schema interference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Initial state CSV\n",
    "\n",
    "How it works: https://github.com/jerryshao/apache-spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L470\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Save CSV\")\n",
    "path_csv = \"D:/Spark/Data/format_csv_large.csv\"\n",
    "sdf.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"True\").save(path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set maxPartitions MB for a fair comparison of 8 partitions also during read \n",
    "maxPartitionsMB = 80\n",
    "maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Load CSV All data\")\n",
    "sdf_csv = spark.read.format(\"csv\").options(header=True).schema(sdf_schema).load(path_csv)\n",
    "sdf_csv.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Column Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"CSV Column Filter\")\n",
    "sdf_csv = spark.read.format(\"csv\").schema(sdf_schema).load(path_csv)\n",
    "sdf_csv = sdf_csv.select(\"id\", \"idstring\")\n",
    "sdf_csv.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3 Row Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"CSV Row Filter id\")\n",
    "sdf_csv = spark.read.format(\"csv\").schema(sdf_schema).load(path_csv)\n",
    "sdf_csv = sdf_csv.filter(f.col(\"id\") < 300)\n",
    "sdf_csv.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 Initial state Parquet\n",
    "\n",
    "How it works: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Save Parquet\")\n",
    "path_parquet = \"D:/Spark/Data/format_parquet_large.parquet\"\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set maxPartitions MB for a fair comparison of 8 partitions also during read \n",
    "maxPartitionsMB = 15\n",
    "maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing Spark config: spark.sql.sources.useV1SourceList, default: avro,csv,json,kafka,orc,parquet,text\n",
    "\n",
    "A comma-separated list of data source short names or fully qualified data source \n",
    "implementation class names for which Data Source V2 code path is disabled. These \n",
    "data sources will fallback to Data Source V1 code path.\n",
    "\n",
    "Data Source V2 is a more flexible data source class supporting easier implementation of new pushdown options and also might be a bit more efficient.\n",
    "\n",
    "https://www.youtube.com/watch?v=U5n-evWfYSQ\n",
    "\n",
    "https://github.com/apache/spark/blob/fe0aa1edff047689a1906eaa86d667e0ddfabfbc/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L3207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check with Spark community why V2 not activated for parquet and aggregate not pushed as default\n",
    "#TODO: Why statistics eliminated from Scanning Tab... Inefficiency?\n",
    "#TODO: filter and count does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"parquet\")\n",
    "sc.setJobDescription(\"Load Parquet all data V1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "sc.setJobDescription(\"Load Parquet all data V2\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 Column Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"parquet\")\n",
    "sc.setJobDescription(\"Parquet Column Filter V1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.select(\"id\", \"idstring\")\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "sc.setJobDescription(\"Parquet Column Filter V2\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.select(\"id\", \"idstring\")\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 Row Filter id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"parquet\")\n",
    "sc.setJobDescription(\"Parquet Row Filter id V1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 300)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "sc.setJobDescription(\"Parquet Row Filter id V2\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 300)\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4 Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"false\")\n",
    "sc.setJobDescription(\"Parquet count aggegratePushdown false\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing: spark.sql.parquet.aggregatePushdown, default false, needs data source not to be listed in spark.sql.sources.useV1SourceList\n",
    "\n",
    "If true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any Parquet file footer, exception would be thrown.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"true\")\n",
    "sc.setJobDescription(\"Parquet count aggegratePushdown true\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-5 Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|max(id)|\n",
      "+-------+\n",
      "|9999999|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"false\")\n",
    "sc.setJobDescription(\"Parquet max aggegratePushdown false\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_max = sdf_parquet.groupBy().max(\"id\")\n",
    "sdf_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|max(id)|\n",
      "+-------+\n",
      "|9999999|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"true\")\n",
    "sc.setJobDescription(\"Parquet max aggegratePushdown true\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_max = sdf_parquet.groupBy().max(\"id\")\n",
    "sdf_max.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-6 filter and a count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"true\")\n",
    "sc.setJobDescription(\"Parquet count with filter 1250000\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") < 1250000)\n",
    "sdf_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-7 filter and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|max(id)|\n",
      "+-------+\n",
      "|1249999|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"true\")\n",
    "sc.setJobDescription(\"Parquet max with filter 1250000\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"id\") <= 1249999)\n",
    "sdf_max = sdf_parquet.groupBy().max(\"id\")\n",
    "sdf_max.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-8 Partitioned and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Save Parquet Partitioned\")\n",
    "path_parquet_part = \"D:/Spark/Data/format_parquet_partitioned.parquet\"\n",
    "sdf.repartition(\"idlast\").write.partitionBy(\"idlast\").format(\"parquet\").mode(\"overwrite\").save(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "sc.setJobDescription(\"Parquet Row Filter idlast Partitioned\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet_part)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"idlast\") == \"1\")\n",
    "sdf_parquet.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-9 Partitioned, filter and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|max(id)|\n",
      "+-------+\n",
      "|9999991|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.useV1SourceList\", \"\")\n",
    "spark.conf.set(\"spark.sql.parquet.aggregatePushdown\", \"true\")\n",
    "sc.setJobDescription(\"Parquet partitioned max with filter idlast 1\")\n",
    "sdf_parquet = spark.read.format(\"parquet\").schema(sdf_schema).load(path_parquet_part)\n",
    "sdf_parquet = sdf_parquet.filter(f.col(\"idlast\") == \"1\")\n",
    "sdf_max = sdf_parquet.groupBy().max(\"id\")\n",
    "sdf_max.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 Initial state AVRO\n",
    "\n",
    "How it works: https://spark.apache.org/docs/latest/sql-data-sources-avro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Save Avro\")\n",
    "path_avro = \"D:/Spark/Data/format_avro_large.avro\"\n",
    "sdf.write.format(\"avro\").mode(\"overwrite\").save(path_avro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set maxPartitions MB for a fair comparison of 8 partitions also during read \n",
    "maxPartitionsMB = 10\n",
    "maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Load Avro All data\")\n",
    "sdf_avro = spark.read.format(\"avro\").schema(sdf_schema).load(path_avro)\n",
    "sdf_avro.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2 Column Filter\n",
    "\n",
    "Results:\n",
    "- Load data: 1.6 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Avro Column Filter\")\n",
    "sdf_avro = spark.read.format(\"avro\").schema(sdf_schema).load(path_avro)\n",
    "sdf_avro = sdf_avro.select(\"id\", \"idstring\")\n",
    "sdf_avro.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3 Row Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Avro ID Filter\")\n",
    "sdf_avro = spark.read.format(\"avro\").schema(sdf_schema).load(path_avro)\n",
    "sdf_avro = sdf_avro.filter(f.col(\"id\") < 300)\n",
    "sdf_avro.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
