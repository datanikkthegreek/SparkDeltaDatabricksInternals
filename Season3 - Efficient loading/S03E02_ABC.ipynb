{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"D:/Spark/Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generator(num_rows, num_files, format = \"parquet\"):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.{format}\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(format).mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: D:/Spark/Data/4_files_1000_rows.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/Spark/Data/4_files_1000_rows.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_generator(1000, 4, \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/Spark/Data/4_files_1000_rows.parquet\"\n",
    "path2 = 'D:/Spark/Data/4_files_1000_rows.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#0L,date#1,timestamp#2,idstring#3,idfirst#4,idlast#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Parquet all\")\n",
    "sdf = spark.read.parquet(path)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#18L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Parquet one column\")\n",
    "sdf = spark.read.parquet(path).select(\"id\")\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(id#33L) AND (id#33L < 100))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#33L,date#34,timestamp#35,idstring#36,idfirst#37,idlast#38] Batched: true, DataFilters: [isnotnull(id#33L), (id#33L < 100)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,100)], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"Parquet filter\")\n",
    "sdf = spark.read.parquet(path).filter(f.col(\"id\") < 100)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_c0#111) AND (cast(_c0#111 as int) < 100))\n",
      "+- FileScan csv [_c0#111,_c1#112,_c2#113,_c3#114,_c4#115,_c5#116] Batched: false, DataFilters: [isnotnull(_c0#111), (cast(_c0#111 as int) < 100)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.csv], PartitionFilters: [], PushedFilters: [IsNotNull(_c0)], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"CSV filter\")\n",
    "sdf = spark.read.csv(path2).filter(f.col(\"_c0\") < 100)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [id#194L,date#195,timestamp#196,idstring#197,idfirst#198,idlast#199] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"CSV Schema\")\n",
    "ddl_schema = \"id bigint, date date, timestamp timestamp, idstring string, idfirst string, idlast string\"\n",
    "sdf = spark.read.schema(ddl_schema).csv(path2)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [_c0#235,_c1#236,_c2#237,_c3#238,_c4#239,_c5#240] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobDescription(\"CSV without Schema\")\n",
    "sdf = spark.read.csv(path2)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(id#137L) AND (id#137L < 100))\n",
      "   +- InMemoryTableScan [id#137L, date#138, timestamp#139, idstring#140, idfirst#141, idlast#142], [isnotnull(id#137L), (id#137L < 100)]\n",
      "         +- InMemoryRelation [id#137L, date#138, timestamp#139, idstring#140, idfirst#141, idlast#142], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *(1) ColumnarToRow\n",
      "                  +- FileScan parquet [id#137L,date#138,timestamp#139,idstring#140,idfirst#141,idlast#142] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet(path)\n",
    "sdf = sdf.cache()\n",
    "sdf = sdf.filter(f.col(\"id\") < 100)\n",
    "sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(id#66L) AND (id#66L < 100))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#66L,date#67,timestamp#68,idstring#69,idfirst#70,idlast#71] Batched: true, DataFilters: [isnotnull(id#66L), (id#66L < 100)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/Spark/Data/4_files_1000_rows.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,100)], ReadSchema: struct<id:bigint,date:date,timestamp:timestamp,idstring:string,idfirst:string,idlast:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
